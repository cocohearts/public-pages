General idea: given samples from a distribution, we want to answer questions about the underlying distribution/predict the future.

Process goes:
- data, assumptions -> model -> predictions, declarations of uncertainty

Notes on moments:
- Variance is $\mathbb{E}[X^{2}]-\mathbb{E}[X]^{2},$ adds linearly over independent variables
- Covariance is $\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y],$ is 0 for indep. variables
- Covariance matrix $\Sigma$ satisfies $\Sigma_{ij}=\text{Covar}[X_{i},X_{j}],$ note that it is symmetric
- Can also think of $\Sigma$ as the expected value of the outer product of random vector $X$ with itself
- Hence $\Sigma^{-1/2}X$ is unit normal, since its outer product is $\Sigma^{-1/2}X X^{T}\Sigma^{-1/2}$  whose expected value is $I$
## Distributions
Poisson—limit of Binomial: $\frac{{\lambda^{k}e^{\lambda}}}{k!}$
Multivariate Gaussian—invert the covariance: $\frac{1}{\sqrt{ (2\pi)^{k}\det(\Sigma) }} \exp((x-\mu)^{\dagger}\Sigma ^{-1}(x-\mu)).$
Beta—likelihood for Bernoulli parameter: $\text{Norm}\left[ x^{a}(1-x)^{b} \right]:x \in [0,1]$
Gamma—
Student-t—fat version of Gaussian used when fitting model with unknown true variance, using sample variance
Chi-squared—with $n$ degrees of freedom is sum of $n$ independent unit normal squared
### CLT
If $X_{i}$ drawn from distribution with average $\mu$ and std $\sigma$ then the $n$th average $\bar{X}$ has mean $\mu$ and std $\frac{\sigma}{\sqrt{ n }}.$ We call the constant $\sigma$ the "asymptotic std".

If the sample averages for $X$ converge to mean $\mu$ and asymptotic std $\sigma$ then the sample averages for $g(X)$ converge to mean $g(\mu)$ and asymptotic std $g'(\mu)\sigma.$
This makes sense since as we grab more samples the density clusters around $\mu$ so we only care about the "stretching" behavior of $g$ local to $\mu.$ If $g$ spreads things out by a factor of $2$ then our linear unit, std., also stretches out by 2.
## Parametric Estimation
### MLE, Fisher
Given data, we have a model $\mathbb{P}_{\theta}$ parametrized by a (usually finite) list of constants $\theta$. Given data we map it to an estimator $\hat{\theta}$ that can be judged from being
- *biased* (if its EV is different from true)
- has *se* (the std from true value)
- *consistent* (if it converges to true value in probability as we take many samples)
- *asymptotically normal* if as $n$ goes to infty it converges to being normal around true value with variance $\propto \,n^{-1/2}$.

We get an MLE estimator for all parameters by plugging in likelihood of drawing all of our data from the pdf formula, and maximizing log likelihood (take a derivative). It is asymptotically normal, consistent, and asymptotically efficient.

Note that true value maximizes *population log likelihood* i.e. log likelihood over the population, because log likelihood is really just KL divergence from true distribution to our proposed distribution. The sample MLE maximizes *sample log likelihood*.
#### Fisher info
Given a model $f_{\theta}$ and a particular value for $\theta,$ the Fisher information is
$$
I(\theta):=\mathbb{E}_{\theta}[-\nabla^{2}_{\theta}\log f_{\theta}(X)]=\mathbb{V}_{\theta}[\nabla \log f_{\theta}(X)],
$$
when drawing a random $X$ from the distribution $f_{\theta}$.

To check the second equality, note that
$$
-\nabla^{2}\log f_{\theta}=-\nabla\frac{{\nabla f_{\theta}}}{f_{\theta}}=-\frac{{\nabla^{2}f_{\theta}f_{\theta}-(\nabla f_{\theta}^{2})}}{f_{\theta}^{2}}=\left( \frac{{\nabla f_{\theta}}}{f_{\theta}} \right) ^{2} - \frac{{\nabla^{2}f_{\theta}}}{f_{\theta}}.
$$
But then $\mathbb{E}\left( \frac{{\nabla g}}{f_{\theta}} \right)=\int \nabla g=0,$ so $\mathbb{E}[-\nabla^{2}\log f_{\theta}]=\mathbb{V}[\nabla \log f_{\theta}].$

Key claim: asymptotic std of the MLE estimator is $I(\theta^{*})^{-1}.$

Proof:
Taylor expand
$$
0=l_{n}'(\theta)\approx l_{n}'(\theta^{*})+l''_{n}(\theta^{*})(\hat{\theta}-\theta^{*}),
$$
so that
$$
\sqrt{ n }(\hat{\theta}-\theta^*)=-\frac{{\sqrt{ n }\overline{\partial_{\theta}  \log f_{\theta}{(X_{i})|_{\theta ^*}} } }}{\overline{\partial^{2}_{\theta}\log f_{\theta}(X_{i})|_{\theta^*}}}.
$$
The numerator has expected value $0$ since $l'(\theta^*)=0,$ and variance equal to $I(\theta^*).$ The denominator converges to its mean value, also equal to $I(\theta^*).$ Hence the quotient has mean $0$ and variance $I(\theta^*)^{-1}.$
### Method of Moments
Given $k$ real parameters $\theta$ that we want to solve for, we plug in for the first $k$ moments of our model $f_{\theta}(X).$ Then we can express the $i$th moment in terms of our $k$ parameters, and we solve the resulting $k$ equations.

Remark: we could take higher order moments, but they would yield higher variance answers, since higher moments "swing around" more.
### EM Algo
We sometimes have a mixture of gaussians. If we determine which Gaussian each data sample is from, we can do normal MLE in order, first by estimating $\mu,\sigma$ for each Gaussian and then balancing $p_{i}.$

However if we don't know, we need to assign a probability distribution of which Gaussian each element is in. Then the log likelihood for a given datapoint is $\sum_{i}p_{i}l_{i}(X),$ where $p_{i}$ is the likelihood of belonging to the $i$th Gaussian.

Bring in the EM-algorithm. On the *Expectation step* we weight the odds of $p_{i}$ by comparing odds according to the current $\mu_{i},\sigma_{i}.$ On the *Maximization step* we calculate the $\mu_{i},\sigma_{i}$ exactly using MLE.
### Bootstrap
We want to get variance on our estimator, but the underlying pdf is intractable. Instead, we "repeat the experiment" by taking *bootstrap samples* from our existing samples *with replacement*. For each sample we compute our estimator, then we create a histogram of drawn estimators from which we can compute variance, confidence intervals, etc. We can do normal confidence interval by taking std and assuming distribution is normal, or we can take *percentile confidence interval* which just takes quartiles using our histogram. Finally we can take *pivotal confidence interval* $\left( 2\hat{\theta}_{0}-q_{\frac{\alpha}{2}}, 2\hat{\theta}_{0}-q_{1-\frac{\alpha}{2}}  \right)$ because we want true to lie within a certain range of our estimate, not vice versa (??) so we flip the interval around $\hat{\theta_{0}}.$

Optimally we do all $n^{n}$ samplings but this is not actually feasible.
## Nonparametric Estimation
Goal: estimating curves/distributions without setting an ansatz and solving.

Generally we have some true distribution $g$ generating noisy samples $X$ which we fit to an estimator $X_{i}\to \hat{g}.$ Then (taking expectations over samples, for conf. interval purposes)
- *bias* is how far estimator mean is from truth, for some input $x$
- *variance* is variance of estimator from its own mean
- *mean squared error* is actual optimization point, MSE of estimator from ground truth
Note that $MSE(\hat{g},x)=b_{\hat{g}}^{2}(x)+v_{\hat{g}}(x).$ We define $R(\hat{g},g)=\int MSE(\hat{g},x)\,dx$ as error over the entire input. Also called mean integral squared error.

For optimization purposes we want to check the MISE, reduces to $\int g(x)\hat{g}(x)\,dx=\mathbb{E}_{x\sim g} \int \hat{g}(x).$ Since $\hat{g}$ depends on chosen $x$ we just split train/test, textbook suggests to run all of the $n-1 / 1$ train-test splits.

Can estimate using
- histograms (box samples and make uniform in each box) called *Regressogram*
- kernel density estimators (put a blob on each sample, fix variance to $h^{2}$ with reparameterization, mean over samples) called *Nadaraya-Watson*

If we want to do regression we have a distribution over pairs $X,Y.$ Can apply either technique—would be taking kernels in $X\oplus Y$ and taking probabilities over fixed $X$.
## Testing
Have null hypothesis, which we take to be status quo. Assume null hypothesis, what is the likelihood of seeing given data? Call that $p$-value, determines the level of our test. Type 1 error is false positive, Type 2 error is false negative.

The "power" of a test is likelihood of rejecting null, defined by $\beta(\theta).$ If $H_{0}: \theta=\theta_{0},$ then likelihood of type 1 (rejecting when shouldn't reject) is $\beta(\theta_{0}).$
### Wald test
Asymptotically normal estimator $\hat{\theta},$ we return positive iff outside percentile confidence interval.
### Chi-squared
Define $\chi_{k}^{2}=\sum_{i.i.d.}^{k}Z_{i}^{2},$ where $Z_{i}\sim \mathcal N(0,1).$ Then given drawn samples $X_{i}$ and null hypothesis pmf, we can count real and expected occurrences of each outcome. Then the value
$$
T=\sum_{i}\frac{{(O_{i}-E_{i})^{2}}}{E_{i}}
$$
follows a $\chi_{k-1}^{2}$ distribution.

Proof: Let our samples $X_{i}$ be represented as one-hot encodings in our pmf. Then null hypothesis is that
$$
\Sigma=\begin{bmatrix}
p_{1}(1-p_{1})&-p_{1}p_{2}&-p_{1}p_{3} \\
-p_{2}p_{1}&p_{2}(1-p_{2})&-p_{2}p_{3} \\
-p_{3}p_{1}&-p_{3}p_{2}&-p_{3}(1-p_{3})
\end{bmatrix}
=
-\vec{p}\otimes \vec{p}+diag(\vec{p}).
$$
Let's remove the last row/column to get $\hat{\Sigma}$. Then letting $\hat{p}$ denote the truncated probability vector, notably
$$
\hat{\Sigma} ^{-1}=diag(\hat{p}^{-1})+\frac{1}{p_{k}}\begin{bmatrix}
1 & \dots&1 \\
\vdots &\ddots &\vdots\\
1 & \dots&1 \\
\end{bmatrix}
$$
To check this, off diagonal dot products $M_{ij}$ of $\hat{\Sigma}\hat{\Sigma}^{-1}$ become $-p_{i}+\frac{1}{}$
### KL and KS Tests
Given iid samples from a fixed pdf, we can use Kolmogorov-Smirnov. We can generate an *empirical CDF* generated from uniform over our samples. The sup of the difference between our empirical CDF and the real CDF is distributed according to the KS distribution, dependent on $n$ but not dependent on the distribution!

In the case that it's a Gaussian whose $\mu$ and $\sigma$ we derive from the sample, we need to use the Kolmogorov Lilliefors test, which requires a smaller distance to reject the null (because the Gaussian is already fitted to the data).
### Permutation, Multiple Hypothesis
Are two samples from the same distribution? Bootstrap-style, mix them up and take a histogram of the difference in sample means. If the difference in means of our two samples are too far OOD then reject.

For multiple hypothesis, we can take a naive union bound over all our hypothesis, dividing the $p$-value for each test by the number of tests (Bonferroni).

Alternatively if they're guaranteed to be independent, the Benjamini-Hochberg method sorts the $p$-values in order, and taking the biggest one that lies below $\frac{\alpha}{m}i$ (where $i$ is the index of $p$-value) keeps (#false positives)/(#positives) below $\alpha$ in expectation.
### Student-t
Smarter version of Wald's. It compensates for not knowing the std $\alpha$. Specifically $\sqrt{ n }(\bar{X}_{n}-\mu)\hat{\sigma}\sim t_{n-1}$ for $n-1$ DOF.
### Things to Rem.
Within 1 std: 68%
Within 2 std: 95%
Within 3 std: 99.7%
### Distributions
*Beta*: $p(x)=\frac{1}{K}x^{a-1}(1-x)^{b-1}$ for $Beta(a,b).$ Expected value $\frac{a}{a+b}.$
## Bayesian
We have a prior distribution on parameters $\theta.$
After seeing data we adjust the distribution based on a conditional. The net effect is that the posterior is the likelihood of seeing this data, weighted by the prior:
$$
f(\theta|X_{i})\propto f_{\theta}(X_{i})\cdot f(\theta).
$$
If we start with uniform over $\mathbb{R}$ or some interval $[a,b]$ then this is equivalent to MLE.
### Estimators
Bayes estimator is just the mean of our posterior, i.e. mean *a posteriori*. MAP, or maximum a posteriori, is the mode of our posterior.
### Aside: Robustness
An estimator is *robust* if it stays bounded when an adversary changes some amount of it. For instance median is robust with breakdown point $\frac{1}{2}.$ We can arbitrarily make an estimator more robust by dropping the top and bottom percentiles.

More robust estimators do MLE assuming that $\epsilon$ of our samples have been contaminated by an adversary $q$. We can also try omitting samples, i.e. calculate MLE by maximizing $l_{n}$ over all possible omission sets, and then maximize $\theta$ to get a double max:
$$
\hat{\theta}={\arg\max}_{\theta}\max_{|C|=m}l_{n}(\theta,X_{i}\backslash C).
$$
## Regression
### Linear Regression
Given vectors $X_{i}$ we want to map to scalars $Y_{i}.$ We assume our distribution is $f(X)=\mathcal N(X\beta ^{T}, \mathbf{1}).$ Then MLE just wants us to minimize least-squares distance between $X\beta$ and $Y,$ where we now take the matrix $X$ and vector $Y.$ Here the rows of $X$ are the vectors $X_{i}.$

We can take gradients to solve the minimization problem
$$
L=\left( X\beta-Y \right) \cdot \left( X\beta-Y \right) = \beta^{T}X^{T}X\beta-2Y^{T}X\beta.
$$
Then
$$
\nabla_{\beta} L=2X^{T}X\beta - 2Y^{T}X,
$$
so
$$
\beta=(X^{T}X)^{-1}X^{T}Y.
$$
Now note that our "closest solution" $X\beta=X(X^{T}X)^{-1}X^{T}Y$ is in fact the result of projecting $Y$ down to $\text{colspace}(X).$ Clearly the output of
$$P=X(X^{T}X)^{-1}X^{T}$$
is in $\text{colspace}(X),$ and since $P^{2}=P^{T}=P$ we get $P(I-P)^{T}=0$ and so projections onto $\text{colspace}(X)$ are in fact normal. Define $\mathbb{X}$ as the matrix with $X_{i}^{\dagger}$ as rows.
#### Variance on Linear Regression
Suppose our (independent) $Y$ have (diagonalized) variance $\sigma^{2}.$ Then we can model $Y=X\beta^*+\epsilon$ where $\epsilon\sim\mathcal N(0,\sigma).$ Plugging back into our expression for $\beta$ we get that
$$Var(\beta)=\sigma^{2}((X^{T}X)^{-1}X^{T})((X^{T}X)^{-1}X^{T})^{T}=\sigma^{2}(X^{T}X)^{-1}.$$
Note that $\sigma^{2}$ commutes because it's diagonalized. As an operational note we often use $\sigma^{2}=\frac{{\lVert Y-X\beta \rVert^{2}}}{n-k}$ as our error estimate.
### Logistic Regression
We want to predict a binary, or more generally a multiclass, i.e. what is the likelihood $Y$ will happen given $X$? This is very machine-learning style! We get $\sigma(x^{T}\beta)$ as our model, where $x^{T}\beta$ outputs a bunch of logits. We can then do softmax to get our probabilities, and perform MLE on the matrix.

Statisticians will insist we conserve degrees of freedom, so our softmax just sets $l_{0}=0$ and only predicts the latter $m-1$ logits. Note that this is consistent with using sigmoid for binary classification.
### R squared
The $R^{2}$ score is
$$
R^{2}(S)=1-\frac{{\lVert Y-X\beta(S) \rVert ^{2}}}{\lVert Y-\bar{Y}\mathbb 1 \rVert^{2} }.
$$
The denom is squared distance from $Y$ to the line spanned by $\mathbb{1},$ (also equals $Var(Y)$), i.e. performance if we make our model just the mean. It becomes negative when the $X$ features predict worse than the mean.
### Feature selection
Generally adding more features to input data $X$ will allow model to fit better (e.g. project closer, eventually memorize data) but this is bad. We can pick features that increase $R^{2}$ the fastest, but don't include all to avoid overfit. Can do beam search and look for some plateau.

We can also measure overfitting using information. 
- Akaike info: $l_{n}(\hat{\beta}(S))-|S|$
- Bayesian info: $l_{n}(\hat{\beta}(S))-\frac{{\log n}}{2}|S|$
## Confounding Setups
### Survival/Censoring
Sometimes we want to collect samples $T_{i}$ but they get "cut off" arbitrarily at $C_{i}$, independent of the statistic. How to estimate the cdf?

Derive a "harm" estimator $h(t):=\Pr(T_{i}=t\mid T_{i}\geq t)$. Then we can directly count these from taking all $C_{i}>t$ and computing
$$
\frac{{\# T_{i}=t}}{\# C_{i}, T_{i} \geq t}.
$$
Then $\Pr(T_{i}> t)=\prod_{i\leq t} (1-h(t)).$
### Randomized Controlled Trials
We have people $i$ with treatment applied or unapplied $X_{i}$ and reward distribution $C_{X}$ for each person. Aim: general effect of application
$$\theta=\mathbb{E}[C_{1}] - \mathbb{E}[C_{0}].$$
However for each sample we only get to see $C_{X},$ so if we just make $X$ independent of the "person" (since $C_{i}$ is dependent on the person) then we get
$$
\theta=\alpha:=\mathbb{E}[C_{1}\mid X=1] - \mathbb{E}[C_{0}\mid X=0].
$$
### Surveys
We want to know $T:=\sum_{i}Y_{i}$ over the entire body, but can only grab $\sum_{i\in S}Y_{i}$ for some random $S$. Then the Horvitz-Thompson estimator is
$$
T^{HT}:=\mathbb{E}\left[ \sum_{i\in S} \frac{Y_{i}}{\pi(i)} \right]=\sum_{i} \frac{Y_{i}}{\pi(i)}\Pr(i\in S)=\sum_{i}Y_{i}.
$$
Can evaluate the variance in theory using $\pi_{ij},$ then apply Horvitz-Thompson on this by dividing by $\pi_{ij}$ again.
### Classification
Given inputs $X$ want to classify into labels $Y\in \left\{ 0,1 \right\}.$ We find a "Bayes estimator"
$$
h:=\arg\max_{h} \Pr[Y \neq h(X)].
$$
Equivalently, $h=1$ if $r(x):=\mathbb{E}(Y|X=x)\geq \frac{1}{2}$. Can also do if $\frac{\Pr(X=x|Y=1)}{\Pr(X=x|Y=0)} \geq \frac{{\Pr(Y=0)}}{\Pr(Y=1)},$ hence called Bayes.

In high dimensions naive KDE isn't great, so can either
- naively partition input dimensions into independent subsets
- assume $X|Y=i$ are Gaussian, obtain a quadratic classifier
Can also classify by mean of $k$ nearest neighbors, can be represented using KDE (look at "effective range" of each node).
