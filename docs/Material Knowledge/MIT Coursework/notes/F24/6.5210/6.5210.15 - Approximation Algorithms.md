For some optimization problem, we want to get within $\epsilon$ of the true answer, or within ratio of $[(1-\epsilon)O, (1+\epsilon)O].$
## Greedy Algorithms
Often we can get some $C$-approximation (frequently 2 or 3/2) by just making some simplifying assumption, doing things greedily, and then double- or triple-counting to show that our greedy construction is not that much worse than opt.
> [!important] Matching
> Classic example. Just take heavy edges greedily; 2-approximation to opt.

## Scheduling Theory
We have tasks to be completed by machines in parallel, under certain constraints, with certain objective.
### Notation
`machines` | `constraints` | `objective`

## FPAS, PAS
We want to get within $\epsilon$ of error. PAS means for any fixed $\epsilon$ we can get within $\epsilon$ relative error in $n-$polytime. FPAS (fully polynomial approx. scheme) means we can get within $\epsilon$ relative error in $n, \epsilon^{-1}$ polytime.

One common paradigm (for knapsack, bin packing, etc.) is to round item sizes, task times, etc. to integers, and solve with DP. Then runtime is in item size, so we must scale items to a certain function of $n, \epsilon^{-1}$ and round to integers, in order to get good relative error $\epsilon$ and poly runtime.

This general strategy of taking some optimization problem, rounding, solving is called "relaxation".

We have several examples:
### Traveling Salesman
### LP
### Vertex Cover
### Facility Location
### Max SAT
