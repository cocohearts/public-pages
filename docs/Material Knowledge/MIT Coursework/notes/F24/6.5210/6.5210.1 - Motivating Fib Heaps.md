## Shortest Paths, Minimum Spanning Tree
- Kruskal's: sort edges, use union data structure to collect edges
- Prim's: grow tree by keeping PQ of could-be-added edges approx. Dijkstra
- Boruvka's: parallelize by taking minimum edge out of each established component at the same time (start with all nodes as components)
Now we want a *priority queue* to track edges. 
Reminder: heap means f(node) < f(child) for all children

For MST we know in advance the tradeoff between different operations, so we can adjust arity of the heap suitably. Call this a $d$-heap.
#### Laziness paradigm
> Don't work until you need to. When you work make it maximally efficient.
> Work in a way that enables future lazy work to be easy.

Alternatively, create an algorithm where an "adversary" must also work (contrive) a bad counterexample to make you work.
We care about "amortized" randomized performance (*not* worst-case anymore).
For laziness we define a "potential" function that measures the amount of lazy work that needs to be done.

Now let's do min deletion lazily. We go through the whole list from left to right, pick up our min and write down everything—then we get this comparator tree. To balance this tree we run a divide-and-conquer.

Now we call these trees "heap-ordered trees"—somewhat balanced trees that satisfy heap property. On insert, just add HOT(n). On deletion, compare all the roots and consolidate, then cut the head and get a collection of HOT again.

Finally, we choose to only compare roots with equal degree $\to$ subtrees have size $2^{k}$. Then we have buckets by subtree size, and only compare/upgrade within the same bucket.

We use the potential to "amortize" the cost by picking $\phi$ such that $c_{a}=c + \Delta \phi$ so that $c_{a}$ is worst-case not that bad. Then we set $\phi$ to size of our collection. Then 