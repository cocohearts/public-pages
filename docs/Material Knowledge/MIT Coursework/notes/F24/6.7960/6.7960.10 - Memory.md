We can "convolve across time," *or* we can just have some "time-dependent hidden variable." We take some "update" function where new hidden variable takes in old hidden and current input. For gradients of shared parameters, the "real" gradient is computed by summing gradients for each of the original values.

However, if we want memory that lasts for a long time, we need to look at really long gradients, but these can be noisy.

There's this old hand-crafted LSTM model, where the cell state
- first multiplied by sigmoid of some linear map (forget/remember)
- grab some new information (tanh) and consider what to forget/remember (sigmoid) and then add to cell state
- finally, tanh(cell state) and sigmoid again
Because sigmoid is used as multiplicative "filter" function for memory, default is 1, i.e. remembering.

## Long Context
This is the better, newer, more modern form of memory. Sparsifying the attention matrix to make memory better.
## Videos
## Types of memory
We have parameters that are learned (slow memory), statistics of the dataset.
We also have activations that are evaluated (fast memory), statistics of the data point.

There are weird intuitional things going on here, e.g. hypernets, or whatever.

## Summary
Memory is solved with
- CNNs for sequences
- RNNs for recurrent memory
- LSTMs (old technology) for remembering stuff
- Sequence models (attention) for selective analysis of sequences