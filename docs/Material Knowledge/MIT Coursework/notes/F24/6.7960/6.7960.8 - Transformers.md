> [!warning] Issue
> We want to get some *sparse, global* index. E.g.:
> - MLP that doesn't look at everything
> - CNN that looks at stuff across the image
> How can we achieve this? Variable *attention*.

## Idea #1: Tokens
First, we redefine a *token* as a bundle of vectorized data.
> [!important] Token Intuition
> A vector of scalars that describe information about one *single* object (e.g. blob of audio or patch of pixels).

At the input layer, we cut our data (images, text, sound) into pieces and map each piece into a token using a "domain expert". But then tokens can be combined in this transformer system.
### Structure
Similar to a fully connected GNN, we want all of our tokens to talk to each other, so:
- first `AGGREGATE` (linear combination of tokens using *attention*)
- then `NORM` (layernorm or wtv.. why is it the right thing to do ??)
- finally `UPDATE` (some learned MLP or other "token-wise nonlinearity" applied to each *token*)
## Idea #2: Attention
We don't know which tokens we actually *care about*. We use a query-key-value system to figure out which tokens we care about.
> [!important] Query-Key-Value
> - One single *Query* vector corresponds to "what's the question"? Dotted with the
> - *Key* vector generated from each token, determines all the *similarity* values, encoded in a vector $s$. The $s$ values are used to take a weighted average of
> - *Value* vectors, which after summed give us the final output token.

### Self-Attention
One particular strategy is to have our query come from the input tokens.

> [!important] Self-Attention
> - Learn *$W_{q},W_{k},W_{v}$* which returns query, key, and value for each token
> - Get all queries, all keys, all values by *just three matmuls* (since all tokens are stacked row-wise in matrix $T_{in}$)
> - Attention matrix $A$ obtained by
> 	- *Attention* (pairwise dots between keys and queries)
> 	- *Normalization* (division then softmax)

Now have covered three linear layers: MLP, CNN, Self Attn.
### Extra Tricks
Skip connections, multi-head self atten (parallelizing). Hardware loves matmuls. Autoregressive token prediction/log likelihood loss for prediction models.
## Idea #3: Positional Encoding
Various ways to do, e.g. learned or hardcoded, but we can do Fourier basis:
$$\sin(x), \sin\left( \frac{x}{B} \right),\sin\left( \frac{x}{B^{2}} \right),\dots$$
Domain expertise is crucial here: human engineers apply inductive bias here.