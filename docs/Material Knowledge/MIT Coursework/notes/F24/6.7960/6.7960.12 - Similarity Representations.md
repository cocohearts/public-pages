Last time, we tried to learn representations by "forcing" them through encoder/decoder and constraining output to be close to input.

New framing: *contrastive learning*, i.e. learn some distance metric.
- Map data points to element on hypersphere $S^{d-1}$
- Pick an *anchor point* and consider some set of *positive* (similar) examples and *negative* (dissimilar) examples
For supervised learning, we can have classes and learn a softmax classifier, or assign similarity scores by RL.

Alternatively, for *self-supervised* contrastive image representation learning:
- augment data samples for positive
- everything else is negative
and this generates representations that put similar things together!
Can also do co-occurence; e.g. get RGB images and depth images to return similar representations

### Mathematical analysis
A couple sample loss functions:
- Triplet loss: taking the farthest positive and closest negative example, grad descent on these distances
- InfoNCE: taking one positive and all negatives, treat as softmax classifier with logits corresponding to distance to examples

We care about *alignment* (putting similar stuff together), *separation* (putting farther stuff apart), *uniformity* (having representations spread uniformly through $S^{d-1}$). The loss function does (in theory) regularize for this stuff.

Contrastive learning is also more "descriptive" than classifier learning because it's more expressive (has distance)