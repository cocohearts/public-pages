## Families
### Exponential families
Usually used over discrete probability spaces, but also over continuous. Basically if we just ignore normalization and take geometric means across distributions. Yields an affine subspace in log-prob space. This is the "basis" interpretation. The parameter set is $\mathbb{R}^{n},$ and an $n-$parameter family of distributions is
$$
\exp \left\{ \left(   \sum_{i}\lambda_{i}(x_{i})t_{i}(y) \right) - \alpha(x_{i})+\beta(y)\right\} .
$$

Here $\alpha$ is *log-partition function*, $t$ is the *sufficient statistic*, $\lambda$ is *natural statistic*, and $\beta$ is *log base function*.

The single-parameter case has some nice properties; partial in $\alpha$ is $\mathbb{E}[t(y)],$ and $\ddot{\alpha}=\text{var}\left[ t(y) \right]=J_{y}(x).$
### Linear families
We can also take the linear nullspace interpretation. Take some indicators $\lambda,$ then $\mathbb{E}_{q_{x}}[ p(x)]=e_{0}$ becomes a linear constraint in the probability space.
## Entropy / Mutual Info
We make choices. For instance most standard inference uses log-loss cost; we get a distribution for the parameter, and cost is log-prob of true value. E.g. for Bayesian we use $-A \log q(x)+B(x)$ where $A=1$ and $B=-\log p_{prior}.$

A cost (input truth, distribution) is *proper* if argmin is at the true Bayesian posterior. Meanwhile it is *local* if cost only depends on $x,q(x).$

Log-lsos turns out to be the only smooth, local, proper cost function. If we introduce discontinuities then uniform can do ok.

Here we can define entropy $H(p)=\mathbb{E}[-\log p]$, joint uncertainty $H(x,y)=\mathbb{E}_{p_{xy}}[-\log p],$ and mutual information $H(x)+H(y)-H(x,y),$ i.e. how much we learn about $x$ once we know $y$.

Always remember
$$
I(x;y,z)=I(x;z)+I(x;y|z).
$$
Note also that if $s$ is deterministic func of $x$ then $H(x)=H(s)+H(x|s).$ Here conditional entropy is weighted over joint distribution. Alternatively, think of it as sampling the conditioning variable $y$, then computing expected entropy over all resulting parameterized distributions $p_{x}(x;y).$

Hence recall that $$H(x|y)+I(x;y)=H(x),H(y|x)+I(x;y)=H(y), H(x)+H(y)-I(x;y)=H(x,y).$$
Recall that we can also do entropy for continuous variables. It's just defined as $\int_{-\infty}^{\infty}p(x)\log p(x)\,dx.$ It changes on coordinate shifts, but the value change is only a function of the transformation. Letting $x=g(s),$ we get $p(x)=\frac{p(s)}{\dot{g}(s)}$ so 
$$
\begin{align*}
h_{p,q}(x)
&=-\int p\log q(x)\,dx\\
&=-\int \frac{p(s)}{\dot{g}(s)}\log \frac{q(s)}{\dot{g}(s)}\,\dot{g}(s)ds\\
&=\int p(s)\dot{g}(s)\,ds-\int p(s)\log q(s)\,ds\\
&=h_{p,q}(s)+\mathbb{E} [\dot{g}(s)].
\end{align*}
$$
Hence *all relative entropies, including KL and mutual info, are independent of variable changes.*
## Info geometry
Now we define a "distance," the KL/information divergence $D(p||q)=\mathbb{E}_{p(x)}[-\log q(x)+\log p(x)].$ Can be called "relative entropy" of $q$ to $p,$ or thought of as the entropy of the odds $\frac{q}{p}$ when $p$ is truth. Note that $I(x;y)=D(p_{xy}||p_{x}p_{y}).$

Note that the curvature of information divergence as parameterized by $x$ is determined by Fisher info. Specifically,

$$
\frac{\partial^{2}}{\partial x^{2}} D(p_{y}(y;x_{0})||p_{y}(y;x))=J_{y}(x).
$$
(Add a $\log e$ for bit-based systems.) Just rewrite as $\mathbb{E}_{p} \log p_{y}(y;x)- \mathbb{E}_{p} \log p_{y}(y;x_{0})$ and we immediately get $\mathbb{E}_{p} \frac{\partial^{2}}{\partial x^{2}}\log p_{y}(y;x)=J_{y}(x).$

Now we can use KL divergence to look at distances between distributions, geometry in distribution space, etc. As from [[6.7800.3 Info theory#Exponential families]], normalized distributions for an affine space because of sum to 1, and un-normalized exponential families are affine spaces in log-prob space.

The "projection" of $q$ onto a set $P$ is the "ground truth" $p$ that minimizes $D(p||q).$ $P$ is the set of hypotheses, and we want to find the one that "most easily explains" the data $q.$ Let $p^*$ be this projection. Take any other $p,$ then by walking from $p^*$ to $p$, derivative of $D(p||q)$ must be nonnegative.

We get $\sum_{y}(p-p^*) \log \frac{p^*}{q}=D(p||q)-D(p||p^*)-D(p^*||q)$ i.e.

>[!important] Pythagoras', Info Version
>$$
>D(p||q)\geq D(p||p^*)+D(p^*||q)
>$$
>for all $p \in P.$

In other words, when trying to go $p\to p' \to q,$ always pick $p'=p^*$ i.e. greedy for the second step.

When $P$ is a [[6.7800.3 Info theory#Linear families]] we get equality, since we can walk either backwards or forwards and so the derivative must be 0.

Finally, we can precisely describe the set whose projection to a linear family is $q.$ Note that $$D(p||q_{2})=\mathbb{E}_{p} \log \frac{q}{p}+\log \frac{q_{2}}{q}=D(p||q)+\mathbb{E}_{p}\log \frac{q_{2}}{q}.$$
Hence if $\mathbb{E}_{p} \log \frac{q_{2}}{q}$ is independent of $q$ then the projection remains the same. Equivalently $\log \frac{q_{2}}{q}$ must be some linear combination of $t_{i}.$

>[!important] Orthogonality Correspondence
> If the linear family has constraints $\mathbb{E}_{p}[t(y)]=\overline{t}$ for some vector-valued function $t,$ then the set orthogonal to fixed $p^*$ is the exponential family $p^*(y)\exp \left\{ x^{\dagger}t(y)-\alpha(x) \right\}$ for free vector parameters $x.$

We can also represent EM algorithm in info geometry. The $E$ step becomes
$$
\hat{p}_{z}^{(l-1)}=\arg\min_{p_{z}} D(p_{z}||p_{z}(\cdot,\hat{x}^{l-1})),
$$
$$
\hat{x}^{l} = \arg\min_{x} D(\hat{p})
$$