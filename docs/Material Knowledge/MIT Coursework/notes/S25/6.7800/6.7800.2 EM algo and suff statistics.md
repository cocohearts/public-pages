## Sufficient statistics
Defined by a Markov chain, or by a conditional independence, or by equality in a Bayesian setting (i.e. conditional independence).

Given $x$ parameterizing a model that generates data $y,$ we say $s(y)$ is sufficient iff. $\mathbb{P}(x|y)\equiv \mathbb{P}(x|t(y)).$ Equivalently:
- the model $p_{y|t;x}$ is identical for all $x$ i.e. regardless of $x$ the statistic determines $y$
- a single $t$ bunches multiple equivalent $y,$ i.e. $$ p_{y}(y;x)=a(t(y),x) \cdot b(y) $$ for some $a,b$ (Neyman Factorization). Use the independent $x$ definition. We get $a=p_{t|x}(t(y)|x)$ and $b=p_{y|t}(y|t(y)).$
- $p_{y|t,x}(y|t(y),x)=p_{y|t}(y|t(y))$ (Bayesian interp); by inverting we get $p(x|y)\equiv p(x|t=t(y))$ i.e. all info we gain about $x$ from $y$ we also get from $t$
This last one is a little nontrivial and needs rewriting a lot of conditionals.

We also have degree of tightness. In particular, a statistic is sufficient iff. equivalent inputs yield equivalent inferences about $x.$ It is minimal if it can be mapped to from any sufficient statistic, i.e. nonequivalent inputs map to different inferences.
## EM algo
Suppose we have some complex process with parameters and underlying latents. Without knowing the latents, the parameters are correlated in weird ways depending on the prior on the latent.

If we want to model the parameters, might as well have a prior on the latent. Then:
- start with some random guess for parameters $x$
- compute posterior for the latent (conditional on guess)
- revise estimate for $x$
This is justified since the "pseudo-log likelihood"
$$ \mathbb{E} \left[ \log p(y,s;x) \right] - \mathbb{E}\left[ \log q(s|y) \right] , $$
lower bounds log likelihood. Then the $E$ step updates $q$ and the $M$ step picks $x$ maximizing the first term.

Here $q$ is actually $p_{s|y;x_{0}}.$ Equality happens when $p_{s|y;x}\equiv q_{s|y;x_{1}}$ i.e. when the $x$ estimate converge.