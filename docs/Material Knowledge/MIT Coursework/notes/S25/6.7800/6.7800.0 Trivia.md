Jensen's
Log is concave down

Applying log with weights $b_{i}$ to the values $\frac{a_{i}}{b_{i}}$ yields
$$
\sum b_{i}f\left( \frac{a_{i}}{b_{i}} \right) \geq \left( \sum b_{i} \right) f\left(  \frac{{\sum a_{i}}}{\sum b_{i}} \right).
$$
Letting $f=x \log(x)$ yields
$$
\sum a_{i} f\left( \frac{a_{i}}{b_{i}} \right) \geq \left( \sum a_{i} \right)  f\left( \frac{{\sum a_{i}}}{\sum b_{i}} \right).
$$

Gibbs tells us that KL divergence is nonzero, i.e. entropy is always greater than cross-entropy.