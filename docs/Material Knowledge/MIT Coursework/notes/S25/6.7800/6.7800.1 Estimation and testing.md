## Decision theory
Start with typical estimation. Recall the typical stats flow:
- (with unparameterized model) data -> parameters -> error
- (with hypothesis) data -> parameters, error -> probability, decision
MLE just takes parameters with highest likelihood. MAP is the maximum after a Bayesian update, i.e. generalization of MLE with non-uniform prior.

Given some model, minimax decision theory gives you an optimal estimator (data -> parameter function) by taking whichever estimator minimizes the expected cost when nature gets to pick arbitrary parameter.
## Hypothesis testing
Also remember likelihood ratio tests, used for binary hypothesis tests. Any Bayesian prior and cost matrix can be represented as: $H_{1}$ iff
$$
\frac{\mathbb{P}(x|H_{1})}{\mathbb{P}(x|H_{0})} \geq \lambda
$$
or something like that. Recall $P_{F},$ the false alarm probability ($H_{1}$ when $H_{0}$) and $P_{D},$ detection probability ($H_{1}$ when $H_{1}$).

Neyman-Pearson says for fixed $P_{F},$ best possible $P_{D}$ is obtained with an LRT. Let set of $x$ such that we pick $H_{1}$ be $S$.

Note that fixed $P_{F}$ means $\sum_{x \in S} \mathbb{P}(x|H_{0})=P_{F},$ i.e. $\sum_{x \in S} \mathbb{P}(x|H_{0})$ is fixed. Maximizing detection means maximizing $\sum_{x \not\in S} \mathbb{P}(x|H_{1})=1 - \sum_{x \in S}\mathbb{P}(x|H_{1}),$ i.e. minimizing $\sum_{x \in S} \mathbb{P}(x|H_{1}).$

Then clearly we want the best "bang for our buck" i.e. just take the highest-ratio stuff.

Hence $P_{D}$ as a function of $P_{F}$ must be concave, since any point on line segment between two frontier points is a mixed strategy, which cannot be better than the corresponding LRT.
## Least-squares and linear estimation
Estimators of data have *bias* and *variance*. Bias can be defined for a given parameter value (expected diff of expected estimate from truth) or globally (over all parameters using a prior). Estimator is *unbiased* if bias is $0$ for all parameters.

Given a cost $C$ we get the corresponding estimator that minimizes $C$ over the posterior. Least-squares is known to be mean of the posterior, and $L_{1}$ known to be median. Note also that estimator is least-squares iff error is "orthogonal" to any function of the data, as a random variable.

If we have a linear model with fixed-variance noise, then the MLE estimator is exactly the LLS estimator. The model $x=My+b+\epsilon$ comes with an "ideal model" $\hat{x}=My+b.$ (Here $x$ is not necessarily scalar.)

Shift all $x$ and $y$ to be mean $0$, then we get $b=0.$ Then we're minimizing $(My-x)(My-x)^{\dagger},$ so taking partial in $M$ yields $(My-x)y^{\dagger}=0.$ Equivalently $M\lambda_{y}=\lambda_{xy}$ and so $M=\lambda_{xy}\lambda_{y}^{-1},$ and
$$
\hat{x} = \lambda_{xy}\lambda_{y}^{-1}(y-\mu_{y})+\mu_{x}.
$$
Note that RHS has mean 0 and correlation $\lambda_{xy}$ with $y,$ exactly mimicking the mean and correlation of $x$ with $y$.

It turns out that if we only know second-order moments of $x,y$ then minimax solution for $\hat{x}(y)$ is the unique joint Gaussian with the given mean/moments, and our estimator is the LLS. This is because the joint Gaussian is kind of the "widest" nature can spread the joint, and BLS estimator for joint Gaussian is linear.
## Cramer-Rao
Fix some $x,$ and consider the resulting distribution over $y.$ Take any unbiased estimator $\hat{x}.$ Then consider the "rate of change of log likelihood" $S(y;x)=\frac{\partial}{\partial x}\ln p(y;x)$, and then consider its second moment $J_{y}(x)=\mathbb E [S(y;x)^{2}]$. This is a measurement of the spikiness of the distribution $p.$

Then note that the covariance of the random variables
$$
\begin{align*}
\text{cov}(e(y),S(y,x))
&= \int p(y;x)(\hat{x}(y)-x)\frac{\partial}{\partial x}\ln p(y;x)\, dy \\
&= \int (\hat{x}(y)-x) \frac{\partial}{\partial x} p(y;x) \,dy\\
&= \frac{\partial}{\partial x}\int \hat{x}(y)  p(y;x) \,dy - \int x \frac{\partial}{\partial x} p(y;x) \,dy\\
&= 1.
\end{align*}
$$
Then
> [!important] Cramer-Rao
> $$
> \mathbb V (\hat{x})\leq 
> \frac{\text{cov}(\hat{x}(y)-x,S(y,x))}{\mathbb E [S(y;x)^{2}]}=\frac{1}{J_{y}(x)}.
> $$

Most of the time we can assume $\int p''=0,$ i.e.
$$
\begin{align*}
0
&=\int p''\\
&=\int p\left( \frac{p''}{p} -\frac{p'^{2}}{p^{2}} + \frac{p'^{2}}{p^{2}}\right)\\
&= \int p \left(   \frac{\partial^{2}}{\partial x^{2}}\ln p +\left(   \frac{\partial}{\partial x}\ln p\right)^{2} \right)\\
&= \mathbb E\left[  \frac{\partial^{2}}{\partial x^{2}}\ln p\right]  + \mathbb E \left[  \left(   \frac{\partial}{\partial x}\ln p\right)^{2}\right] .
\end{align*}
$$
Hence $J_{y}(x)=\mathbb E\left[  \frac{\partial^{2}}{\partial x^{2}}\ln p\right].$

For every joint distribution there exists at most one minimum-variance unbiased estimator $\hat{x}_{MVU}(y).$ It is unbiased for all parameters $x$ and has lower variance for its estimate than any other unbiased estimator.

We call an estimator "efficient" if it's tight with Cramer-Rao. Equality case is when error exactly matches up with $\frac{S}{J_{y}(x)}.$

Corollary of this tightness is that ((Gauss-Markov Theorem)) smth.

At this point recall linear and logistic regression.