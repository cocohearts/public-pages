Modeling is fundamentally different from parameter estimation. We can't simply take our best parameter estimate and then apply it to a prediction, because it's a different loss we care about. For parameter estimation we use MSE or log loss; for modeling we want to use KL. This is the key idea.
## Minimax Modeling
Let nature choose the true parameter $x$. Hell, let them choose the prior $p_{x}$ parameterized by $w(x).$ Then we want the model $q$ defined by
>[!important] Minimax Decision Theory: Modeling
> $$
> \arg \min_{q \in \mathcal P^{\mathcal Y}}\max_{w \in \mathcal P^{\mathcal X}}\sum w(x)D(p_{y}(y;x)||q).
> $$

Turns out that since $q$ and $w$ are both continuous, and $|\mathcal X|$ is finite, we can apply a saddle theorem to get the maximin. Specifically,
$$
\begin{align*}
\min_{q \in \mathcal P^{\mathcal Y}}\max_{w \in \mathcal P^{\mathcal X}}\sum w(x)D(p_{y}(y;x)||q)
&= 
\max_{w \in \mathcal P^{\mathcal X}}\min_{q \in \mathcal P^{\mathcal Y}}\sum w(x)D(p_{y}(y;x)||q).\\
\end{align*}
$$
We denote constant shifts using $\equiv.$ The inner term is just
$$
\sum_{x}w(x)\sum_{y} p(y;x) \log \frac{q(y)}{p(y;x)}\equiv \sum_{x,y}w(x)p(y;x)\log q(y) \equiv \sum_{y} p_{w}(y) \log \frac{q(y)}{p_{w}(y)}=D(p_{w}(y)||q(y)),
$$
where $p_{w}(y)$ is the "aggregate" posterior on $y$ given prior $w(x).$ Hence best $q$ is just $p_{w},$ yielding inner term $\sum_{x,y}p_{w}(x,y)\log \frac{p_{w}(y)}{p(y;x)}.$

Then we pick $p_{x}$ and $p_{y|x}$ to maximize this, i.e.
$$
\begin{align*}
\sum_{x} p(x) \sum_{y} p(y|x) \log \frac{p(y)}{p(y|x)}
&=\sum_{x,y} p(x,y) \log \frac{p(x)p(y)}{p(x,y)}\\
&= D(p_{xy}||p_{x}p_{y})=I(x;y).
\end{align*}
$$
Hence the best $q$ for $y$ is that parametrized by $x$ in such a way that minimizes $I(x;y)$ over all possible joint distributions. This is called the *least informative prior* $p^*(x,y)$ and the *capacity* is $I_{p^*}(x,y)$.

>[!important] Equidistance Property
> The optimum mixture model is equidistant from all candidate models whose parameters have nonzero prior.

Since the optimization space is $w(x),$ if the prior is nonzero that means partial in that prior (subject to a Lagrange multiplier) must be $0.$ If the prior is zero then the partial is negative. Evaluating the partial $\frac{{\partial I(x;y)}}{\partial p_{x}(a)}$ yields $D(p(y|x=a)||p_{y})-1,$ so combined with the Lagrange multiplier we get the desired equidistance. Finally because of the bound, the equidistance must be exactly equal to the cost $C.$
### Inference
Whereas normal MLE->parameter->model flow assigns a "hard" weighting $w(x)$ for the prior, with mixture models, after seeing data you just use $w_1(x) \propto w_{0}(x)L_{y}(x)$. Back to Bayesian!
### Picking a prior
We want *maximum entropy distributions*, i.e. projections from uniform distribution. Just apply the info geometry results to generate an exponential family from the uniform.
## Conjugate Priors
they exist lmao. idfk what do you want