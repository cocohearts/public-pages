---
dg-publish: "true"
tags:
  - perm3
---
## reasoning
- [ ] [bytedance dapo](https://arxiv.org/pdf/2503.14476)
- [ ] [Large Memory Layers with Product Keys](https://arxiv.org/pdf/1907.05242)
- [x] [intuitor](https://arxiv.org/pdf/2505.19590)
	- same thing as grpo (rl with verified rewards), except use confidence (average negative log-likelihood) instead of entropy (expected negative log-likelihood)
	- importantly, confidence is "mode-seeking" instead of "mode-covering"
## sys/inference optimization, large models
- [ ] [prime](https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f)
- [ ] [flashinfer](https://arxiv.org/abs/2501.01005)
- [ ] [olmo](https://arxiv.org/pdf/2501.00656)
- [ ] https://huggingface.co/spaces/nanotron/ultrascale-playbook
- [ ] thunderkittens
- [ ] [nvidia whitepaper](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)
- [x] claude 4
	- improved coding, with better problem solving
	- MCP, with integrations into Asana, gdocs, gmail, github, etc.
	- memory files, longer context problem solving
- [ ] [qwen2.5-omni](https://arxiv.org/pdf/2503.20215)
- [ ] pluralis papers
	- [projection compression](https://arxiv.org/pdf/2506.01260)
		- because of adamW, low-SV vectors are depressed at train-time anyways
		- PP break happens after each down-projection, so we only need to communicate activations (forward) and their gradients (backward)
		- activations and gradients can be compressed (cuz they came out of down-projection)
		- activations are broken into
			- post-attention (per block)
			- post-ffn (per block)
			- TE and PE (init)
		- have shared low-rank subspace (shared $UU^{\dagger}$) for all down-projections
		- update subspace every 500 steps diloco-style using a loss penalizing non-orthogonality (Grassman manifold)
	- https://openreview.net/pdf?id=HxPDWbaK1E
	- https://openreview.net/pdf?id=4O8nzTkHPI
	- https://openreview.net/pdf?id=UyPDg7ksTM
- [ ] tencent t1
- [ ] qwen 2.5 omni
- [ ] gemma
- [x] prime intellect's PRIME / DiLoCo
	- DiLoCo: distributed low-communication training, where each island does its own opt; inner is usual AdamW (or Muon), outer is Nesterov momentum
	- Outer optimization happens once every 500 steps or so
	- Used to do pretraining for INTELLECT-1
- [x] prime intellect's distributed RL
	- permissionless service using "inference provider" and "relay nodes" using verification with random sampling, final layer logit hashes, etc. various statistical checks to ensure larger model is being sampled correctly
	- per-island backprop as usual.
- [ ] grok 1
- [ ] siglip
- [x] [flashattention](https://arxiv.org/abs/2205.14135)
	- want to focus on minimizing i/o writes to and from gpu vram
	- the goal is to do chunked attention QV muls in the cache close to the gpu
	- make multiple passes over the output, each time updating by adjusting softmax denom and adding new term
	- obv. keep running log of the softmax denom and have to re-read on every block
	- prefetch/kernel fuse as desired
- [x] [olmo](https://arxiv.org/pdf/2501.00656)
	- training curriculum with general text/fineweb-edu, then text for specific tasks to push benchmark scores
	- stability tricks: Z-score regularization, gradient clipping, KV-norm, RMSnorm, no biases, model soups, careful model init, low lr (1e-8), post-norm, weight decay, dropout
- what i learned from william brandon:
	- nvidia gpus have two caches, L2 and L1
	- each streaming multiprocessor has its own L1 physically colocated with the SM
	- in the time it takes to send data between L1 and SM, can do 100 tensor operations
	- four datatypes to be aware of:
		- fp32, horrible; tf32, better than bf16 but takes more memory
		- fp16, really not great; bf16, what everyone uses
	- bf16 (if not careful) will get converted into fp32 which is horrible
	- V100s are horrible for the above reason
	- also talked about modular duality, lipschitz stuff from jeremy + laker newhouse, new optimizers
	- life advice: do stuff, make stuff, remake primitives from scratch, when possible if i'm scared of something just do it
	- also learn cuda in 100hrs in [6.S894 labs](https://accelerated-computing-class.github.io/fall24/labs/) (use 1xRTX A4000)
- [ ] ray python dist package
- [ ] deepspeed
- [ ] megatron
- [ ] [jetformer](https://arxiv.org/abs/2411.19722)
- [ ] ddp, fsdp, megatron, gpipe
- [ ] [what to knwo about cpu memory](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)
- [ ] flexattention
- [x] streamingllm, linformer
	- linformer just projects $n\times d$ key, weight matrices into $k\times d$ using direct linear $n\to k$ linear map ("squishing" vectors to make the seq shorter)
	- streamingllm analyzed attention magnitudes, found that "leftover" attention would be dumped into "attention sinks" in the softmax which would then be ignored
	- added initial tokens back to inference, perplexity fixed (compared to sliding window)
- [ ] [speculative decoding](https://arxiv.org/pdf/2211.17192)
- [ ] [yarn](https://arxiv.org/pdf/2309.00071)
	- uses RoPE (instead of adding pos. embedding, multiply by $D/2$ complex numbers with AP magnitude)
	- pretrain on 10K context/RoPE, then interpolate arguments of complex numbers in pos. embedding carefully
	- preserve higher frequency numbers and move lower 
- [ ] [lolcats](https://arxiv.org/pdf/2410.10254)
- [ ] deepseek v3
	- MTP, Yarn, "long-winded and overly reflective" R1 with system prompt during SFT for transfer, carefully load balanced MoE
- [x] [hymba](https://arxiv.org/pdf/2411.13676)
	- combine SSM and transformer *side by side* as opposed to in sequence, has good "synergy" because of opposing behaviors
	- Transformers arch with LN, LinUp, split SSM/Attn, LinDown, (+), LN, FFN, (+)
	- (N-3)/2 SWA Hymba blocks on each side of global attention
## chips
- [x] [tpus](https://henryhmko.github.io/posts/tpu/tpu.html)
## interp
- [ ] [Patchscope: A Unifying Framework for Inspecting Hidden Representations of LLMs](https://arxiv.org/pdf/2401.06102)
- [ ] [Improving Alignment and Robustness with Circuit Breakers (s/o rowan for coauthoring)](https://arxiv.org/pdf/2406.04313)
- [ ] [Quantization Model of Neural Scaling (s/o Uzay)](https://arxiv.org/pdf/2303.13506)
- [ ] [Gradient Routing (s/o Jacob)](https://arxiv.org/pdf/2410.04332)
- [ ] [Binlear MLPs for Interp](https://openreview.net/pdf?id=gI0kPklUKS)
- [ ] [Recovering the Pre-Fine-Tuning Weights of Generative Models](https://arxiv.org/abs/2402.10208)
- [ ] [cot monitoring](https://openai.com/index/chain-of-thought-monitoring/)
- [ ] [automated circuit discovery](https://papers.nips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf)
- [ ] [path patching](https://arxiv.org/pdf/2304.05969)
- [ ] [the logit lens (blogpost)](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
- [ ] [nnsight docs](https://nnsight.net/about/)
- [x] [achyuta's sts paper](https://arxiv.org/pdf/2404.14349)
	- uses cross-layer-attribution matrix based on gradient of next layer on each neuron in current
	- take top $k$ in each layer locally, then iteratively refine by taking last layer, grab most influential neurons from prev, etc. go backwards, then go forwards, until circuit stops changing
	- stich images from two classes into a single image, analyze behavior of both circuits under this hybrid image
**FROM TAMAR**
- [x] [https://anishk23733.github.io/vl-interp/](https://anishk23733.github.io/vl-interp/)  
	- VLM embeds images and places before text in transformer decoder
	- hence image embeddings are in the same "space" as text embeddings
	- goal: measure "internal confidence" about an object in an image, potentially ablate
	- defines "internal confidence" about an object in an image as maximum of logit lens (result of applying final unembedding matrix and grabbing logit for token for that object) over all image embeddings (one for each patch) over all layers in the VLM
	- cutoff at a certain internal confidence, and then ablate/activate based on that
	- questions:
		- why are all layers in the model treated equally? can we learn a better combination of information to get internal confidence in a self-supervised way?
		- what about image encoders that don't keep spatial information in an easily interpretable way, how to check segmentation for these?
		- logit lens seems "naive"—why do we expect image tokens to speak "exactly" the "same language"? should we try to learn adjustments to the embedding matrix, esp. wrt. position?
		- good results for end task—hallucination reduction—but gap between hallucation reduction v.s. cd reduction seems small, perhaps can be made more effective with ablation engineering or internal confidence engineering
- [x] https://vision-of-vlm.github.io/ 
	- "the mechanisms underlying how VLMs process visual information remain largely unexplored."
	- through a creative "knockout" experiment, demonstrate implicit image information through text tokens is sufficient, alternatively image tokens only needed for layers 20-40
	- in layers 20-40 attention values make an attempt at segmentation
	- able to get "compressed context" by, for each layer, take only the image tokens with top-K attention values
	- notably works much better with InternVL2-76B (patches into 40x40) than with LLaVa-1.5-7B (patches in 16x16), suggesting that large patching is just very redundant/inefficient and should no longer be pursued
	- question: if we take the best tokens for *each* layer then perhaps over all layers we end up including all image tokens at least once? even worse, if we change which image tokens are included on inference for each text token, perhaps over all text tokens, each layer eventually sees all image tokens? also perhaps these results are to be "expected" because of dropout training schedules?
- [x] [https://arxiv.org/pdf/2410.07149](https://arxiv.org/pdf/2410.07149)
	- adapter VLM that maps CLIP patch embeddings into language token space
	- refers to a register token hypothesis—that blank background patches collect global information; relegates as a side effect
	- uses three methods of checking token understanding, finds that ablating object tokens (replacing with global mean) removes understanding—expected; how to know there's a dog when there are no dog patches?
	- uses logit lens, finds that tokens become aligned with corresponding text embeddings as layers progress (surprising! no compulsion to align with text, some implicit discovery of "best coding" for each patch)
	- most valuable communication between image and text tokens happens in later layers (attention knockout)
	- i'm surprised they discard register hypothesis so quickly, i believe register
- [x] [https://arxiv.org/abs/2310.10348](https://arxiv.org/abs/2310.10348)
	- general program of ACDC: Automated Circuit DisCovery
	- activation patching does one forward pass for every edge/node/wtv. that has to be tweaked, replacing activation at that node with either than average ablation or the activation from some other "corrupted" prompt, sees what the change in metric is
	- attribution patching uses derivative of metric in activation at each node/edge to construct a circuit
	- represent network as a computational graph, differentiate between modules, nodes, edges
	- specifically use EAP, edge attribution patching
	- measure false positive rate/true positive rate from human expert baseline, both increase as %pruning increases
	- two approaches to calculating relevance: take a gradient all the way to the top, or take gradient of next layer(s) from this layer and compute recursively
	- recursive is easier to ensure selected subgraph is actually connected

- [ ] [A Mathematical Framework for Transformer Circuits](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf)
## rl agents
- [ ] [sakana](https://arxiv.org/abs/2408.06292)
- [ ] [diffusion policy](https://diffusion-policy.cs.columbia.edu/)
	- use diffusion to denoise action latents that guide some agent in rl
	- for whatever reason diffusion is rlly good lol idk
## robotics control
- [ ] openvla
- [ ] pizero source sent by ge
- [ ] [comma.ai driver](https://arxiv.org/pdf/2504.19077)
- [x] [egozero](https://arxiv.org/pdf/2505.20290)
	- use smart glasses that have a fisheye lens rgb and two lidar sensors for triangulation of fixed points in the scene
	- noticed that need open hand gesture to make sure hand poses can be accurately estimated
	- use normal behavior cloning, achieve good results on one-shot in-distribution tasks
- [x] [better vlas from pi](https://www.pi.website/research/knowledge_insulation)
	- use "knowledge insulation" where discretized actions (high-level actions) are trained on the backbone, then stop-gradient and train a continuous diffusion head for high-granularity high-frequency dexterous actions
	- the continuous action head uses flow matching (MCMC) to diffuse the action chunk (contiguous time interval) into the desired future subsequence
	- VLM backbone trained using text / prompt inputs and reasoning / discrete action token outputs
	- Details: use FAST (discrete cosine transform for pose), followed by quantization and BPE, to create supervised discrete action data
- [ ] [kinetix rl physics lib](https://arxiv.org/abs/2410.23208)
- [ ] [aloha robotics](https://mobile-aloha.github.io/resources/mobile-aloha.pdf)
- [ ] [openvla](https://arxiv.org/abs/2406.09246)
- [x] stockfish vs alphazero
	- stockfish has a hardcoded/manually finetuned eval, uses early pruning on a huge (depth 18, 20) MCTS to calculate minimax value
	- alphazero uses a CNN to model, has a policy and value head, policy head is policy backprop'd against "stronger version" of itself from MCTS, value head is pushed up to rollout result
- [x] [ppo](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
	- want your policy to change but not by too much
	- use advantage weighted log likelihood updates, but clip the upside to some $\epsilon$ makes it a lot more stable
- [ ] [grpo/deepseekmath](https://arxiv.org/pdf/2402.03300)
	- data mix using iterative pipeline
	- word n-gram neural model, averages all n-gram embeddings and mlp classification
	- collect dataset, do grpo on DeepSeekCoder
	- unified rl perspective: all algos can be described in terms of space parameterization, reward signal parametrization, policy gradient coefficient, GRPO just uses rule 
- [ ] [diffusion for robotics](https://github.com/mbreuss/diffusion-literature-for-robotics?tab=readme-ov-file#Diffusion-in-Robotics)
- [x] pi's [new paper](https://www.physicalintelligence.company/download/pi0.pdf) (read ??)
- [x] [action latents](https://arxiv.org/html/2410.11758v1)
- [ ] [sergey levine](https://people.eecs.berkeley.edu/~svlevine/) from berkeley has takes
- [ ] some yilun du papers: original [EBM](https://arxiv.org/abs/1903.08689), his [thesis](https://yilundu.github.io/thesis.pdf), his [research statement](https://yilundu.github.io/research_statement.pdf)
## diffusion / flows
- [x] ditto
- [x] [diffusion loss derivation](https://www.peterholderrieth.com/blog/2023/Diffusion-Models-with-Stochastic-Differential-Equations-A-Introduction-with-Self-Contained-Mathematical-Proofs/)
	- diffusion is an sde (ito's equation). we want to run it backwards. assuming affine diffusion, running backwards requires score function of distribution at current timestamp.
	- but distribution is a mixture of gaussians, so score is mixture of scores of gaussians, weighted by the posterior probabilities, each of which is just "point back to the mean"
	- using mse against true loss and then Cauchy yields the "per-data point" MSE diffusion loss usually used
- [ ] [one-step using jensen-shannon](https://arxiv.org/pdf/2502.09609)
	- can backprop differentiable losses on diffusion model outputs to noise inputs! and get desired outputs
- [ ] flow-dpm
- [x] [sander.ai: noise schedules](https://sander.ai/2024/06/14/noise-schedules.html#superfluous)
	- between timestep schedules and noise schedules, they're not super relevant; the root DOF is "relative noise level" i.e. noise level at consecutive inference steps
	- countless params: have diff. noise levels at train, inference time, can randomize spacing, loss weighting, diff. forcing, etc.
- [ ] boyuan diffusion papers
- [x] [fusing diffusion to llm](https://arxiv.org/pdf/2505.10046)
	- do "deep fusion" of llm blocks with dit blocks
	- doesn't make sense to put llm last-token activations into dit cuz they're focused on predicting the next token
	- instead, condition by fusing the attention; concat image tok seq to text tok seq, and image has full self-attention plus attending to text on each block
	- can adjust dim size, skip blocks, etc. many modifications
	- llm weights stay frozen
- [ ] [2022 diffusion design survey](https://arxiv.org/pdf/2206.00364)
- [ ] [6.S184 course notes](https://diffusion.csail.mit.edu/docs/lecture-notes.pdf)
- [ ] [diffusion math](https://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/)
	- fokker-plank from ito diffusion equation using ito's lemma
	- derive ddim
- [x] [more diffusion math](https://arxiv.org/pdf/2406.08929)
- [x] [diffusion forcing](https://boyuan.space/diffusion-forcing/)
	- autoregressive: unmasking one tok at a time
	- diffusion: denoising full seq
	- chunked diffusion: denoising next bit of sequence
	- diffusion forcing: next N toks all have arbitrary noise levels, predict the $\epsilon$
	- thesis: noising is like masking but continuous instead of T/F
	- this way, can have farther-away stuff be noisier, so denoising sequence and then add a single full-noise token to the end for long-context denoising, autoregressive style
- [x] [flow matching math](https://arxiv.org/abs/2210.02747)
	- CNF's: flow a probability field from data to gaussian, use Jacobians to track compression and maximize log prob
	- flow matching: match the flow / velocity field directly, but use conditional instead of marginal
	- inference-time do it using the ODE, use an ODE solver / can go forward or backwards
	- sorta like ddim
- [x] [Inductive Moment Matching](https://arxiv.org/pdf/2503.07565)
	- generalize diffusion, flow matching with stochastic interpolants (some stochastic process that interpolates from data to prior)
	- inform the model on distance / make diffusion "consistent" by inferring the marginal $x_{s}$ given some $x_{t},$ and pick an interpolation s.t. the resulting distribution for $x_{s}$ is $t$-independent
	- loss is diff between $x_{s}$ from $x_{r}$ vs from $x_{t},$ for $r<t$
	- They use the DDIM interpolant 
	- Somehow they match "higher" moments (powers of the distribution) ?? idk lmfao wtf
- [ ] [moviegen](https://ai.meta.com/static-resource/movie-gen-research-paper)
- [ ] dall-e algorithm
- [ ] [papers](https://arxiv.org/abs/2305.03486) from [william brandon](https://arxiv.org/abs/2206.00364)
- [x] [tarflow](https://arxiv.org/pdf/2412.06329)
	- autoregressive normalizing flow
		- it's a flow (langevin process) from original distribution to unit normal
		- loss = norm plus log "compression" (determinant of jacobian), derived from NLL of prob. density flow
		- parameterized by $x'[1:]=x[1:]e^{a(x)}+b(x)$
- [x] [sander ai diffusion](https://sander.ai/2023/07/20/perspectives.html) ✅ 2025-06-09
	- different parametrizations of noise scheduling, cfg
	- diffusion models as noise-regularized decoders
	- they solve SDEs
	- are flow-based
	- are iterative / RNNs
	- estimate expectations of the posterior
- [ ] https://diffusion.csail.mit.edu/
## sci of dl
- [ ] [muon](https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf)
- [ ] [sampling-aware finetuning](https://arxiv.org/pdf/2412.15287)
- [ ] [scaling via data manifolds](https://arxiv.org/abs/2004.10802)
- [x] [low rank](https://arxiv.org/abs/2106.09685)
	- can update matrix weights using low rank factorizations of matrices
- [x] [modular duality](https://arxiv.org/pdf/2410.21265) by bernstein
	- two angles for motivation:
	1. gradients are not same type as weights, they are linear functionals on the weights i.e. $\Delta \mathcal{L}=g^{T}w.$ Hence we should update by the result of some "duality map" applied to the gradient.
	3. In the gradient case where we want to optimize $g^{\dagger}\Delta w + \lVert \Delta w \rVert^{2}$ we want $\Delta w$ to be in the direction of the dualizer.
	2. input/outputs are normed, we want to update weights so that activations stay in the same approximate norm. Specifically we set weight norms to be dual norms on input/output using their respective norms, i.e. $\lVert W \rVert=\max \frac{{\lVert W x_{in} \rVert_{out}}}{\lVert x_{in} \rVert_{in}}.$
	4. Generalizing, we want to maintain smoothness of our layers for easy future gradient descent. Given the gradient, want to update in the direction that gives the most "bang for buck" accounting for some norm on the weights. Above maximum can be interpreted as curvature, i.e. fastest ROC of output given fixed input change. Dualizer is the argmax of this norm.
	- They arrive at several duality maps for different modules using specific norms, e.g. RMS for linear, max(RMS) over channels for convolution, $l_{1}\to RMS$ for embedding. Using the above max expression, get spectral nom for Linear, max spectral norm for Conv2D, some other weird stuff idk lmao
	- Formalize modules into a fwd, mass, sensitivity (curvature), and norm for the weights (defined by norms over input/output).
- [x] [typical sampling](https://arxiv.org/pdf/2202.00666) 
	- sample from the tokens whose negative log-prob is within a band of current conditional entropy, as opposed to nucleus (top-p)
		- why do we trust conditional entropy to be accurate? low-prob logits are inaccurate becuase of bad coverage by training data :sus:
## subquadratic
- [ ] [gdm titans](https://arxiv.org/pdf/2501.00663v1)
- [ ] albert gu's thesis
- [ ] [mamba2: ssm > transformers](https://arxiv.org/abs/2405.21060)
- [ ] [mamba is attentino](https://arxiv.org/abs/2403.01590)
- [ ] grok linear transformers, mamba
## audio/image/video specific
- [ ] moviegen
- [ ] [spark-tts](https://arxiv.org/pdf/2503.01710)
- [ ] sesame paper
- [x] [var](https://arxiv.org/pdf/2404.02905)
	- motivation: want good bidirectional, want to be able to know bottom from top, want matryoshka-doll like stuff, just think harder about key causality relationship; tokenize-flatten seems bad
	- autoregressive "chunk" is a discrete token "resolution", each chunk is still several tokens generated either in parallel or autoregressively (prolly AR)
- in audio "conv subsampling" is just decreasing spatial resolution with convs
- [x] [conformer](https://arxiv.org/pdf/2005.08100)
	- FFM, MHA, Conv, FFM
	- Conv is actually kinda MLP, with linear up channel (pointwise), activation, norm, conv, activatoin, linear down
- [x] moondream
	- uses image patches with crops of fixed sizes, vit, recombine crops to get image embedding
	- image adapter, prompt prefix, text tokens
	- 
- [ ] pixtral (vlm)
- [x] [Stable Diffusion audio codec](https://arxiv.org/pdf/2411.19842) (400-700 bps!!)
	- patch ViT style, 300 ish
	- blocks of strided conv followed by lots of transformers (sliding attention, 128 tokens)
	- fully scalar quantization—bottleneck into even lower dim, and then quantize each dim -> one token
	- either learn N scalars to snap to, or parametrize (they use floor tanh)
	- quantizer-dropout occasionally
- [x] [Hiera](https://arxiv.org/pdf/2306.00989)
	- develops a new fully-transformer based vit encoder/decoder for classification/segmentation tasks on image/video
	- on each downsize, instead of using strided conv, use Q-pooled attention, similar to cross attention but Q terms get pooled via maxpool
	- masked loss, mask tokens don't get fed into the encoder only the decoder (they get "filled in")
	- use local window attention at lower resolutions, then global later on
- [x] [Nvidia Cosmos 2](https://d1qx31qr3h6wln.cloudfront.net/publications/NVIDIA%20Cosmos_2.pdf) AE and generative
	- start with Haar 3D wavelet in time/space to downsample spatially
	- alternating factorized resblock and factorized attention with downsampling
	- ends at latent dimension 16 for continuous (diffusion) and 6 for discrete (autoregressive)
	- every video token is predicted autoregressively after flattening in sequence, use finite scalar quantization to quantize to total 64K vocab size
	- because attention is used on "every layer" context length is only 34 frames-quite sad lmao
	- use 8x16x16 compression for autoregressive and 8x8x8 for diffusion
	- autoregressive: hybrid 3D RoPE with absolute embeddings per head for each token, cross attention from text prompt (NOT concat, think abt why)
	- diffusion: typical DiT, patchify using 1x2x2 and project/flatten, then text CA plus adaLN from time
	- like suno, best is to use autoregressive to generate discrete tokens, then use "diffusion decoder"—condition on autoregressive tokens and regenerate continuous latents using diffusion
- [x] [MAE](https://arxiv.org/abs/2111.06377)
	- Make AE modeling a masked task by masking some of the patches
	- feed into encoder (with pos embeddings) without mask tokens, feed into decoder with mask tokens, allows huge speedup
	- downstream ViT task still great, AE learns to encode global "semantic properties" cuz of masking task
- [x] [meta visual tokenizers](https://arxiv.org/pdf/2501.09755)
	- opposing the conv/attention interleave strategy used by cosmos/SD, try full send single conv with 8x16x16 stride=kernel (aka patching) into ~1k feature dim, then full ViT with 3D RoPE/SwiGLU, then bottleneck
	- works well, however performance does *not* scale with encoder/decoder size (why???)
- [x] [original dit](https://arxiv.org/pdf/2212.09748)
	- u-nets are great but let's try transformers instead
	- standard latent diffusion, break latent into $p\times p$ patches (use $p=2$)
	- classifier-free guidance, i.e. constraining $p(c|x)$ implicitly by using $\log p(x | c) - \log p(x|\emptyset)$ since $p(c|x)=\frac{{p(x|c)p(c)}}{p(x)}$, then setting $\mathcal{L}=p(x)+\gamma p^*(c|x)$ for $\gamma>1$
	- various conditioning strategies including
		- cross-attention
		- adaptive layernorm with "zero scaling" (scaling right before residual by multiplying by MLP output $\alpha$ initialized to 0)
		- straight concat
	- Transformer decoder with just a linear and rearrange to get predicted $\epsilon$ and diagonal $\Sigma.$
- [x] original [vit](https://arxiv.org/pdf/2010.11929)
- [x] [sana](https://arxiv.org/pdf/2410.10629)
	- huge compression vae, with F32C32P1 instead of standard F8C8P2, leads to 4x fewer tokens (1/16 "patches", 4x channels, 4x patches)
	- linear attention using $ReLU(Q)ReLU(K)^{T}$ instead of softmax, then instead of computing $n\times n$ matrix can do $ReLU(K)^{T}V$ first to get $d\times d$ matrix for linear time full sequence training
		- inference is $O(1)$—additional K,V vectors add to the new $d\times d$ matrix, then mv-mul with new Q vector
	- add nonlinearity with 1x1 convs along channel and 3x3 conv along channel, token index and GLU
	- no positional encoding! rely on 3x3 conv for implicit positional info
	- bit quantization, triton cuda shit, for faster inference
	- [x] some Flow-DPM math ⏫ 📅 2025-01-31 ✅ 2025-02-01
- [x] [wav2vec](https://arxiv.org/pdf/2006.11477)
	- architecture: 6-deep convnet for latent representation (stride 20ms, receptive field 25ms at 16k hz), then quantize using product codebook w linear projection (gumbel softmax to get $V\times G$ logits for codebook), causal transformer on latents to get context representation
	- train by masking some latents with learned mask token, cosine similarity for cross-entropy loss between "real" masked quantized latent and several distractors
		- mask $\approx$ half of latents
	- intuition:
		- can reconstruct missing latent (when quantized) i.e. latents contain useful info (note no reconstruction!)
		- quantization isn't used for one-hot encodings autoregressive-style, rather is used to "regularize" the latents with some amount of "smoothness"
	- fine-tune final contextual representations into text token classes with a linear layer (includes blank token)
	- uses Connectionist Temporal Classification loss i.e. takes "best possible" alignment allowing for repetitions and blank tokens, takes log prob
- [ ] seq-2-seq (in comparison to wave2vec)
- [ ] [non-spiky ctc loss](https://arxiv.org/pdf/2406.02560v3)
	- basically, it's easy to just say blank by default, want to keep blank
	- use *priors* that prefer unigram i.e. non-blank
	- so that probability for seeing given character is divided by prior probability to incentivize picking unigrams more often
- [x] [ctc loss](https://www.cs.toronto.edu/~graves/icml_2006.pdf)
	- CTC probability is sum over *all* alignments of getting the desired output, because this is subset of all possible labellings of all things this is guaranteed $\leq 1$
	- use DP to compute sum of probabilities of all alignments ending at given target sequence index, for given frame
	- introduce blank token and merging adjacent equal tokens to get "skips"
- [ ] [HuBERT](https://arxiv.org/pdf/2106.07447)
	- cnn encoder ofc (it's 2021)
	- idea: masked loss training on quantized targets
	- but what are the targets? a: really degenerate bootstrapped middle activation
		- start with random quantization with some linear map
		- do the best you can, then use middle layer of transformer as representation, k-means cluster
		- try again
- [ ] [tracks-to-4d](https://tracks-to-4d.github.io/)
- [ ] [mqvae](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf)
## bio
- [ ] [diffdock](https://arxiv.org/pdf/2210.01776)
- [ ] [SE3 transformers for rfdiffusion](https://arxiv.org/abs/2006.10503)
- [ ] [DiT for molecules](https://www.arxiv.org/pdf/2503.03965)
- [ ] [discrete diff for proteins](https://arxiv.org/abs/2410.13643)
- [ ] af3 architecture, with config
- [ ] rosettafold architecture, with config
- [ ] proteinmpnn architecture, with config
- [ ] boltz-1
- [ ] https://arcinstitute.org/news/blog/evo2
- [ ] drfold2
## miscell
- [ ]  [DPO](https://arxiv.org/pdf/2305.18290)
- [ ] [transfer learning survey](https://arxiv.org/pdf/2010.03978)
- [ ] [shampoo](https://arxiv.org/pdf/2309.06497)
- [ ] decision transformer
- [x] dqvae
- [ ] vqgan
- [x] vqvae
- [ ] [deepseek prover](https://www.arxiv.org/pdf/2408.08152)
- [ ] [little book of dl](https://fleuret.org/public/lbdl-a5-booklet.pdf)
- [x] BERT and original [transformers](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
	- attention (2017): add in positional encodings, also mask attention before softmax using causal, transformers solve text gen
	- BERT: full self-attention with masks using masked language modeling (MLM) loss; cross-entropy on logits on masked tokens
- [ ] srush tensor puzzles
- [ ] simulating time w sqrt space
- [ ] [meta lingua](https://github.com/facebookresearch/lingua)
- [ ] [triton softmax](https://maharshi.bearblog.dev/optimizing-softmax-cuda/)
- [ ] srush triton puzzles
- [ ] read math in [prob ml](https://probml.github.io/pml-book/toc1.pdf)
- [ ] [Location Verification for AI chips](https://static1.squarespace.com/static/64edf8e7f2b10d716b5ba0e1/t/6670467ebe2a477eb1554f40/1718634112482/Location%2BVerification%2Bfor%2BAI%2BChips.pdf)
- [ ] [sasha rush awesome (outdated?)](https://github.com/huggingface/awesome-papers)
- [ ] [Anatomy of a Bit: Information in a Time Series Observation](https://arxiv.org/pdf/1105.2988)
- [ ] dreamcoder
- [ ] "AI and the future of work" pdf in downloads from us national academies
- [ ] [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593)
- [ ] [Prediction, Retrodiction, and The Amount of Information Stored in the Present](https://arxiv.org/abs/0905.3587)
- [ ] [long awesome ML](https://github.com/aimerou/awesome-ai-papers?tab=readme-ov-file)
- [ ] [o1-related](https://github.com/srush/awesome-o1/)
- [ ] [awesome ML tools](https://github.com/srush/awesome-machine-learning)
- [ ] [awesome discrete diffusion](https://github.com/kuleshov-group/awesome-discrete-diffusion-models)

## graveyard