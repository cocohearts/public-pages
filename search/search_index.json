{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"All raw files available on Github . These files are a hand-picked subset of my personal Obsidian vault. About Me love food Sections Food \u2013 selected food and cooking notes","title":"Home"},{"location":"#about-me","text":"love food","title":"About Me"},{"location":"#sections","text":"Food \u2013 selected food and cooking notes","title":"Sections"},{"location":"Material%20Knowledge/Food%2BCooking/24%2B7%20ways%20to%20do%20eggs/","text":"[x] scrambled [x] Tamagoyaki style scramble [x] sunny-up [x] poached [ ] eggs in cocotte [ ] soft-boiled (ramen eggs) [ ] tea eggs [x] hard-boiled [x] french omelette [x] country omelette [ ] baked eggs american country style [ ] deviled [ ] quiche (no pastry) [ ] frittata [ ] egg casserole [ ] breakfast egg wrap [x] eggs benedict [ ] breakfast egg sandwich (bacon, egg, cheese on muffin) [ ] egg salad [ ] scotch egg [x] french toast [ ] \u70b8\u86cb [ ] crepe [ ] omurice Desserts [ ] egg custard tart [ ] egg pancakes [ ] creme brulee (?) [ ] flan [ ] egg souffle [ ] egg cakes [ ] rum eggnog","title":"24+7 ways to do eggs"},{"location":"Material%20Knowledge/Food%2BCooking/24%2B7%20ways%20to%20do%20eggs/#desserts","text":"[ ] egg custard tart [ ] egg pancakes [ ] creme brulee (?) [ ] flan [ ] egg souffle [ ] egg cakes [ ] rum eggnog","title":"Desserts"},{"location":"Material%20Knowledge/Food%2BCooking/Things%20To%20Cook/","text":"[x] eggs benedict [x] https://www.thespruceeats.com/rum-eggnog-recipes-760561 [ ] easy over, correctly [ ] french souffle omelette https://www.youtube.com/watch?v=4XiWUis2eKc&ab_channel=ItaliaSquisita [ ] \u5364\u8089\u996d [ ] ahi poke [ ] gnocchi o pizza (with premade gnocchi ?) [ ] cotoletta alla milanese [ ] spezzatino with potatos [ ] melanzane alla parmigiana [ ] the korean egg rice thing i saw on insta","title":"Things To Cook"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Asian%20Barbecue%20Breaded%20Chicken/","text":"Marinate in brown sugar, barbecue mix, drumstick asian barbecue mix Cover with flour Melt butter in plate, arrange drumsticks tightly on the butter Turn halfway thru 425 F for 45 min","title":"Asian Barbecue Breaded Chicken"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/","text":"Batter 2 large eggs, 1 cup whole milk, dash salt, 2tbsp butter, 1/2 cup water using blender , add in 1cup allpurpose flour carefully dash of vanilla optional chill for 1hr Cooking Medium heat 10in skillet, oil with high-tolerance vegetable oil (butter if daring) 1/4 cup batter in, swirl, wait till brown underneath + flip Garnishes/Flourish Oil on medium heat, then a rested crepe, then low heat Galette Completes Egg on, don't break yolk, move egg white around [flavored, marinated] meat around yolk to fix in place, cheese on top Fold edges, careful not to burn on high heat, lid on until brown DW about uncooked egg Black pepper to finish Folded Egg on, use spoon to break and spread Cheese of choice Vegetables/thin meats of choice Fold into quarter circle, shift so inside can be seen Rolled Bottom 3/4 add meat, potatoes, cheese, vegetables etc. Roll, and place edge down to seal Roll into rectangle or circle shape","title":"Crepes"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#batter","text":"2 large eggs, 1 cup whole milk, dash salt, 2tbsp butter, 1/2 cup water using blender , add in 1cup allpurpose flour carefully dash of vanilla optional chill for 1hr","title":"Batter"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#cooking","text":"Medium heat 10in skillet, oil with high-tolerance vegetable oil (butter if daring) 1/4 cup batter in, swirl, wait till brown underneath + flip","title":"Cooking"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#garnishesflourish","text":"Oil on medium heat, then a rested crepe, then low heat","title":"Garnishes/Flourish"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#galette-completes","text":"Egg on, don't break yolk, move egg white around [flavored, marinated] meat around yolk to fix in place, cheese on top Fold edges, careful not to burn on high heat, lid on until brown DW about uncooked egg Black pepper to finish","title":"Galette Completes"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#folded","text":"Egg on, use spoon to break and spread Cheese of choice Vegetables/thin meats of choice Fold into quarter circle, shift so inside can be seen","title":"Folded"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#rolled","text":"Bottom 3/4 add meat, potatoes, cheese, vegetables etc. Roll, and place edge down to seal Roll into rectangle or circle shape","title":"Rolled"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Eggs%20Benedict/","text":"Hollandaise - three egg yolks, teaspoon dijon mustard, tablespoon lemon juice, good bit of salt, some cayenne, blend with electric whisk - hot melt a LOT (half cup !!) of butter, and pour into mixture while whisking Poached Eggs - Can safely make multiple at once - Larger pan, deep enough water to cover the egg no more - Bring to a boil, then simmer, add (couple) splashes white vinegar - Very very very gently lower desired number of eggs into water, shape eggs in the water - Four minutes, take em out Microwaved version: - 1/4 cup water in a flat-bottom mug, microwave 50secs or till steaming - egg into water, microwave on 100 for 30secs or to desired consistency - alternatively, lower power, longer time? - take out and DRY Plus a toasted English Muffin, + fried Canadian Bacon 1min each side Plating - Bacon, Salmon if desired, not too much hollandaise - With parsley/rosemary garnish if desired","title":"Eggs Benedict"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/French%20Omelette/","text":"Butter + olive oil to barely a foam at low-medium heat Three eggs, whisk with salt white pepper and a bit of cold water Pour in, whisk and stir and shake, AVOID COLORING at all costs Three stages: scramble, spread, fold Hold at 45 deg angle and strike handle to get to slip and fold in that way","title":"French Omelette"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/French%20Toast/","text":"one egg, 1/6cup milk, 1tbsp flour, dash of vanilla/cinnamon, salt to taste, for two slices of Costco croissant bread in a lightly buttered hot skillet, add the two slices, one quarter of the egg on each face of each slice serve","title":"French Toast"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Sous%20Vide%20Steak/","text":"Any cut, add seasonings/rubs/butter/herbs to both sides Add the machine, fill the pot to about half full with warm water Set up preheating, cook with \"rare\" (127) cook, when done dry thoroughly with paper towels take to skillet outside and sear with olive oil (350degs F) till just brown flipping 30 secs","title":"Sous Vide Steak"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/%E7%83%A4%E5%A4%A7%E6%8E%92%E9%AA%A8/","text":"Typically roast at 375 for 20-30 mins Kinder's barbecue is king Take large bone-in pork rib, season richly with barbecue rub, prime rib rub, brown sugar Marinate overnight Roast at 275 preheated for two-three hours Either stack crosswise in a dutch oven Or on an oven tray wrapped in aluminum foil Keeps moisture intact; won't dry out Serve","title":"\u70e4\u5927\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/Tasty%20things/Snacks/","text":"giant ono shrimp chips \u7d2b\u7c73\u7ca5 \u6761\u5934\u7cd5 \u5c0f\u9f99\u867e\u5473 \u9505\u5df4","title":"Snacks"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/","text":"\u7092\uff0c\u52a0\u8c03\u6599 \u4e94\u82b1\u8089\u5207\u6210\u201d\u5c0f\u5757\u201c\u4e0d\u8981\u5207\u5f97\u592a\u5c0f\uff0c\u8089\u4f1a\u7f29 \u7528\u5c11\u8bb8\u6cb9\u5148\u7ffb\u7092\uff0c\u7b49\u5230\u8868\u9762\u53d8\u8272\uff0c\u8fd9\u662f\u5c0f\u5fc3\u8868\u9762\u6e29\u5ea6 \u52a0\u51b0\u7cd6\u548c\u91d1\u5170\u6cb9\u818f\uff0c\u62cc\u5300 \u52a0\u5927\u91cf\u6599\u9152\uff0c\u5c11\u91cf\u7c73\u9152 \u52a0\u9999\u6599 2-3\u7247\u516b\u89d2 1-2\u7247\u9999\u53f6 2-3\u68f5\u4e01\u9999 \uff5e8\u514b\u4e94\u9999\u7c89 \u62cc\u5300\u540e\uff0c\u52a0\u6c34\u4e00\u76f4\u5230\u628a\u8089\u5168\u90e8\u76d6\u4f4f \u52a04-6\u716e\u597d\u4e86\u7684\u9e21\u86cb \u6700\u540e\u52a0\u9002\u91cf\u7092\u6d0b\u8471\u9165 \u987f\u3001\u6536\u6c41 \u7528\u6587\u706b\u6162\u7096\uff0c\u7b49\u5230\u5feb\u8981\u6536\u6c41 \u9700\u8981\u7b49\u7684\u8bdd\u53ef\u4ee5\u653e\u5230\u4e00\u8fb9 \u6700\u540e\u6536\u6c41\u65f6 \u8089\u63a8\u5230\u4e00\u8fb9 \u9505\u62ac\u8d77\u6765\uff0c\u7528\u52fa\u5b50\u628a\u7559\u4e0b\u6765\u7684\u6c41\u5012\u5230\u8089\u4e0a \u8fd9\u6837\u8089\u4e0d\u4f1a\u5e72 \u6446\u76d8 blanch \u4e00\u4e9b\u5c0f\u767d\u83dc\u5fc3 \u628a\u9e21\u86cb\u5207\u6210\u534a\u6839\u7c73\u996d\u6446\u5728\u4e00\u8d77","title":"\u5364\u8089\u996d"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_1","text":"\u4e94\u82b1\u8089\u5207\u6210\u201d\u5c0f\u5757\u201c\u4e0d\u8981\u5207\u5f97\u592a\u5c0f\uff0c\u8089\u4f1a\u7f29 \u7528\u5c11\u8bb8\u6cb9\u5148\u7ffb\u7092\uff0c\u7b49\u5230\u8868\u9762\u53d8\u8272\uff0c\u8fd9\u662f\u5c0f\u5fc3\u8868\u9762\u6e29\u5ea6 \u52a0\u51b0\u7cd6\u548c\u91d1\u5170\u6cb9\u818f\uff0c\u62cc\u5300 \u52a0\u5927\u91cf\u6599\u9152\uff0c\u5c11\u91cf\u7c73\u9152 \u52a0\u9999\u6599 2-3\u7247\u516b\u89d2 1-2\u7247\u9999\u53f6 2-3\u68f5\u4e01\u9999 \uff5e8\u514b\u4e94\u9999\u7c89 \u62cc\u5300\u540e\uff0c\u52a0\u6c34\u4e00\u76f4\u5230\u628a\u8089\u5168\u90e8\u76d6\u4f4f \u52a04-6\u716e\u597d\u4e86\u7684\u9e21\u86cb \u6700\u540e\u52a0\u9002\u91cf\u7092\u6d0b\u8471\u9165","title":"\u7092\uff0c\u52a0\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_2","text":"\u7528\u6587\u706b\u6162\u7096\uff0c\u7b49\u5230\u5feb\u8981\u6536\u6c41 \u9700\u8981\u7b49\u7684\u8bdd\u53ef\u4ee5\u653e\u5230\u4e00\u8fb9 \u6700\u540e\u6536\u6c41\u65f6 \u8089\u63a8\u5230\u4e00\u8fb9 \u9505\u62ac\u8d77\u6765\uff0c\u7528\u52fa\u5b50\u628a\u7559\u4e0b\u6765\u7684\u6c41\u5012\u5230\u8089\u4e0a \u8fd9\u6837\u8089\u4e0d\u4f1a\u5e72","title":"\u987f\u3001\u6536\u6c41"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_3","text":"blanch \u4e00\u4e9b\u5c0f\u767d\u83dc\u5fc3 \u628a\u9e21\u86cb\u5207\u6210\u534a\u6839\u7c73\u996d\u6446\u5728\u4e00\u8d77","title":"\u6446\u76d8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/","text":"\u5907\u6599 \u4e24\u7aef\u5207\u4e0b\u6765\uff0c\u76ae\u8584\u4e86 \u5207\u6210\u4e1d\uff1a\u6cbf\u7740equator\u5207 \u5207\u6210\u7247\uff1a\u6cbf\u7740radial\u5207 \u7f8a\u8089\u4e1d\u6ed1\u597d \u52a0\uff1a\u9178\u7c89\uff0cyoshida\uff0c\u6d0b\u8471\u5206\uff0c\u6599\u9152\uff0c\u7cd6\uff0c\u9171\u6cb9\u4e5f\u884c\uff0c\u8c46\u74e3\u9171\u4e5f\u884c \u8c03\u6599 \u5b5c\u7136 \u9e21\u6c64 \u70f9\u996a\u6cd5 \u52a0\u6cb9\uff0c\u5148\u628a\u8089\u7092\u4e86\uff0c\u52a0\u5b5c\u7136\u7c89 \u52a0\u6d0b\u8471\uff0c\u52a0\u9e21\u6c64\uff0c\u7092\u5230\u7f29\uff0c\u5728\u52a0\u5b5c\u7136\u7c89 \u8089\u52a0\u56de\u53bb\uff0c\u6709\u52a0\u5b5c\u7136\u7c89\uff0c\u53ef\u4ee5\u5728\u52a0\u9e21\u6c64\uff0c\u6700\u540e\u5c1d\u4e00\u53e3\u8bd5\u8bd5\u54b8\u86cb","title":"\u5b5c\u7136\u7f8a\u8089\u6d0b\u8471"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_1","text":"\u4e24\u7aef\u5207\u4e0b\u6765\uff0c\u76ae\u8584\u4e86 \u5207\u6210\u4e1d\uff1a\u6cbf\u7740equator\u5207 \u5207\u6210\u7247\uff1a\u6cbf\u7740radial\u5207 \u7f8a\u8089\u4e1d\u6ed1\u597d \u52a0\uff1a\u9178\u7c89\uff0cyoshida\uff0c\u6d0b\u8471\u5206\uff0c\u6599\u9152\uff0c\u7cd6\uff0c\u9171\u6cb9\u4e5f\u884c\uff0c\u8c46\u74e3\u9171\u4e5f\u884c","title":"\u5907\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_2","text":"\u5b5c\u7136 \u9e21\u6c64","title":"\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_3","text":"\u52a0\u6cb9\uff0c\u5148\u628a\u8089\u7092\u4e86\uff0c\u52a0\u5b5c\u7136\u7c89 \u52a0\u6d0b\u8471\uff0c\u52a0\u9e21\u6c64\uff0c\u7092\u5230\u7f29\uff0c\u5728\u52a0\u5b5c\u7136\u7c89 \u8089\u52a0\u56de\u53bb\uff0c\u6709\u52a0\u5b5c\u7136\u7c89\uff0c\u53ef\u4ee5\u5728\u52a0\u9e21\u6c64\uff0c\u6700\u540e\u5c1d\u4e00\u53e3\u8bd5\u8bd5\u54b8\u86cb","title":"\u70f9\u996a\u6cd5"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E6%B8%85%E7%82%92%E5%B0%8F%E7%99%BD%E8%8F%9C/","text":"\u5c0f\u767d\u83dc\u5207\u6210\u5757\uff0c\u6839\u5207\u7684\u5c0f\uff0c\u53f6\u5b50\u5207\u7684\u5927 \u52a0\u4e00\u70b9\u6cb9\uff0c\u70e7\u70ed \u52a0\u83dc\u65f6\u8be5\u6709\u201c\u5472\u5566\u201d\u58f0 \u7b49\u53f6\u5b50\u7f29\uff0c\u52a0\u7cd6\uff0c\u76d0\uff0c\u9e21\u6c64 \u6491\u51fa\u6765 \u5403\uff01","title":"\u6e05\u7092\u5c0f\u767d\u83dc"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/","text":"dice ribs to desired size, bring water to a boil in large pot blanch ribs until white (20min) skim top stuff for each 500g of meat, add: lots of \u6599\u9152 2tbsp \u9171\u6cb9 mixed 3tbsp \u918b/\u7c73\u918b 4tbsp \u7ea2\u7cd6 \u59dc\u7247 if desired turn down heat, sit for 40 mins till color changes skim top stuff, turn up heat when sticky, tilt pot and baste in its own sauce, \u6536\u6c41 serve with green onion finely diced and white sesame, with modernist plating","title":"\u7cd6\u918b\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E7%99%BD%E8%8F%9C%EF%BC%88%E8%A6%81%E7%BB%83%EF%BC%89/","text":"\u7528\u6cb9\u70e7\u8471\u82b1 \u653e\u85d5\u7247/\u767d\u83dc \u7ffb\u7092 \u52a0\u76d0\uff0c\u7cd6 \u5feb\u719f\u4e86\uff0c\u52a0\u918b \u51fa\u9505\u524d\uff0c\u5173\u706b\uff0c\u52a0\u82b1\u6912\u6cb9 \u79f0\u51fa\u6765\uff0c\u52a0\u751f\u8471\u82b1","title":"\u7cd6\u918b\u767d\u83dc\uff08\u8981\u7ec3\uff09"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%82%A5%E8%82%A0%E7%82%92%E8%92%9C%E8%8B%97/","text":"\u80a5\u80a0\u7a0d\u5fae\u7092\uff0c\u5012\u51fa\u6765 \u52a0\u849c\u82d7\uff0c\u7a0d\u5fae\u52a0\u76d0 \u6700\u540e\u975e\u5e38\u52a0\u56de\u53bb \u814a\u8089\u7092\u82b9\u83dc \u5f88\u50cf","title":"\u80a5\u80a0\u7092\u849c\u82d7"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/","text":"\u5907\u6599 \u7528\u5200\u628a\u83dc\u82b1\u6839\u5207\u4e0b\u6765\uff0c\u7528\u5200\u5728\u6839\u90a3\u91cc\u5207\u6210\u5757 \u7528\u5200\uff0c\u4f7f\u52b2\u538b\uff0c\u628a\u814a\u8089\u5207\u6210\u7247 \u8c03\u6599 \u8fa3\u8c46\u74e3\u9171 \u70f9\u996a\u6cd5 \u814a\u8089\u7092\u4e00\u4e0b\uff0c\u7b49\u53d8\u900f\u660e\uff0c\u5012\u51fa\u6765 \u83dc\u82b1\u52a0\u8fdb\u53bb\uff0c\u52a0\u9e21\u6c64\uff0c\u52a0\u8fa3\u8c46\u74e3\u9171\uff0c\u8d85\u51fa\u989c\u8272 \u52a0\u8089","title":"\u83dc\u82b1\u7092\u814a\u8089"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_1","text":"\u7528\u5200\u628a\u83dc\u82b1\u6839\u5207\u4e0b\u6765\uff0c\u7528\u5200\u5728\u6839\u90a3\u91cc\u5207\u6210\u5757 \u7528\u5200\uff0c\u4f7f\u52b2\u538b\uff0c\u628a\u814a\u8089\u5207\u6210\u7247","title":"\u5907\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_2","text":"\u8fa3\u8c46\u74e3\u9171","title":"\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_3","text":"\u814a\u8089\u7092\u4e00\u4e0b\uff0c\u7b49\u53d8\u900f\u660e\uff0c\u5012\u51fa\u6765 \u83dc\u82b1\u52a0\u8fdb\u53bb\uff0c\u52a0\u9e21\u6c64\uff0c\u52a0\u8fa3\u8c46\u74e3\u9171\uff0c\u8d85\u51fa\u989c\u8272 \u52a0\u8089","title":"\u70f9\u996a\u6cd5"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%9C%82%E9%92%88%E6%8E%92%E9%AA%A8/","text":"\u7528\u9171\u6cb9\u548c\u6599\u9152\uff0c\u732a\u6392\u814c\u4e24\u5c0f\u65f6 \u7528\u7c89\u9488\u6392\u9aa8\u7c89\uff0c\u628a\u6392\u9aa8\u7ed9\u76d6\u5300 \u653e\u9ad8\u538b\u9505\uff0c\u7528\u6c34\u84b855\u5206\u949f\uff08\u6309\u7167\u8bf4\u660e\uff09","title":"\u8702\u9488\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%A5%BF%E7%BA%A2%E6%9F%BF%E7%82%92%E9%B8%A1%E8%9B%8B/","text":"\u897f\u7ea2\u67ff\u5207\u6210\u4e01 \u52a0\u6cb9\uff0c\u9e21\u86cb\u7092\u4e0a\uff0c\u534a\u719f\u4fbf\u5012\u51fa\u6765 \u52a0\u897f\u7ea2\u67ff\uff0c\u52a0\u7cd6\u548c\u897f\u7ea2\u67ff\u9171\uff0c\u52a0\u9e21\u6c64\uff0c\u628a\u6c41\u7092\u51fa\u6765 \u677e\u4e86\u4ee5\u540e\uff0c\u53ef\u4ee5\u7528\u7b77\u5b50\u6311\u897f\u7ea2\u67ff\u76ae \u9e21\u86cb\u52a0\u56de\u53bb\uff0c\u626b\u4e00\u628a\u76d0\uff0c\u7ee7\u7eed\u7092 \u628a\u9e21\u86cb\u7092\u7684\u51dd\u56fa\u4e00\u70b9 \u52a0\u918b\uff0c\u82b1\u6912\u6cb9\uff0c\u8d85\u5300\uff0c\u6700\u540e\u52a0\u8471","title":"\u897f\u7ea2\u67ff\u7092\u9e21\u86cb"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%B1%86%E8%A7%92%E7%84%96%E9%9D%A2/","text":"\u5728\u84b8\u9505\u91cc\u55b7\u4e00\u5c42\u6cb9\uff0c\u628a\u9762\u6563\u5f00\uff0c\u7528\u70ed\u6c34\u84b8 \u516b\u5206\u949f \u732a\u8089\u672b\u5907\u597d\uff0c\u52a0\u9171\u6cb9\uff0c\u869d\u6cb9\uff0c\u9ed1\u548c\u767d\u80e1\u6912\uff0cyoshida\uff0c\u849c\u7c89\uff0c\u6d0b\u8471\u7c89\uff0c\u62cc\u5300 \u849c\u9897\u5907\u597d\uff0c\u76ae\u8584\u4e86 \u8c46\u89d2\u5907\u597d\uff0c\u8fb9\u63b0\u4e0b\u6765\uff0c\u8c46\u89d2\u63b0\u6210\u5757 \u8471\u82b1\u5907\u597d \u9505\u91cc\u52a0\u6cb9\uff0c\u70e7\u70ed\uff0c\u7b49\u8089\u672b\u91cc\u7684\u6cb9\u7092\u51fa\u6765\uff0c\u52a0\u8c46\u74e3\u9171 \u5728\u52a0\u8c46\u89d2\uff0c\u6301\u7eed\u7ffb\u7092 \u51fa\u8272\u65f6\uff0c\u628a\u9762\u52a0\u5728\u4e0a\u9762\uff0c\u7116\u4e09\u5206\u949f \u52a0\u4e00\u534a\u8471\uff0c\u518d\u7ffb\u7092 \u628a\u989c\u8272\u5747\u5300\u5435\u5230\u9762\u4e0a","title":"\u8c46\u89d2\u7116\u9762"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/Final%20Notes/","text":"![[U1 Chem of Life]] ![[U2 The Cell]] ![[U3 Genetics]] ![[U4 Evolution]] ![[U5 Evo History]] ![[U6 Plant Function]] ![[U7 Animal Function]] ![[U8 Ecology]]","title":"Final Notes"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/Planning/","text":"1220 pages 8 hours 0.4 mins per page, 24 secs per page notes at end of each chapter, calc. total time allotted for each chapter ig","title":"Planning"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U1%20Chem%20of%20Life/","text":"Chap 2 Uusal atomic chem, protons/neutrons/electrons/P table bond types, bond shapes molecular structure -> function Chap 3 Water is important, has much volume on Earth Hydrogen bonds lead to High heat capacity Adhesion Floating ice Solvency Water permits acids/bases, note buffer solutions, pairing of weak acid/corresponding base, v/versa Chap 4 Carbons r backbone, 4 valence allows for varied structure Isomers: structural , cis-trans (angle of alignment), enantiomer (symmetry) Some functional groups and chemical groups are key to remember Chap 5 Macromolecules are made from polymers, except proteins Lipids: fats are glycerol with fatty acids, can have sat/unsat/trans Carbs: sugars are single-bonded C chains, usually H-C-O-H, can have multiple saccharides in a chain Proteins: amino acids with C, Amino group, Carboxyl group, and Functional group linear amino structure, secondary helices/sheets, tertiary disulfide chains/hydrophobic interactions, quaternary polypeptide interactions Nucleic acids Sugar/phosphate backbone chain Double helix","title":"U1 Chem of Life"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U1%20Chem%20of%20Life/#chap-2","text":"Uusal atomic chem, protons/neutrons/electrons/P table bond types, bond shapes molecular structure -> function","title":"Chap 2"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U1%20Chem%20of%20Life/#chap-3","text":"Water is important, has much volume on Earth Hydrogen bonds lead to High heat capacity Adhesion Floating ice Solvency Water permits acids/bases, note buffer solutions, pairing of weak acid/corresponding base, v/versa","title":"Chap 3"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U1%20Chem%20of%20Life/#chap-4","text":"Carbons r backbone, 4 valence allows for varied structure Isomers: structural , cis-trans (angle of alignment), enantiomer (symmetry) Some functional groups and chemical groups are key to remember","title":"Chap 4"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U1%20Chem%20of%20Life/#chap-5","text":"Macromolecules are made from polymers, except proteins Lipids: fats are glycerol with fatty acids, can have sat/unsat/trans Carbs: sugars are single-bonded C chains, usually H-C-O-H, can have multiple saccharides in a chain Proteins: amino acids with C, Amino group, Carboxyl group, and Functional group linear amino structure, secondary helices/sheets, tertiary disulfide chains/hydrophobic interactions, quaternary polypeptide interactions Nucleic acids Sugar/phosphate backbone chain Double helix","title":"Chap 5"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/","text":"Chap 6 12m - Light/electron microscopes; bend/reflect photons/electrons to see stuff inside cell - Plasma membranes, phopholipid bilayers; Prokaryotes don't have organelles, eukeryotes do - DNA in nucleus, implemented by ribosomes - soft ER creates lipids, detoxifies - Rough ER takes mRNA, makes proteins in a vesicle, transports to Golgi for packing; also makes membrane - Golgi synthesizes, sorts - Lysosome contains digestive enzymes - Mitochondria, chloroplasts also do things - Cytoskeleton does structural work, microfibers; transport proteins \"walk\" with ATP along microtubules carrying stuff - Plant cell walls from cellulose fibers knitted; animal cells have collagen Chap 7 Cellular membrane is rapidly moving bilayer interspersed with proteins and other junk Proteins mediate what can cross Diffusion allows for passive transport along electric/chemical gradients Level of water in environ is important; mediated by aquaporins Pumps can create electric/chemical gradients, by ATP \"cotransport\"; pump creates gradient, which is then used to collect outside stuff Exocytosis (send stuff out), endocytosis (bring stuff in) for bulk transport Chap 8 Metabolism through catabolic (breaks down) or anabolic (builds) Gibbs free energy = H - T*S, energy, temp, entropy resp., must be decreasing for reaction to be spontaneous ATP releases inorganic phosphate group, phosphorylates, changes protein shape -> does work Enzymes control the \"hump\" of the free-energy chemical reaction graph Regulation through binding to enzymes/deactivating them (e.g. feedback), or just by enzymes being separated Chap 9 Start with glucose, use two NADP+ to change into NADPH, break into two pyruvates, also two ATP Pyruvates into Acetyl CoA through citric acid cycle, output mostly NADPH via oxidative phosphorylation there's an H+ pump that turns a motor that turns ADP into ATP, via electronegativity decrease ladder ending at O_2 to H2O Fermentation returns NADPH to NADP+, via lactic acid or alcohol, skips oxidative phosphorylation Fermentation/anaerobic is 16 times less efficient than aerobic All other macromolecules feed into this system, maybe skip glycolysis Chap 10 Light hits chlorophyll, excites electron Like relay, electron gets passed around until hits one chlorophyll, excited electron transferred to \"primary electron acceptor\", produces ATP Secondary photosystem then creates NADPH asw Finally Calvin cycle uses carbon fixation, reduction, and regeneration to create G3P (three carbon sugar) There are also other mechanisms Chloroplast organelle is called plastid Chap 11 Long-distance signaling via hormones, or short-distance via short-lived signalling molecules or direct interfacing A kinase is smth that initiates phosphorylation of a protein with ATP, activating it Ligand binds with bigger receptor, changing shape, releasing/causing phosphorylation Receptor activates G protein, doing smth Ion channels opened by ligands, allow ions into cytoplasm Phosphorylation is widespread activation action, with large cascades from exponential chains Internal cell signalling can include ions, small molecules, e.g. Ca ions are much lower in cytoplasm than in blood, release from ER can signal stuff Apoptosis is controlled by multiple streams, signals, DNA damage, protein misfolding, results in blebs that are \"cleaned up\" Chap 12 Genome split into 23 chromosomes, along with wrapper called chromatin. Gametes are half-DNA cells; rest are called somatic Eukaryotic animal/plant cells have mitosis , a spindle is created that attaches to chromosomes, pulling in half, while cell is stretched by microtubules Chromosomes split; mitosis has five stages; last one combined with \"cytokinesis,\" new membrane forming and two cells moving apart Cell cycle control as checkpoints, usually at end of G1 before chromatid doubling; if failed checkpoint, cell goes to G0 usually Cancer if cycle control is broken; metastasis is cells moving to other parts of the body","title":"U2 The Cell"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-6","text":"12m - Light/electron microscopes; bend/reflect photons/electrons to see stuff inside cell - Plasma membranes, phopholipid bilayers; Prokaryotes don't have organelles, eukeryotes do - DNA in nucleus, implemented by ribosomes - soft ER creates lipids, detoxifies - Rough ER takes mRNA, makes proteins in a vesicle, transports to Golgi for packing; also makes membrane - Golgi synthesizes, sorts - Lysosome contains digestive enzymes - Mitochondria, chloroplasts also do things - Cytoskeleton does structural work, microfibers; transport proteins \"walk\" with ATP along microtubules carrying stuff - Plant cell walls from cellulose fibers knitted; animal cells have collagen","title":"Chap 6"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-7","text":"Cellular membrane is rapidly moving bilayer interspersed with proteins and other junk Proteins mediate what can cross Diffusion allows for passive transport along electric/chemical gradients Level of water in environ is important; mediated by aquaporins Pumps can create electric/chemical gradients, by ATP \"cotransport\"; pump creates gradient, which is then used to collect outside stuff Exocytosis (send stuff out), endocytosis (bring stuff in) for bulk transport","title":"Chap 7"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-8","text":"Metabolism through catabolic (breaks down) or anabolic (builds) Gibbs free energy = H - T*S, energy, temp, entropy resp., must be decreasing for reaction to be spontaneous ATP releases inorganic phosphate group, phosphorylates, changes protein shape -> does work Enzymes control the \"hump\" of the free-energy chemical reaction graph Regulation through binding to enzymes/deactivating them (e.g. feedback), or just by enzymes being separated","title":"Chap 8"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-9","text":"Start with glucose, use two NADP+ to change into NADPH, break into two pyruvates, also two ATP Pyruvates into Acetyl CoA through citric acid cycle, output mostly NADPH via oxidative phosphorylation there's an H+ pump that turns a motor that turns ADP into ATP, via electronegativity decrease ladder ending at O_2 to H2O Fermentation returns NADPH to NADP+, via lactic acid or alcohol, skips oxidative phosphorylation Fermentation/anaerobic is 16 times less efficient than aerobic All other macromolecules feed into this system, maybe skip glycolysis","title":"Chap 9"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-10","text":"Light hits chlorophyll, excites electron Like relay, electron gets passed around until hits one chlorophyll, excited electron transferred to \"primary electron acceptor\", produces ATP Secondary photosystem then creates NADPH asw Finally Calvin cycle uses carbon fixation, reduction, and regeneration to create G3P (three carbon sugar) There are also other mechanisms Chloroplast organelle is called plastid","title":"Chap 10"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-11","text":"Long-distance signaling via hormones, or short-distance via short-lived signalling molecules or direct interfacing A kinase is smth that initiates phosphorylation of a protein with ATP, activating it Ligand binds with bigger receptor, changing shape, releasing/causing phosphorylation Receptor activates G protein, doing smth Ion channels opened by ligands, allow ions into cytoplasm Phosphorylation is widespread activation action, with large cascades from exponential chains Internal cell signalling can include ions, small molecules, e.g. Ca ions are much lower in cytoplasm than in blood, release from ER can signal stuff Apoptosis is controlled by multiple streams, signals, DNA damage, protein misfolding, results in blebs that are \"cleaned up\"","title":"Chap 11"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U2%20The%20Cell/#chap-12","text":"Genome split into 23 chromosomes, along with wrapper called chromatin. Gametes are half-DNA cells; rest are called somatic Eukaryotic animal/plant cells have mitosis , a spindle is created that attaches to chromosomes, pulling in half, while cell is stretched by microtubules Chromosomes split; mitosis has five stages; last one combined with \"cytokinesis,\" new membrane forming and two cells moving apart Cell cycle control as checkpoints, usually at end of G1 before chromatid doubling; if failed checkpoint, cell goes to G0 usually Cancer if cycle control is broken; metastasis is cells moving to other parts of the body","title":"Chap 12"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/","text":"Chap 13 Genes have loci, chromosomes come in pairs of sister chromatids Mitosis is asexual reproduction, or cell proliferation Human haploid number=23, diploid=46, there are 46 chromosomes each a pair of chromatids Meiosis: each chromosome pair combines/crosses and gets recombinated, and then split into two homologs -> split into four daughter cells/gametes Prophose/Metaphase/Anaphase I/II Chap 14 Punnet squares, recessive/dominant alleles Since each person carries two versions of each chromosome (each pair of sister chromatids), etc.etc. \"carriers\", hereditary disease analysis etc. Chap 15 Sex-linked genes (live on X,Y) inherit in their own way Linked genes, similar to each other, violate independent selection as predicted by Mendel Percentage of \"recombinants\" can indicated linkedness Aneuploidy is weird copying mistakes resulting in abnormal chromosome counts Genome imprinting, from DNA methylation, from only the mother Other disorders, e.g. mitochondria DNA issues Chap 16 Experiments with phages, crystal spectroscopy determined the structure; AT/GC (AC-DC) Semiconservative replication: Primase adds RNA primers pol III adds in the 3'-5' direction (on bottom) on leading strand pol I replaces RNA primers and Okazaki fragments with DNA Rep. fork opened at the origin , held open by helicase Chromatin is made of DNA, histones DNA width 2nm, becomes 10 after wrapping around histones, 30 in a \"fiber\", 300 in the loops which condense to 700 chromosome Chap 17 Transcription from RNA polymerase, starts at promoter, TATA box 25 nucleotides before start Transcription unit refers to actual region RNA is spliced by spliceosome (a ribozyme) into ex pressed-ons and in tervening-trons (exons, introns) UTR acts as caps on the ends, for movement/protection/binding to ribosome Translation starts at methionine, ends at stop More complex in eukaryotes; some enzymes chase the 5' end until transcription stops tRNAs map codons to amino acids, created at aminoacyl-tRNA synthetase Different kinds of mutations; missense, deletion, frameshift, nonsense etc. Chap 18 Repressors, activators, which react to environment Activator/repressor can be turned on/off by binding to relevant proteins For prokaryotes: Operon consists of promoter, operator (interacts activator/repressor), and all coding regions for one pathway (grouped together) Distal control elements can bend around and help construct transcription factor, once activated Coordinate control can through hormones/other molecules/etc Post-transcription regulation through non-coding RNA (ncRNA) such as miRNA, siRNA, etc EVO-DEVO: determined->differentiated Cells communicate/emerge through induction, signal molecules from direct contact Cytoplasmic determinants are asymmetries in protein/mRNA distribution first discovered is bicoid , in gradient from anterior to posterior Cancer results from oncogenes , control/avoid controlling cell cycle; multi-mutation model of cancer progression Chap 19 Virus is nucleic acid surrounded by protein coat, capsid and sometimes viral envelope After DNA injection, two paths: virulent (lytic cycle), make a bunch and kill/attack temperate (lysogenic cycle), make a few and then coexist/inhabit for a while Defenses include CAS and proviruses Viruses can enter through broken cell walls Prions are slow-acting, self-replicating proteins , nearly indestructible Chap 20 DNA sequencing by adding dRNA one at a time, flashes occur on binds PCR, using primase/polymerase/nucleotides/buffer to quickly copy fragments Restriction enzymes can make desire recombinant DNA, stitched together by ligase Hacking existing viral tech, like CRISPR Can un-differentiate cells by introducing certain proteins Chap 21 Systems biology where bioinfo is used to study whole genes ( genomics ) and protein systems ( proteomics ) Repetitive DNAs that probably play structural roles in DNA","title":"U3 Genetics"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-13","text":"Genes have loci, chromosomes come in pairs of sister chromatids Mitosis is asexual reproduction, or cell proliferation Human haploid number=23, diploid=46, there are 46 chromosomes each a pair of chromatids Meiosis: each chromosome pair combines/crosses and gets recombinated, and then split into two homologs -> split into four daughter cells/gametes Prophose/Metaphase/Anaphase I/II","title":"Chap 13"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-14","text":"Punnet squares, recessive/dominant alleles Since each person carries two versions of each chromosome (each pair of sister chromatids), etc.etc. \"carriers\", hereditary disease analysis etc.","title":"Chap 14"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-15","text":"Sex-linked genes (live on X,Y) inherit in their own way Linked genes, similar to each other, violate independent selection as predicted by Mendel Percentage of \"recombinants\" can indicated linkedness Aneuploidy is weird copying mistakes resulting in abnormal chromosome counts Genome imprinting, from DNA methylation, from only the mother Other disorders, e.g. mitochondria DNA issues","title":"Chap 15"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-16","text":"Experiments with phages, crystal spectroscopy determined the structure; AT/GC (AC-DC) Semiconservative replication: Primase adds RNA primers pol III adds in the 3'-5' direction (on bottom) on leading strand pol I replaces RNA primers and Okazaki fragments with DNA Rep. fork opened at the origin , held open by helicase Chromatin is made of DNA, histones DNA width 2nm, becomes 10 after wrapping around histones, 30 in a \"fiber\", 300 in the loops which condense to 700 chromosome","title":"Chap 16"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-17","text":"Transcription from RNA polymerase, starts at promoter, TATA box 25 nucleotides before start Transcription unit refers to actual region RNA is spliced by spliceosome (a ribozyme) into ex pressed-ons and in tervening-trons (exons, introns) UTR acts as caps on the ends, for movement/protection/binding to ribosome Translation starts at methionine, ends at stop More complex in eukaryotes; some enzymes chase the 5' end until transcription stops tRNAs map codons to amino acids, created at aminoacyl-tRNA synthetase Different kinds of mutations; missense, deletion, frameshift, nonsense etc.","title":"Chap 17"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-18","text":"Repressors, activators, which react to environment Activator/repressor can be turned on/off by binding to relevant proteins For prokaryotes: Operon consists of promoter, operator (interacts activator/repressor), and all coding regions for one pathway (grouped together) Distal control elements can bend around and help construct transcription factor, once activated Coordinate control can through hormones/other molecules/etc Post-transcription regulation through non-coding RNA (ncRNA) such as miRNA, siRNA, etc EVO-DEVO: determined->differentiated Cells communicate/emerge through induction, signal molecules from direct contact Cytoplasmic determinants are asymmetries in protein/mRNA distribution first discovered is bicoid , in gradient from anterior to posterior Cancer results from oncogenes , control/avoid controlling cell cycle; multi-mutation model of cancer progression","title":"Chap 18"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-19","text":"Virus is nucleic acid surrounded by protein coat, capsid and sometimes viral envelope After DNA injection, two paths: virulent (lytic cycle), make a bunch and kill/attack temperate (lysogenic cycle), make a few and then coexist/inhabit for a while Defenses include CAS and proviruses Viruses can enter through broken cell walls Prions are slow-acting, self-replicating proteins , nearly indestructible","title":"Chap 19"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-20","text":"DNA sequencing by adding dRNA one at a time, flashes occur on binds PCR, using primase/polymerase/nucleotides/buffer to quickly copy fragments Restriction enzymes can make desire recombinant DNA, stitched together by ligase Hacking existing viral tech, like CRISPR Can un-differentiate cells by introducing certain proteins","title":"Chap 20"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U3%20Genetics/#chap-21","text":"Systems biology where bioinfo is used to study whole genes ( genomics ) and protein systems ( proteomics ) Repetitive DNAs that probably play structural roles in DNA","title":"Chap 21"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U4%20Evolution/","text":"Chap 22 Evolution, selection Convergent evolution Chap 23 Several modes of genetic variation: drift (just moves) natural selection flow (new population moves in) Several types of selection: directional, stabilizing, disruptive (for a feature in one direction/to the middle) Balancing Sexual Chap 24 Species def'd : largest group that can mate within itself sustainably Allotropic speciation: gene flow is reduced, diff. evo changes Sympatric: in the same environ, caused by sexual or habitat shifts Hybridization; effects of environ can merge species/keep them separate Chap 25 Life came from: abiotic organic molecules hot surfaces polymerize first life: self-replicating RNA Fossils mark evo history, using radiometric dating (carbon half-life) Large evolutionary events ( adaptive radiations ) catalyzed by global natural disasters (pruning many branches, creating more diversity), and geological movements Developmental genes (HOX) can create rapid body plan changes with small nucleotide changes","title":"U4 Evolution"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U4%20Evolution/#chap-22","text":"Evolution, selection Convergent evolution","title":"Chap 22"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U4%20Evolution/#chap-23","text":"Several modes of genetic variation: drift (just moves) natural selection flow (new population moves in) Several types of selection: directional, stabilizing, disruptive (for a feature in one direction/to the middle) Balancing Sexual","title":"Chap 23"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U4%20Evolution/#chap-24","text":"Species def'd : largest group that can mate within itself sustainably Allotropic speciation: gene flow is reduced, diff. evo changes Sympatric: in the same environ, caused by sexual or habitat shifts Hybridization; effects of environ can merge species/keep them separate","title":"Chap 24"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U4%20Evolution/#chap-25","text":"Life came from: abiotic organic molecules hot surfaces polymerize first life: self-replicating RNA Fossils mark evo history, using radiometric dating (carbon half-life) Large evolutionary events ( adaptive radiations ) catalyzed by global natural disasters (pruning many branches, creating more diversity), and geological movements Developmental genes (HOX) can create rapid body plan changes with small nucleotide changes","title":"Chap 25"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/","text":"Chap 26 Linnaean (Linnaeus) naming system: genus + species domains -> kingdoms -> phyla -> classes -> orders -> family -> genus Homology (shared ancestor) neq analogy (convergent evo) Clade is an entire subtree Orthologous genes were copied during speciation Some DNA change at consistent rate; molecular clock Horizontal gene transfer complicates Chap 27 Prokaryotes! - Cell wall, and then capsule - Circular chromosome, with circular plasmids - No endo-membrones - Flagella or cilia, plus pilus for occasional DNA donation - DNA change also by phage or by absorbing environmental DNA - Nutritional diversity with - photoautrophy (light, producer from air) e.g. cyanobacteria - chemoautotrophy (chemical energy, producer from air) - photoheterotrophy (light, consumer/consumer from organic material) - chemoheterotrophy (chemical, consumer/consumer from organic material) - Bacteria (majority) plus Archea (lots of extremophiles) - Helpful/harmful, also used for big pharma (bacteria) Chap 28 Protists! - plastids were cyanobacteria descendents - Most eukaryotes by species count are protists; there are four major groups - Excavates (modified mitochondria, unique flagella, inside rod) - SAR (DNA similarities lol) - Archaeplastida (plastids, i.e. algae and plants) - Unikonts (fungi, animals, + sisters e.g. amoeba, slime molds) - Cyanobacteria are key ecosystem producers Chap 29 Plant evo 1 - Evolved from algae; protective layer of sporopollenin may have allowed going on land - Plants have cuticles, stomata , algae don't have - Mosses have gametophytes , gamete organisms, leading to alternating generations - Ferns were the first tall plants; unseeded; had vascular tissue that can carry water against gravity Chap 30 Plant evo 2 - Gymnosperms : no shell around seeds - Angiosperms : fruit, flowers, seeds, dandelion etc. Include monocot/eudicot/etc. Pollination is good. Chap 31 All fungi are heterotrophs, can secrete enzymes Grow hyphae filaments to collect food, can become mycelia networks of branched hyphae Reproduce sexually or asexually, with spores Diverged from animals at unicellular flagellum ancestor Chytrids: flagella spores Zygomycetes: zygosporangium for sex Glomeromycota: formed with plants Ascomycetes: sexual spores in sacs, many asexual as well Basidiomycetes: Elaborate fruiting with sexual spores Chap 32 Animal def'n Multicellular, heterotrophic, with regulatory genes determining some asymmetric body plan Generally left/right, top/down symmetry, with ventral/dorsal and anterior/posterior Embyro from one cell which divides, and (blastopore) folds in on itself to create digestive system Dipoblastic or Triploblastic (depending on # germ layers) Sponges, then shelled animals w/ digestive systems, then fish, then vertebrates mammals humans Larvae have distinct morphology from adults sometimes Chap 33 Invertebrates! - Sponges: no tissues, just cells - Cnidarians: stinging cells, diploblastic, radially symmetric, single-opening digestion - Lophotrochozoans: DNA-linked, include some worms, mollusks - Ecdysozoans: Roundworms, chitin exoskeletons, includes insects - Deuterostomes: sea stars, sea urchins, fish/vertebrates Chap 34 Vertebrates! The intro says it all. - Chordates have a notochord , a stiff, fibrous rod acting as basal skeleton, and a dorsal hollow nerve cord - Vertebrates have a backbone - Gnathostomes have jaws; tetrapods have limbs - Amniotes have a terrestially adapted egg - Mammals have hair and produce milk - Humans, emerged 1-3MY ago, used tools, bipedal, large brain If not tetrapod, prolly fish If not amniote, amphibia If not mammal, then reptile","title":"U5 Evo History"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-26","text":"Linnaean (Linnaeus) naming system: genus + species domains -> kingdoms -> phyla -> classes -> orders -> family -> genus Homology (shared ancestor) neq analogy (convergent evo) Clade is an entire subtree Orthologous genes were copied during speciation Some DNA change at consistent rate; molecular clock Horizontal gene transfer complicates","title":"Chap 26"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-27","text":"Prokaryotes! - Cell wall, and then capsule - Circular chromosome, with circular plasmids - No endo-membrones - Flagella or cilia, plus pilus for occasional DNA donation - DNA change also by phage or by absorbing environmental DNA - Nutritional diversity with - photoautrophy (light, producer from air) e.g. cyanobacteria - chemoautotrophy (chemical energy, producer from air) - photoheterotrophy (light, consumer/consumer from organic material) - chemoheterotrophy (chemical, consumer/consumer from organic material) - Bacteria (majority) plus Archea (lots of extremophiles) - Helpful/harmful, also used for big pharma (bacteria)","title":"Chap 27"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-28","text":"Protists! - plastids were cyanobacteria descendents - Most eukaryotes by species count are protists; there are four major groups - Excavates (modified mitochondria, unique flagella, inside rod) - SAR (DNA similarities lol) - Archaeplastida (plastids, i.e. algae and plants) - Unikonts (fungi, animals, + sisters e.g. amoeba, slime molds) - Cyanobacteria are key ecosystem producers","title":"Chap 28"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-29","text":"Plant evo 1 - Evolved from algae; protective layer of sporopollenin may have allowed going on land - Plants have cuticles, stomata , algae don't have - Mosses have gametophytes , gamete organisms, leading to alternating generations - Ferns were the first tall plants; unseeded; had vascular tissue that can carry water against gravity","title":"Chap 29"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-30","text":"Plant evo 2 - Gymnosperms : no shell around seeds - Angiosperms : fruit, flowers, seeds, dandelion etc. Include monocot/eudicot/etc. Pollination is good.","title":"Chap 30"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-31","text":"All fungi are heterotrophs, can secrete enzymes Grow hyphae filaments to collect food, can become mycelia networks of branched hyphae Reproduce sexually or asexually, with spores Diverged from animals at unicellular flagellum ancestor Chytrids: flagella spores Zygomycetes: zygosporangium for sex Glomeromycota: formed with plants Ascomycetes: sexual spores in sacs, many asexual as well Basidiomycetes: Elaborate fruiting with sexual spores","title":"Chap 31"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-32","text":"Animal def'n Multicellular, heterotrophic, with regulatory genes determining some asymmetric body plan Generally left/right, top/down symmetry, with ventral/dorsal and anterior/posterior Embyro from one cell which divides, and (blastopore) folds in on itself to create digestive system Dipoblastic or Triploblastic (depending on # germ layers) Sponges, then shelled animals w/ digestive systems, then fish, then vertebrates mammals humans Larvae have distinct morphology from adults sometimes","title":"Chap 32"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-33","text":"Invertebrates! - Sponges: no tissues, just cells - Cnidarians: stinging cells, diploblastic, radially symmetric, single-opening digestion - Lophotrochozoans: DNA-linked, include some worms, mollusks - Ecdysozoans: Roundworms, chitin exoskeletons, includes insects - Deuterostomes: sea stars, sea urchins, fish/vertebrates","title":"Chap 33"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U5%20Evo%20History/#chap-34","text":"Vertebrates! The intro says it all. - Chordates have a notochord , a stiff, fibrous rod acting as basal skeleton, and a dorsal hollow nerve cord - Vertebrates have a backbone - Gnathostomes have jaws; tetrapods have limbs - Amniotes have a terrestially adapted egg - Mammals have hair and produce milk - Humans, emerged 1-3MY ago, used tools, bipedal, large brain If not tetrapod, prolly fish If not amniote, amphibia If not mammal, then reptile","title":"Chap 34"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/","text":"Chap 35 Organs, tissues, cells Stems, leaves, flowers Apical (apex) bud vs axillary (side) bud Branch points are called nodes , in-between internode Dermal (skin), Vascular (carry water, nutrients), Ground (other stuff) Xylem (water), Phloem (nutrients) constitute vascular Parenchyma undifferentiated, synthesis/storage Collenchyma support young parts Sclerenchyma have thick walls Tracheids, vessel elements are dead but carry water Sieve-tube elements , devoid of organelles, transport sugar Primary growth elongates, Secondary growth widens Meristem , undifferentiated tissue; vascular cambium produces secondary xylem/phloem; cork cambium provides covering for woody plants Chap 36 Recall plasmodesmata , thin cytoplasm between adjacent plant cells Fungal hyphae indirectly offer larger surface area for roots Apoplast, outside plasma membranes; symplast , inside Passive osmosis/respiration moves water up, cohesion-tension hypothesis Wilting is water lost from transpiration not being replaced Stomata can be closed; controlled by light, CO2, drought hormone abscisic acid , circadian Sucrose cotransported with H+, moved with proton pumps Symplast can be regulated dynamically Chap 37 Soil has several layers; topsoil is what we care about, composed of sand, silt, clay, optimally equal parts A lot is of topsoil is humus, aka dead stuff Hope is to hold equal parts air/water, and water is saturated with minerals Macronutrients include C,O,H,N, plus some other stuff Rhizosphere is soil near the roots; rhizobacteria+others live symbiotically with plants Only bacteria can \"fixate\" nitrogen, i.e. N2->HN3, done anaerobically with dead materials Legumes can be \"infected\" by rhizobacteria, replenishing soil nitrogen -> crop rotation Mycorrhizae is hyphae on plant roots Chap 38 Angiosperm, like mosses, have gametophyte alternate generations; gametophyte is the flower, composed of Sepals, bud protection Petals Stamens, the sticks bearing anthers where microspores become pollen grains Carpels, the embryos Double fertilization where one becomes zygote, other becomes food-storing endosperm Many features to avoid self-fertilization Chap 39 Signal transduction as before, with second messengers, mostly Ca2+ ==Hormones== control plant growth Auxin: stimulates elongation; regulates bending, produced at apex Cytokinins, stimulates division; promotes later bud growth Gibberellins: promotes stem elongation, helps seeds break dormancy Abscisic acid: (abscission of leaves) closes stoma, keeps seed dormant Ethylene: creates triple response , plant begins \"changing direction\" Light responses are critical; shoot begins greening (de-etiolation) as soon as light is encountered","title":"U6 Plant Function"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/#chap-35","text":"Organs, tissues, cells Stems, leaves, flowers Apical (apex) bud vs axillary (side) bud Branch points are called nodes , in-between internode Dermal (skin), Vascular (carry water, nutrients), Ground (other stuff) Xylem (water), Phloem (nutrients) constitute vascular Parenchyma undifferentiated, synthesis/storage Collenchyma support young parts Sclerenchyma have thick walls Tracheids, vessel elements are dead but carry water Sieve-tube elements , devoid of organelles, transport sugar Primary growth elongates, Secondary growth widens Meristem , undifferentiated tissue; vascular cambium produces secondary xylem/phloem; cork cambium provides covering for woody plants","title":"Chap 35"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/#chap-36","text":"Recall plasmodesmata , thin cytoplasm between adjacent plant cells Fungal hyphae indirectly offer larger surface area for roots Apoplast, outside plasma membranes; symplast , inside Passive osmosis/respiration moves water up, cohesion-tension hypothesis Wilting is water lost from transpiration not being replaced Stomata can be closed; controlled by light, CO2, drought hormone abscisic acid , circadian Sucrose cotransported with H+, moved with proton pumps Symplast can be regulated dynamically","title":"Chap 36"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/#chap-37","text":"Soil has several layers; topsoil is what we care about, composed of sand, silt, clay, optimally equal parts A lot is of topsoil is humus, aka dead stuff Hope is to hold equal parts air/water, and water is saturated with minerals Macronutrients include C,O,H,N, plus some other stuff Rhizosphere is soil near the roots; rhizobacteria+others live symbiotically with plants Only bacteria can \"fixate\" nitrogen, i.e. N2->HN3, done anaerobically with dead materials Legumes can be \"infected\" by rhizobacteria, replenishing soil nitrogen -> crop rotation Mycorrhizae is hyphae on plant roots","title":"Chap 37"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/#chap-38","text":"Angiosperm, like mosses, have gametophyte alternate generations; gametophyte is the flower, composed of Sepals, bud protection Petals Stamens, the sticks bearing anthers where microspores become pollen grains Carpels, the embryos Double fertilization where one becomes zygote, other becomes food-storing endosperm Many features to avoid self-fertilization","title":"Chap 38"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U6%20Plant%20Function/#chap-39","text":"Signal transduction as before, with second messengers, mostly Ca2+ ==Hormones== control plant growth Auxin: stimulates elongation; regulates bending, produced at apex Cytokinins, stimulates division; promotes later bud growth Gibberellins: promotes stem elongation, helps seeds break dormancy Abscisic acid: (abscission of leaves) closes stoma, keeps seed dormant Ethylene: creates triple response , plant begins \"changing direction\" Light responses are critical; shoot begins greening (de-etiolation) as soon as light is encountered","title":"Chap 39"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/","text":"Chap 40 No cell wall, need folded membranes for aqueous surface area for cells Organ systems -> organs -> tissues Epitheliel tissue interfaces external/internal surfaces Connective tissue binds and supports Muscle tissue Skeleta/striated, long fibers of merged cells yield stripes Smooth muscle, for involuntary Cardiac muscle, striated, but fibers relay signals and sync heart contraction Nervous tissue Endocrine/hormone system, nervous system Thermoregulation: endothermic (from within) vs ectothermic (from outside) Homeostasis is the means by which smth is kept constant By surface/volume, metabolism rate per gram is inversely related to size Chap 41 Essential nutrients include Amino acids, Fatty acids, Vitamins, Carbon chains, energy/ATP Feeding strategies: bulk, filter, substrate, fluid Compartmentalization (via blastopore), yields either Gastrovascular cavity (goes in/out) Alimentary canal (long path with sacs, allows parallel) Generally: Mouth/esophagus adds saliva, careful involuntary engineering controls airway (trachea) closure, sphincter for esophagus opens Gastric glands, tubes in the tissue of stomach, adds HCl, create mucus, secretions In small intestine: liver->bile in gallbladder->emulsifies fats pancreas adds basic bicarbonate Villi with hairs absorb nutrients Chylomicron go into bloodstream After small intestine goes to colon (recovers water) and cecum (ferments fibers) Rectum holds shit until defecated Herbivore cellulose digestion requires bacteria Regulation through various hormones affecting liver/gallbladder/stomach secretions to intestin Insulin increases glucose->cell uptake, glucagon decreases, both from pancreas Diabetes I: insulin cells in pancrease destroyed by immune system Diabetes II: insulin produced correctly, but cells fail to respond due to inactivity/diet Chap 42 Arteries take blood away from heart, veins towards Capillaries are tighter areas, blood passes more slowly, more thin Marine invertebrates use hemolymph i.e. their blood is just the surrounding fluid Fish have one-pump heart, two spaces (one atra in, other ventricle out); reptiles have three (two atria in, one ventricle pushes out, alternating); mamals have pulmonary circuit (two suck in, two push out) Pulmonary artery/veins from heart to lungs; systemic arteries/veins from heart to system (four parts, each unique name) Like an engine, valves control everything Chemical gradients (again!) used to transfer O2, CO2 passively from air/blood, through epithelial cells Amphibians use positive pressure ; birds use one-way air sacs ; mammals expand rib/diaphragm using negative pressure More regulation through pH measures in aorta, arteries Chap 43 Innate immunity: Skin, lysozomes in acid/saliva/mucus/etc., phagocytic cells that combine with lysosomes, complement system proteins, interferons, etc. antimicrobial peptides Histamine creates inflation, increasing immune cell access Adaptive immunity, run from system recognizing antigens outside pathogens Lymphocytes, B/T cells; B have antigen receptors , it's a diceroll, find one that matches Phagocyutes/B cells present the antigens on their boundary; helper T cells pick them up Cytotoxic T cells match reported antigens and attack; antibodies bind to antigens marking phagocytosis ( humoral response ), complement system lysis Antibodies lead to passive immunity/vaccines/etc Self-tolerance i.e. immune system attacks itself HIV, resulting disease called AIDS, destroyed helper T cells No such immune defense against cancer cells, only against viruses Chap 44 Osmoregulation: maintaining ratios of solvents, waters, etc. Freshwater fish have salt transported out actively by gills, water comes in naturally; marine fish are opposite; terrestial vertebrates are a mix Nitrogenous wastes are important; ammonia is toxic, fish dispose directly, vertebates must synthesize into urea or uric acid (greater energy cost) Excretion through filtration, reabsorption, secretion, excretion . Other types of animals don't have kidneys; all use tubules wrapped around blood and mix of osmosis/active transfer/selective permeability/blood pressure/osmosis Kidney is substructure with many nephrons (capillary-wrappers) Chap 45 Endocrine, aka hormones; paracrine for neighbors, autocrine for self Neurotransmitters from nerve cells act locally; neurohormones go into blood; pheromones into the environment Polypeptides (insulin), steroids (all lipids), and amines are the three classes Water soluable bind to receptors; lipid-soluble pass through membrane into interior Endocrine starts from hypothalamus, posterior pituitary/anterior pituitary Endocrine is self-negative-feedback, neuroendocrine (such as oxytocin/opiate/epinephrine) is self-positive-feedback Long hormone cascades from hypothalamus to other parts Gonads are main source of sex hormones: androgens (e.g. testosterone) more in males, estrogens (e.g. estradiol), progesterones more in females Chap 46 Diploid zygote Asexual, usually among females, sometimes precipitated by mating-like action; hermaphrodite, each fertilize the other Fertilization via gonads , all animals have; anatomically: sperm produced in testes, stored in seminal vesicle, ejected by mascular vas deferens eggs completed meiosis before birth, waiting in ovaries; after maturation move to uterus, next step depends on fertilization strategy Spermatogenesis in males (continuous), oogenesis in females (periodic) Chap 47 Sperm/egg have small vesicles on boundary; sperm tip dissolves egg jelly wall, trigging Ca2+ wave across egg; cortical granule release in egg instigates fertilization envelope (blocking polyspermy) Then cleavage , splitting into ~100 blastomeres, creating blastula that has an asymmetric internal cavity; in mammals blastocyst embeds itself into uterus Gastrula -> membrane folds in on itself, tube goes all the way through, become digestive tract Membranes fold/pinch to create multiple germ layers : Ectoderm, outer layer Mesoderm, middle layer Endoderm, innermost tissues/organs/digestion Notochord/neural cord Organogenesis/morphogensis occurs now Specific parts of blastula become specific body parts Chap 48 Most of brain is glial cells wrapped around neuron, e.g. Schwann cells forming myelin sheath around axon Dendrites are receivers, synapses are senders Sensory neurons, interneurons, motor neurons Ion pumps of Na+, K+ create voltage difference, just like other cells Pump exchanges Na+, K+, most Na+ gates closed, so inside Na+ concentration depressed; K+ gates are all open always, K+ concentration stays constant After signal received from synapse, binds to ligand-gated ion channel , ion gates open triggering Na+ inflow, then Ka+ outflow, bringing membrane potential back to resting potential Action potential is a brief actionable peak of potential; jumps from one node of Ranvier to the next along axon Chap 49 Central nervous system is interneurons; peripheral NS is sensory/motor neurons Sensory -> afferent -> CNS -> efferent -> motor, autonomic -> sympathetic (excited), parasympathetic (cooldown, digestion), enteric (digestion, pancreas) Reactive neural circuits in spinal cord Brain three parts: Forebrain: cerebrum (most visible part), thalamus (sensory traffic hub), hypothalamus (homeostasis, survival instincts) Midbrain: goofy ahh sensory stuff, small for humans Hindbrain: cerebellum (motor, sensor), pons/medulla oblongata (sensory hubs to cerebrum) Emotions dependent on limbic system (thalamus, hypothalamus, hippocampus, amygdala) bordering brainstem Thinking happens on cerebral cortex outside of cerebrum, inside is white matter , connections/wires Hippocampus for working-memory temporary links, cerebral cortex for long-term Cortex has four sides; left is math/logic/language, right is pattern/nonverbal/\"art\" Higher thinking->wonkier more folded cortex Reshaping of connections is learning Schizophrenia is broken dopamine, addiction is broken reward system, Alzheimers is amyloid plaques, Parkinson's is death of dopamine-neurons Chap 50 Sensory transduction from CNS -> motor system Mechano-, chemo- electromagnetic, thermo-receptors Invertabrates have statocysts , cilia balls with sand grains determining orientation Outer ear collects sound, middle ear in tympanic membrane (eardrum) transmits to bones of middle ear, through oval window to fluid in two bone canals contained in cochlea In between bone canals are hair cells that signal each time direction change Receptors can detect acceleration via viscous blobs of liquid moving around Vertebrate eye, a single eye with pigmented muscle-like choroid , exposed part is iris, choroid attached to suspensory ligaments contracts/relaxes to squeeze and move the lens Lens in front is aqueous humor , behind is vitreous humor Choroid surrounded by sclera and cornea , white stuff Retina inside choroid, all rod/cone/epithelial cells connected to bipolar signalers to neurons Light -> protein active -> channels open/close -> bipolar signalers have potential change -> signal Focusing area fovea mostly cones (color), can see sharp images; rods for seeing in the dark, more sensitive to light Taste through papilla on tongue, donut-like indentations into the muscle with taste buds embedded on the walls, taste buds have sensory receptors, are linked to neurons Smell is similar; olfactory receptor cells embedded in epithelial cells, with cilia that detect odorants in mucus Muscle -> bundle of fibers -> each fiber is own cell, with membrane -> contains myofibrils Each myofibril striated with sarcomeres segments; thin actin filament attached to sarcomere boundary, thick myosin filament in the middle Neurons signal gates to open releasing Ca+ into sarcoplasmic reticulum surrounding myofibril Ca+ exposes myosin binding sites, myosin grabs the actin and walks five times a second (via ATP phosphorylation protein shape change) along the actin creating contraction Energy burn ladder: ATP <- glucose <= glycogen <= oxygen limited; lactic acid ferment Smooth is just not striated, more uniform Skeletons exist lmao Chap 51 Behavior defn'd responses to external/internal stimuli Learning happens through imprinting (of young animals), cognition (prediction), associative learning (experience), social learning (from others) Weird social behaviors explained by sexual competition/picking fit males; e.g. taking a mate that others have already taken Opposite; mating for life; altruism etc.","title":"U7 Animal Function"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-40","text":"No cell wall, need folded membranes for aqueous surface area for cells Organ systems -> organs -> tissues Epitheliel tissue interfaces external/internal surfaces Connective tissue binds and supports Muscle tissue Skeleta/striated, long fibers of merged cells yield stripes Smooth muscle, for involuntary Cardiac muscle, striated, but fibers relay signals and sync heart contraction Nervous tissue Endocrine/hormone system, nervous system Thermoregulation: endothermic (from within) vs ectothermic (from outside) Homeostasis is the means by which smth is kept constant By surface/volume, metabolism rate per gram is inversely related to size","title":"Chap 40"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-41","text":"Essential nutrients include Amino acids, Fatty acids, Vitamins, Carbon chains, energy/ATP Feeding strategies: bulk, filter, substrate, fluid Compartmentalization (via blastopore), yields either Gastrovascular cavity (goes in/out) Alimentary canal (long path with sacs, allows parallel) Generally: Mouth/esophagus adds saliva, careful involuntary engineering controls airway (trachea) closure, sphincter for esophagus opens Gastric glands, tubes in the tissue of stomach, adds HCl, create mucus, secretions In small intestine: liver->bile in gallbladder->emulsifies fats pancreas adds basic bicarbonate Villi with hairs absorb nutrients Chylomicron go into bloodstream After small intestine goes to colon (recovers water) and cecum (ferments fibers) Rectum holds shit until defecated Herbivore cellulose digestion requires bacteria Regulation through various hormones affecting liver/gallbladder/stomach secretions to intestin Insulin increases glucose->cell uptake, glucagon decreases, both from pancreas Diabetes I: insulin cells in pancrease destroyed by immune system Diabetes II: insulin produced correctly, but cells fail to respond due to inactivity/diet","title":"Chap 41"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-42","text":"Arteries take blood away from heart, veins towards Capillaries are tighter areas, blood passes more slowly, more thin Marine invertebrates use hemolymph i.e. their blood is just the surrounding fluid Fish have one-pump heart, two spaces (one atra in, other ventricle out); reptiles have three (two atria in, one ventricle pushes out, alternating); mamals have pulmonary circuit (two suck in, two push out) Pulmonary artery/veins from heart to lungs; systemic arteries/veins from heart to system (four parts, each unique name) Like an engine, valves control everything Chemical gradients (again!) used to transfer O2, CO2 passively from air/blood, through epithelial cells Amphibians use positive pressure ; birds use one-way air sacs ; mammals expand rib/diaphragm using negative pressure More regulation through pH measures in aorta, arteries","title":"Chap 42"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-43","text":"Innate immunity: Skin, lysozomes in acid/saliva/mucus/etc., phagocytic cells that combine with lysosomes, complement system proteins, interferons, etc. antimicrobial peptides Histamine creates inflation, increasing immune cell access Adaptive immunity, run from system recognizing antigens outside pathogens Lymphocytes, B/T cells; B have antigen receptors , it's a diceroll, find one that matches Phagocyutes/B cells present the antigens on their boundary; helper T cells pick them up Cytotoxic T cells match reported antigens and attack; antibodies bind to antigens marking phagocytosis ( humoral response ), complement system lysis Antibodies lead to passive immunity/vaccines/etc Self-tolerance i.e. immune system attacks itself HIV, resulting disease called AIDS, destroyed helper T cells No such immune defense against cancer cells, only against viruses","title":"Chap 43"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-44","text":"Osmoregulation: maintaining ratios of solvents, waters, etc. Freshwater fish have salt transported out actively by gills, water comes in naturally; marine fish are opposite; terrestial vertebrates are a mix Nitrogenous wastes are important; ammonia is toxic, fish dispose directly, vertebates must synthesize into urea or uric acid (greater energy cost) Excretion through filtration, reabsorption, secretion, excretion . Other types of animals don't have kidneys; all use tubules wrapped around blood and mix of osmosis/active transfer/selective permeability/blood pressure/osmosis Kidney is substructure with many nephrons (capillary-wrappers)","title":"Chap 44"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-45","text":"Endocrine, aka hormones; paracrine for neighbors, autocrine for self Neurotransmitters from nerve cells act locally; neurohormones go into blood; pheromones into the environment Polypeptides (insulin), steroids (all lipids), and amines are the three classes Water soluable bind to receptors; lipid-soluble pass through membrane into interior Endocrine starts from hypothalamus, posterior pituitary/anterior pituitary Endocrine is self-negative-feedback, neuroendocrine (such as oxytocin/opiate/epinephrine) is self-positive-feedback Long hormone cascades from hypothalamus to other parts Gonads are main source of sex hormones: androgens (e.g. testosterone) more in males, estrogens (e.g. estradiol), progesterones more in females","title":"Chap 45"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-46","text":"Diploid zygote Asexual, usually among females, sometimes precipitated by mating-like action; hermaphrodite, each fertilize the other Fertilization via gonads , all animals have; anatomically: sperm produced in testes, stored in seminal vesicle, ejected by mascular vas deferens eggs completed meiosis before birth, waiting in ovaries; after maturation move to uterus, next step depends on fertilization strategy Spermatogenesis in males (continuous), oogenesis in females (periodic)","title":"Chap 46"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-47","text":"Sperm/egg have small vesicles on boundary; sperm tip dissolves egg jelly wall, trigging Ca2+ wave across egg; cortical granule release in egg instigates fertilization envelope (blocking polyspermy) Then cleavage , splitting into ~100 blastomeres, creating blastula that has an asymmetric internal cavity; in mammals blastocyst embeds itself into uterus Gastrula -> membrane folds in on itself, tube goes all the way through, become digestive tract Membranes fold/pinch to create multiple germ layers : Ectoderm, outer layer Mesoderm, middle layer Endoderm, innermost tissues/organs/digestion Notochord/neural cord Organogenesis/morphogensis occurs now Specific parts of blastula become specific body parts","title":"Chap 47"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-48","text":"Most of brain is glial cells wrapped around neuron, e.g. Schwann cells forming myelin sheath around axon Dendrites are receivers, synapses are senders Sensory neurons, interneurons, motor neurons Ion pumps of Na+, K+ create voltage difference, just like other cells Pump exchanges Na+, K+, most Na+ gates closed, so inside Na+ concentration depressed; K+ gates are all open always, K+ concentration stays constant After signal received from synapse, binds to ligand-gated ion channel , ion gates open triggering Na+ inflow, then Ka+ outflow, bringing membrane potential back to resting potential Action potential is a brief actionable peak of potential; jumps from one node of Ranvier to the next along axon","title":"Chap 48"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-49","text":"Central nervous system is interneurons; peripheral NS is sensory/motor neurons Sensory -> afferent -> CNS -> efferent -> motor, autonomic -> sympathetic (excited), parasympathetic (cooldown, digestion), enteric (digestion, pancreas) Reactive neural circuits in spinal cord Brain three parts: Forebrain: cerebrum (most visible part), thalamus (sensory traffic hub), hypothalamus (homeostasis, survival instincts) Midbrain: goofy ahh sensory stuff, small for humans Hindbrain: cerebellum (motor, sensor), pons/medulla oblongata (sensory hubs to cerebrum) Emotions dependent on limbic system (thalamus, hypothalamus, hippocampus, amygdala) bordering brainstem Thinking happens on cerebral cortex outside of cerebrum, inside is white matter , connections/wires Hippocampus for working-memory temporary links, cerebral cortex for long-term Cortex has four sides; left is math/logic/language, right is pattern/nonverbal/\"art\" Higher thinking->wonkier more folded cortex Reshaping of connections is learning Schizophrenia is broken dopamine, addiction is broken reward system, Alzheimers is amyloid plaques, Parkinson's is death of dopamine-neurons","title":"Chap 49"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-50","text":"Sensory transduction from CNS -> motor system Mechano-, chemo- electromagnetic, thermo-receptors Invertabrates have statocysts , cilia balls with sand grains determining orientation Outer ear collects sound, middle ear in tympanic membrane (eardrum) transmits to bones of middle ear, through oval window to fluid in two bone canals contained in cochlea In between bone canals are hair cells that signal each time direction change Receptors can detect acceleration via viscous blobs of liquid moving around Vertebrate eye, a single eye with pigmented muscle-like choroid , exposed part is iris, choroid attached to suspensory ligaments contracts/relaxes to squeeze and move the lens Lens in front is aqueous humor , behind is vitreous humor Choroid surrounded by sclera and cornea , white stuff Retina inside choroid, all rod/cone/epithelial cells connected to bipolar signalers to neurons Light -> protein active -> channels open/close -> bipolar signalers have potential change -> signal Focusing area fovea mostly cones (color), can see sharp images; rods for seeing in the dark, more sensitive to light Taste through papilla on tongue, donut-like indentations into the muscle with taste buds embedded on the walls, taste buds have sensory receptors, are linked to neurons Smell is similar; olfactory receptor cells embedded in epithelial cells, with cilia that detect odorants in mucus Muscle -> bundle of fibers -> each fiber is own cell, with membrane -> contains myofibrils Each myofibril striated with sarcomeres segments; thin actin filament attached to sarcomere boundary, thick myosin filament in the middle Neurons signal gates to open releasing Ca+ into sarcoplasmic reticulum surrounding myofibril Ca+ exposes myosin binding sites, myosin grabs the actin and walks five times a second (via ATP phosphorylation protein shape change) along the actin creating contraction Energy burn ladder: ATP <- glucose <= glycogen <= oxygen limited; lactic acid ferment Smooth is just not striated, more uniform Skeletons exist lmao","title":"Chap 50"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U7%20Animal%20Function/#chap-51","text":"Behavior defn'd responses to external/internal stimuli Learning happens through imprinting (of young animals), cognition (prediction), associative learning (experience), social learning (from others) Weird social behaviors explained by sexual competition/picking fit males; e.g. taking a mate that others have already taken Opposite; mating for life; altruism etc.","title":"Chap 51"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/","text":"Chap 52 Several parts; climate, abiotic factors Biomes (specific areas/systems) Convection in water bodies across different seasons exists Thermocline separates uniformly warm from uniformly cold Chap 53 Applied ecological theory: population should increase exponentially, actually logistic growth determines carrying capacity Population growth/birth, death rates depend on density; weird DEs Chap 54 Types of food consumers/interactions, energy efficiency etc Food webs/chains, species diversity Thinking bottom-up (energy flow) vs top-down (population control) Successive land/ecosystem transformation, e.g. by nitrogen fixation by plants Chap 55 Energy transfer between trophic levels is ==10%== efficient Introduction of a single new species (e.g. foxes eating detritus birds on islands, reducing fertilization) esp. at a higher trophic level can quickly transform ecosystems Mass/energy transfer can be accounted Plants/fungi are key for reclaiming water, minerals, nutrients; otherwise erosion, soil runoff etc. Sometimes people engineer ecosystems by introducing suitable species Chap 56 Biodiversity: genetic -> species -> ecosystem diversity Landscape conservation tries to main biodiversity , population conservation tries to maintain above minimum","title":"U8 Ecology"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/#chap-52","text":"Several parts; climate, abiotic factors Biomes (specific areas/systems) Convection in water bodies across different seasons exists Thermocline separates uniformly warm from uniformly cold","title":"Chap 52"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/#chap-53","text":"Applied ecological theory: population should increase exponentially, actually logistic growth determines carrying capacity Population growth/birth, death rates depend on density; weird DEs","title":"Chap 53"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/#chap-54","text":"Types of food consumers/interactions, energy efficiency etc Food webs/chains, species diversity Thinking bottom-up (energy flow) vs top-down (population control) Successive land/ecosystem transformation, e.g. by nitrogen fixation by plants","title":"Chap 54"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/#chap-55","text":"Energy transfer between trophic levels is ==10%== efficient Introduction of a single new species (e.g. foxes eating detritus birds on islands, reducing fertilization) esp. at a higher trophic level can quickly transform ecosystems Mass/energy transfer can be accounted Plants/fungi are key for reclaiming water, minerals, nutrients; otherwise erosion, soil runoff etc. Sometimes people engineer ecosystems by introducing suitable species","title":"Chap 55"},{"location":"Material%20Knowledge/Sciences/Campbell%20Reading/01-13-24%20speedread/U8%20Ecology/#chap-56","text":"Biodiversity: genetic -> species -> ecosystem diversity Landscape conservation tries to main biodiversity , population conservation tries to maintain above minimum","title":"Chap 56"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/","text":"Already Mastered h,l for left, right j,k for down, up y,p for copy, paste combine as in y3w , use p/P v for visual o,O for open new line, after, before i,a obvious w,b known, capitalize for only whitespace no punc. . repeat, ; next using find character f , , to find previous >>,<< for indents, pref with N to apply below lines 0,^,$ for line movement Use / to search, n,N for movement Use ? to search backwards i for inner (no whitespace) e.g. diw , a for outer, also can do diW etc. V for visual by line e for word end C,D delete to end; cc,dd delete whole; but Y,yy are equiv. A for append to end of sentence t basically fh % to jump to matching ({[ Learning :reg to see list of registers; \"1p to paste yank before this, \"ayw to yank word to \"a ge for back word end {,[]()} for first new whitespace after last para (,) for { plus firstline para gU<>,gu<>,~ for case (last toggle) CTRL-O to old positions, CTRL-I to new ones CTRL-U , CTRL-D use q[x] to record macro to key [x] <UP> to try previous searches from search history m[x] create mark, '[x] to go to mark J used to join selected lines with space delim :grep to use grep inside current file s{char}{char} v-sneak, can use op, in Obs just cl ]] next { in first column; [[ next } in first column [( for previous unmatched ( and [) for next unmatched ) ; aka. previous/next in this \"scope\" cs{sel}([ make a selection, replace surrounding ( with [ similarly ds , or ys to add WTF <CTRL-W> S/V to split (horizontal line) or (vertical line) <CTRL-W> h/j/k/l to move windows","title":"Vim Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#already-mastered","text":"h,l for left, right j,k for down, up y,p for copy, paste combine as in y3w , use p/P v for visual o,O for open new line, after, before i,a obvious w,b known, capitalize for only whitespace no punc. . repeat, ; next using find character f , , to find previous >>,<< for indents, pref with N to apply below lines 0,^,$ for line movement Use / to search, n,N for movement Use ? to search backwards i for inner (no whitespace) e.g. diw , a for outer, also can do diW etc. V for visual by line e for word end C,D delete to end; cc,dd delete whole; but Y,yy are equiv. A for append to end of sentence t basically fh % to jump to matching ({[","title":"Already Mastered"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#learning","text":":reg to see list of registers; \"1p to paste yank before this, \"ayw to yank word to \"a ge for back word end {,[]()} for first new whitespace after last para (,) for { plus firstline para gU<>,gu<>,~ for case (last toggle) CTRL-O to old positions, CTRL-I to new ones CTRL-U , CTRL-D use q[x] to record macro to key [x] <UP> to try previous searches from search history m[x] create mark, '[x] to go to mark J used to join selected lines with space delim :grep to use grep inside current file s{char}{char} v-sneak, can use op, in Obs just cl ]] next { in first column; [[ next } in first column [( for previous unmatched ( and [) for next unmatched ) ; aka. previous/next in this \"scope\" cs{sel}([ make a selection, replace surrounding ( with [ similarly ds , or ys to add","title":"Learning"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#wtf","text":"<CTRL-W> S/V to split (horizontal line) or (vertical line) <CTRL-W> h/j/k/l to move windows","title":"WTF"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/","text":"Plonky3 ALI setup Programs are represented as a matrix of numbers that satisfy low-degree polynomial constraints on adjacent rows. The height of the matrix must be a power of 2: $h=2^k$. Take the base field $F$ and an evaluation domain $L$, which is a multiplicative subgroup of order $h$. Then each column is encoded as a polynomial $C_{i}(\\omega^{j})$, the $i$th item in the $j$th row. (Power of 2 is good because it makes FFT easier.) We commit to each column by Merkle hashing its evaluation over $L$. In particular, constraints $Q(x_{i},y_{i})$ [where $x_{i}$ is the current row, $y_{i}$ is the next row] are represented as $$\\forall x \\in L \\colon Q(C_{i}(x),C_{i}(\\omega \\cdot x))=0.$$ Let $H_{i}$ be the polynomial with roots that $Q_{i}$ must vanish on, let $H$ be the polynomial with all roots of L, and $s_{i}=\\frac{H}{H_{i}}$. Now we want to show $H_{i}|Q_i,$ or instead $H | Q_{i}s_{i}$. Quotient polynomial definition Instead of proving this for each constraint one at a time, the verifier stitches all the constraints together using $\\theta \\in F_{ext}.$ $F_{ext}$ is a low-degree extension of $F$, with order $|F|^{d}$. For default plonky3 the field $F=\\mathbb F_{15\\cdot{2}^{27}+1}$ and $d=4$ (quartic field extension), yielding 128-bit security. Stitching is just $$ R(x)=\\sum_{i} \\theta^{i}Q_{i}(C_{j})\\cdot s_{i}(C_{j}). $$ Then we want to show $$ Q(x)=\\frac{R(x)}{H(x)} $$ exists. Let log_blowup=2 , meaning we blow up $L$ by 4, so $L_{b}$ is now a multiplicative subgroup of order $4h$. Then interpolate the degree-$h$ polynomials into $L_{b}$ to create a $4h$-long vector, and Merkle hash it, and the quotient $Q$. PCS Now apply FRI to the Merkle hash of $Q$ to prove it's low degree. Finally, prove $Q$ is actually the quotient we want by opening the quotient $Q$ (actually represented as 2 polynomials with same length as $C$) and all the columns $C$ at $\\zeta$ from $F_{ext}$. Verify that $Q(\\zeta)H(\\zeta)=R(\\zeta)$. Prove the opening by supplying $V=C(\\zeta)$ and directly running FRI on the rational function $Q'=\\frac{{C(x)-V}}{x-\\zeta}$. Finally, note that we could have directly run FRI on the original quotient polynomial but that would require opening every column for every FRI query. Using ALI we committed to the quotient $Q$ and queried it before running FRI (saving lots of openings). Using DEEP we proved FRI on $Q$ by evaluating at $\\zeta$, an extension element and running FRI on $Q'$. This saved lots of rounds of FRI. Batch-PCS Footnote For multi-STARK Plonky3 (multiple traces with interactions between them), we can batch-PC by doing a \"rolling commitment\". Pad the polynomials to degrees power of 2, and add new polynomials at every FRI round. Add them as late as possible, so to minimize the total number of \"folds\". Additional randomness needs to be generated at every round to stitch the new polynomials together. Evaluation Domain Footnote Note that if the evaluation is over $L$ then the prover would need to symbolically compute the quotient by dividing through all the coefficients. This sucks because expanding, adding etc. is expensive. Instead it's easier to evaluate each component of the quotient for a set of values where the divisor doesn't vanish, so the Merkle commit is actually to the evaluations of the polynomial over the coset. We now have this distinction between the interpolation domain , $L$, and the evaluation domain , $\\gamma L.$ DEEP-ALI Reasoning Note that we could have evaluated the quotient $Q=\\frac{R}{H}$ at every step of FRI (by opening all the extended column commitments, evaluating directly etc.) instead of committing to this Merkle query and checking that. The reason we don't do this is because we'd have to do lots of Merkle openings at each step of the FRI, probably opening every column at every step. We'd rather put everything into a single Merkle hash and then have a much simpler quotient that we check directly. FRI and RS codewords A linear space of codewords is a linear subspace where no two distinct elements have many terms in common. It is known that the codewords resulting from evaluating low-degree polynomials over any evaluation domain is \"maximally distance separated\" i.e. has the lowest maximum number of collisions (degree $d$ has at most $d$ collisions). Thus in the literature, \"Reed-Solomon codeword\" and \"low-degree polynomial\" are used interchangably. Protocol COMMIT Verifier sends random $\\gamma\\in \\mathbb{F}$ Prover takes current polynomial $P(y)=P_{0}(y^{2})+yP_{1}(y^{2})$ and returns Merkle hash of $P_{0}+\\gamma P_{1}$ over a half-sized domain. QUERY Verifier does checks by asking for $P_{i}(s ^{2^{i}})$ and finally opening the very last $P_{i}$ at all points to check it's constant. This is called a \"colinearity check\". Does this for multiple values of $s$ Finally, check that $Q(\\zeta)H(\\zeta)=R(\\zeta)$ for some $\\zeta \\in \\mathbb{F} {ext}$. The polynomials $H$ and $R$ are easy to evaluate, we evaluate $Q$ by taking $V=Q(\\zeta)$ and $Q {2}=\\frac{{Q(x)-V}}{x-\\zeta}$. For $Q_{2}$ we don't need a new Merkle hash, just query the Merkle hash for $Q$ over the same $L$. Simply run FRI directly on $Q_{2}=\\frac{{Q(x)-V}}{x-\\zeta}$ by opening $Q$ for each query. Soundness First analyze FRI soundness. Assuming the prover Merkle hashes honestly, FRI proves that the vector has at least $\\sqrt{ \\rho }$ shared coefficients with some codeword. A blackbox theorem (Reed-Solomon correlated agreement) that says in a random combination over $\\mathbb{F}$ with a certain probabilistic error the maximum distance is preserved. When we do the degree splitting the distance of one of the halves must be at least that of the original vector. Then if the final result is all constant then the original distance was also small. Now we consider the probability prover lied on some hashes. If the original vector was $\\delta$ far from RS-codeword, then it must make a total of $L\\delta$ \"corrections\". Each round of the query phase is akin to a random \"FFT merkle check\". Then soundness error goes to $(1-\\delta)^{t}$. Here's the black box for completeness: Take the evaluation domains $L^i$. In each round $i$, if we have $l$ function evaluations, then the FRI random $\\gamma$ combination preserves distance $\\delta$ with probability $$\\varepsilon = \\left( \\frac{(m+0.5)^{2}}{6 \\rho^{1.5}}\\cdot \\frac{{L^{i}}}{\\mathbb{F}}+\\frac{{2m+1}}{\\rho^{0.5}}\\frac{L^{0}}{\\mathbb{F}} \\right) \\cdot l.$$ The error over all rounds is then $$\\frac{(m+0.5)^{2}}{3\\rho^{1.5}} \\frac{{L^{0}}}{\\mathbb{F}}+\\frac{{2m+1}}{\\rho^{0.5}} \\frac{{L^{0}}}{\\mathbb{F}}\\cdot 2k.$$ Constants for Security For default plonky3 the field $F=\\mathbb F_{15\\cdot{2}^{27}+1}$ and $d=4$ (quartic field extension), yielding 128-bit security. The rules of thumb provided by EthStark documentation say - hash function needs 256bits - field needs $2^{128}$ bits (addressed) - Every colinearity check gets $\\log_{2}\\rho^{-1}$ bits of security. For log_blowup=2 we get $\\rho=\\frac{1}{4}$ so we need 64 colinearity checks. For plonky3 default we use $m=3$ i.e. $\\delta \\leq 1- \\sqrt{ \\rho }\\left( 1+\\frac{1}{2m} \\right) = \\frac{5}{12}$. For plonky3 quotient degree is $2^{k}\\leq 8$ so commit error is $$ \\left( 116 \\frac{2}{3}\\right) \\frac{L^{0}}{\\mathbb{F}}. $$ Given that $\\mathbb{F}=2^{31}$ and $L^{0}=4h$ we get a maximum trace height of $2^{20}=1048576$. Plonky3 LogUp Implements a Lookup table with log derivatives\u2014aka. sum of inverse linear terms. Multiset equality can be determined by computing $$ \\sum_{i} \\frac{m_{i}}{X-a_{i}}. $$ How this is implemented is by evaluating this function at $\\zeta\\in \\mathbb{F} {ext}$. During trace generation, the Fiat-Shamir hash of the rest of the trace is taken. Columns are combined with a random power linear combination of $\\alpha$ starting at $1$, and then different bus indices are combined with random power linear combination of $\\beta$ (so bus 1 send/receive, bus 2 send/receive etc.). Then $\\zeta$ is computed during trace generation and used to generate and constrain the evaluations of $\\frac{m {i}}{X-a_{i}}$ and the partial sum, with coefficient of $1$ for send and $-1$ for receive. Total sum should be 0 in $\\mathbb{F}_{ext}$, hence the debugger showing four field elements upon failure, at least one of which is nonzero. With multiple rounds of committing/proving/hashing/providing randomness, the AIR is generalized to a RAP, which stands for something . Binius Smaller fields are better. More FFT and parallel recursion with less CPU looping is better. How about a special small field composed of a tower binary construction, where field addition becomes 32-bit XOR. Makes massively parallel Poseidon hashes possible. Circle STARKs Try Mersenne31 ($2^{31}-1$ prime field) except we pick the set of points $x,y$ satisfying $x^{2}+y^{2}=1$. It's like we're doing unit norm complex numbers, but mod $p$. Halo2 and KZG oh idk ive no fucking clue","title":"ZK Notes"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#plonky3","text":"","title":"Plonky3"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#ali-setup","text":"Programs are represented as a matrix of numbers that satisfy low-degree polynomial constraints on adjacent rows. The height of the matrix must be a power of 2: $h=2^k$. Take the base field $F$ and an evaluation domain $L$, which is a multiplicative subgroup of order $h$. Then each column is encoded as a polynomial $C_{i}(\\omega^{j})$, the $i$th item in the $j$th row. (Power of 2 is good because it makes FFT easier.) We commit to each column by Merkle hashing its evaluation over $L$. In particular, constraints $Q(x_{i},y_{i})$ [where $x_{i}$ is the current row, $y_{i}$ is the next row] are represented as $$\\forall x \\in L \\colon Q(C_{i}(x),C_{i}(\\omega \\cdot x))=0.$$ Let $H_{i}$ be the polynomial with roots that $Q_{i}$ must vanish on, let $H$ be the polynomial with all roots of L, and $s_{i}=\\frac{H}{H_{i}}$. Now we want to show $H_{i}|Q_i,$ or instead $H | Q_{i}s_{i}$.","title":"ALI setup"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#quotient-polynomial-definition","text":"Instead of proving this for each constraint one at a time, the verifier stitches all the constraints together using $\\theta \\in F_{ext}.$ $F_{ext}$ is a low-degree extension of $F$, with order $|F|^{d}$. For default plonky3 the field $F=\\mathbb F_{15\\cdot{2}^{27}+1}$ and $d=4$ (quartic field extension), yielding 128-bit security. Stitching is just $$ R(x)=\\sum_{i} \\theta^{i}Q_{i}(C_{j})\\cdot s_{i}(C_{j}). $$ Then we want to show $$ Q(x)=\\frac{R(x)}{H(x)} $$ exists. Let log_blowup=2 , meaning we blow up $L$ by 4, so $L_{b}$ is now a multiplicative subgroup of order $4h$. Then interpolate the degree-$h$ polynomials into $L_{b}$ to create a $4h$-long vector, and Merkle hash it, and the quotient $Q$.","title":"Quotient polynomial definition"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#pcs","text":"Now apply FRI to the Merkle hash of $Q$ to prove it's low degree. Finally, prove $Q$ is actually the quotient we want by opening the quotient $Q$ (actually represented as 2 polynomials with same length as $C$) and all the columns $C$ at $\\zeta$ from $F_{ext}$. Verify that $Q(\\zeta)H(\\zeta)=R(\\zeta)$. Prove the opening by supplying $V=C(\\zeta)$ and directly running FRI on the rational function $Q'=\\frac{{C(x)-V}}{x-\\zeta}$. Finally, note that we could have directly run FRI on the original quotient polynomial but that would require opening every column for every FRI query. Using ALI we committed to the quotient $Q$ and queried it before running FRI (saving lots of openings). Using DEEP we proved FRI on $Q$ by evaluating at $\\zeta$, an extension element and running FRI on $Q'$. This saved lots of rounds of FRI.","title":"PCS"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#batch-pcs-footnote","text":"For multi-STARK Plonky3 (multiple traces with interactions between them), we can batch-PC by doing a \"rolling commitment\". Pad the polynomials to degrees power of 2, and add new polynomials at every FRI round. Add them as late as possible, so to minimize the total number of \"folds\". Additional randomness needs to be generated at every round to stitch the new polynomials together.","title":"Batch-PCS Footnote"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#evaluation-domain-footnote","text":"Note that if the evaluation is over $L$ then the prover would need to symbolically compute the quotient by dividing through all the coefficients. This sucks because expanding, adding etc. is expensive. Instead it's easier to evaluate each component of the quotient for a set of values where the divisor doesn't vanish, so the Merkle commit is actually to the evaluations of the polynomial over the coset. We now have this distinction between the interpolation domain , $L$, and the evaluation domain , $\\gamma L.$","title":"Evaluation Domain Footnote"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#deep-ali-reasoning","text":"Note that we could have evaluated the quotient $Q=\\frac{R}{H}$ at every step of FRI (by opening all the extended column commitments, evaluating directly etc.) instead of committing to this Merkle query and checking that. The reason we don't do this is because we'd have to do lots of Merkle openings at each step of the FRI, probably opening every column at every step. We'd rather put everything into a single Merkle hash and then have a much simpler quotient that we check directly.","title":"DEEP-ALI Reasoning"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#fri-and-rs-codewords","text":"A linear space of codewords is a linear subspace where no two distinct elements have many terms in common. It is known that the codewords resulting from evaluating low-degree polynomials over any evaluation domain is \"maximally distance separated\" i.e. has the lowest maximum number of collisions (degree $d$ has at most $d$ collisions). Thus in the literature, \"Reed-Solomon codeword\" and \"low-degree polynomial\" are used interchangably.","title":"FRI and RS codewords"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#protocol","text":"","title":"Protocol"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#commit","text":"Verifier sends random $\\gamma\\in \\mathbb{F}$ Prover takes current polynomial $P(y)=P_{0}(y^{2})+yP_{1}(y^{2})$ and returns Merkle hash of $P_{0}+\\gamma P_{1}$ over a half-sized domain.","title":"COMMIT"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#query","text":"Verifier does checks by asking for $P_{i}(s ^{2^{i}})$ and finally opening the very last $P_{i}$ at all points to check it's constant. This is called a \"colinearity check\". Does this for multiple values of $s$ Finally, check that $Q(\\zeta)H(\\zeta)=R(\\zeta)$ for some $\\zeta \\in \\mathbb{F} {ext}$. The polynomials $H$ and $R$ are easy to evaluate, we evaluate $Q$ by taking $V=Q(\\zeta)$ and $Q {2}=\\frac{{Q(x)-V}}{x-\\zeta}$. For $Q_{2}$ we don't need a new Merkle hash, just query the Merkle hash for $Q$ over the same $L$. Simply run FRI directly on $Q_{2}=\\frac{{Q(x)-V}}{x-\\zeta}$ by opening $Q$ for each query.","title":"QUERY"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#soundness","text":"First analyze FRI soundness. Assuming the prover Merkle hashes honestly, FRI proves that the vector has at least $\\sqrt{ \\rho }$ shared coefficients with some codeword. A blackbox theorem (Reed-Solomon correlated agreement) that says in a random combination over $\\mathbb{F}$ with a certain probabilistic error the maximum distance is preserved. When we do the degree splitting the distance of one of the halves must be at least that of the original vector. Then if the final result is all constant then the original distance was also small. Now we consider the probability prover lied on some hashes. If the original vector was $\\delta$ far from RS-codeword, then it must make a total of $L\\delta$ \"corrections\". Each round of the query phase is akin to a random \"FFT merkle check\". Then soundness error goes to $(1-\\delta)^{t}$. Here's the black box for completeness: Take the evaluation domains $L^i$. In each round $i$, if we have $l$ function evaluations, then the FRI random $\\gamma$ combination preserves distance $\\delta$ with probability $$\\varepsilon = \\left( \\frac{(m+0.5)^{2}}{6 \\rho^{1.5}}\\cdot \\frac{{L^{i}}}{\\mathbb{F}}+\\frac{{2m+1}}{\\rho^{0.5}}\\frac{L^{0}}{\\mathbb{F}} \\right) \\cdot l.$$ The error over all rounds is then $$\\frac{(m+0.5)^{2}}{3\\rho^{1.5}} \\frac{{L^{0}}}{\\mathbb{F}}+\\frac{{2m+1}}{\\rho^{0.5}} \\frac{{L^{0}}}{\\mathbb{F}}\\cdot 2k.$$","title":"Soundness"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#constants-for-security","text":"For default plonky3 the field $F=\\mathbb F_{15\\cdot{2}^{27}+1}$ and $d=4$ (quartic field extension), yielding 128-bit security. The rules of thumb provided by EthStark documentation say - hash function needs 256bits - field needs $2^{128}$ bits (addressed) - Every colinearity check gets $\\log_{2}\\rho^{-1}$ bits of security. For log_blowup=2 we get $\\rho=\\frac{1}{4}$ so we need 64 colinearity checks. For plonky3 default we use $m=3$ i.e. $\\delta \\leq 1- \\sqrt{ \\rho }\\left( 1+\\frac{1}{2m} \\right) = \\frac{5}{12}$. For plonky3 quotient degree is $2^{k}\\leq 8$ so commit error is $$ \\left( 116 \\frac{2}{3}\\right) \\frac{L^{0}}{\\mathbb{F}}. $$ Given that $\\mathbb{F}=2^{31}$ and $L^{0}=4h$ we get a maximum trace height of $2^{20}=1048576$.","title":"Constants for Security"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#plonky3-logup","text":"Implements a Lookup table with log derivatives\u2014aka. sum of inverse linear terms. Multiset equality can be determined by computing $$ \\sum_{i} \\frac{m_{i}}{X-a_{i}}. $$ How this is implemented is by evaluating this function at $\\zeta\\in \\mathbb{F} {ext}$. During trace generation, the Fiat-Shamir hash of the rest of the trace is taken. Columns are combined with a random power linear combination of $\\alpha$ starting at $1$, and then different bus indices are combined with random power linear combination of $\\beta$ (so bus 1 send/receive, bus 2 send/receive etc.). Then $\\zeta$ is computed during trace generation and used to generate and constrain the evaluations of $\\frac{m {i}}{X-a_{i}}$ and the partial sum, with coefficient of $1$ for send and $-1$ for receive. Total sum should be 0 in $\\mathbb{F}_{ext}$, hence the debugger showing four field elements upon failure, at least one of which is nonzero. With multiple rounds of committing/proving/hashing/providing randomness, the AIR is generalized to a RAP, which stands for something .","title":"Plonky3 LogUp"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#binius","text":"Smaller fields are better. More FFT and parallel recursion with less CPU looping is better. How about a special small field composed of a tower binary construction, where field addition becomes 32-bit XOR. Makes massively parallel Poseidon hashes possible.","title":"Binius"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#circle-starks","text":"Try Mersenne31 ($2^{31}-1$ prime field) except we pick the set of points $x,y$ satisfying $x^{2}+y^{2}=1$. It's like we're doing unit norm complex numbers, but mod $p$.","title":"Circle STARKs"},{"location":"Material%20Knowledge/Sciences/Coding/Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/#halo2-and-kzg","text":"oh idk ive no fucking clue","title":"Halo2 and KZG"},{"location":"Material%20Knowledge/Sciences/Coding/JS%20hacking/Karina%27s%20thoughts/","text":"console hacking grab elements; querySelector via class or id manipulate objects automate stuff like this greasemonkey for using custom/existing js scripts download cookies/python automate things like this","title":"Karina's thoughts"},{"location":"Material%20Knowledge/Sciences/Coding/JS%20hacking/NODE/","text":"npm is the package installer node allows you to run js files just in the terminal like any other programming language still need to figure out browser interactions","title":"NODE"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/","text":"Chap 2 Introduce linear regression, least-squares loss A \"generative model\" is $x=f(y,\\phi),$ whereas discriminative is $y=f(x,\\phi).$ Technicality; but most modern generative models are just very flexible discriminative models. Chap 3 First generalize to multivariate linear regression, $y=\\theta_0+\\Theta_1\\cdot x,$ where $\\theta_0$ scalar, $\\Theta_1$ matrix. Introduce shallow NN. Add a \"hidden layer\", becoming a vector of ReLU'd linear combinations of $x$, namely $a(\\Theta_0+\\Theta_1 x)$ with vector $\\Theta_0$ and matrix $\\Theta_1$. In single-variable case we have vector dotted with hidden layer, plus scalar; extending to multivariate we get matrix $\\Phi_1$ and vector $\\Phi_0$ yielding $$y = \\Phi_0+{\\bf\\Phi_1}\\cdot a(\\Theta_0+{\\bf\\Theta_1}\\cdot x).$$ Here the vector-to-vector function $a$ does ReLU on each coordinate. Intuitional Notes Each hidden layer contributes a hyperplane; resulting linear sum is composition of all of these Consider model input size $D_i$ hidden size $D$, region count is exp. in $D_i$ but param count is $D\\cdot D_i.$ Higher $D$ values result in greater region counts per param count. Syntax Notes Recall that my_nd_array.clip(0.0) literally just works for ReLU. Chap 4 Now we generalize to deep NNs. Layers def'd hidden layers; depth def'd hidden layers+1; width def'd maximal nodes in one layer. Then each layer becomes $$a(\\beta_i+\\Omega_i x).$$ There are depth-many such layers; in a shallow NN there's one layer $\\mathbb R \\to \\mathbb R^D,$ and then another one $\\mathbb R^D \\to \\mathbb R.$ For example, two shallow $\\mathbb R\\to\\mathbb R$ NNs composed become one $\\mathbb R\\to\\mathbb R^D,$ and then a $\\mathbb R^D\\to\\mathbb R^D$ matrix. This is actually the outer product of the two vectors corresponding to what was previously $\\RR^D\\to\\RR$ followed by $\\RR\\to\\RR^D$. Intuitional Notes Consider $f:[0,1]\\to[0,1]$ with four linear pieces from $0$ to $1$ or vice versa. Then self-composing $k$ times yields a function with $4^k$ linear pieces. In this way composing multiple layers creates \"self-folding\" which can create exponentially many facets; however each \"sublayer\" is a mirror image of other layers, making the layers related to each other in convoluted/difficult ways. In general, we can optimally get $D^K$ linear regions, but then they're entangled annoyingly. Syntax Notes The object np.zeros(b,a) creates a mapping $\\RR^a\\to\\RR^b.$ Beware malformed np shapes! Chap 5 So-called \"recipe\" for loss functions: - For your inference domain, pick a distribution represented by parameters $\\theta$ outputted by the model - For each data point, cost is $-\\log(\\Pr(y_i\\mid \\theta))$ - Finally optimize parameters $\\phi$ that yield parameters $\\theta$ - For inference, take prediction that maximizes probability according to $\\theta$ And finally, take negative log everywhere, cuz of course. Examples Gaussian for univariate (yields squared-loss); variance can also be an output parameter Remap using $\\exp$ and divide for multiclass Random other distros (beta, poisson, von Mises, exponential/gamma, Plackett-Luce) Intuitional Notes In general we want prob distributions since we're good Bayesians We helpfully give structure to our model by describing the distro with these parameters Then evaluate based on log-sum-score of our guesses $\\newcommand{\\RR}{\\mathbb R}$","title":"UDL   Week 1"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#chap-2","text":"Introduce linear regression, least-squares loss A \"generative model\" is $x=f(y,\\phi),$ whereas discriminative is $y=f(x,\\phi).$ Technicality; but most modern generative models are just very flexible discriminative models.","title":"Chap 2"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#chap-3","text":"First generalize to multivariate linear regression, $y=\\theta_0+\\Theta_1\\cdot x,$ where $\\theta_0$ scalar, $\\Theta_1$ matrix. Introduce shallow NN. Add a \"hidden layer\", becoming a vector of ReLU'd linear combinations of $x$, namely $a(\\Theta_0+\\Theta_1 x)$ with vector $\\Theta_0$ and matrix $\\Theta_1$. In single-variable case we have vector dotted with hidden layer, plus scalar; extending to multivariate we get matrix $\\Phi_1$ and vector $\\Phi_0$ yielding $$y = \\Phi_0+{\\bf\\Phi_1}\\cdot a(\\Theta_0+{\\bf\\Theta_1}\\cdot x).$$ Here the vector-to-vector function $a$ does ReLU on each coordinate.","title":"Chap 3"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#intuitional-notes","text":"Each hidden layer contributes a hyperplane; resulting linear sum is composition of all of these Consider model input size $D_i$ hidden size $D$, region count is exp. in $D_i$ but param count is $D\\cdot D_i.$ Higher $D$ values result in greater region counts per param count.","title":"Intuitional Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#syntax-notes","text":"Recall that my_nd_array.clip(0.0) literally just works for ReLU.","title":"Syntax Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#chap-4","text":"Now we generalize to deep NNs. Layers def'd hidden layers; depth def'd hidden layers+1; width def'd maximal nodes in one layer. Then each layer becomes $$a(\\beta_i+\\Omega_i x).$$ There are depth-many such layers; in a shallow NN there's one layer $\\mathbb R \\to \\mathbb R^D,$ and then another one $\\mathbb R^D \\to \\mathbb R.$ For example, two shallow $\\mathbb R\\to\\mathbb R$ NNs composed become one $\\mathbb R\\to\\mathbb R^D,$ and then a $\\mathbb R^D\\to\\mathbb R^D$ matrix. This is actually the outer product of the two vectors corresponding to what was previously $\\RR^D\\to\\RR$ followed by $\\RR\\to\\RR^D$.","title":"Chap 4"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#intuitional-notes_1","text":"Consider $f:[0,1]\\to[0,1]$ with four linear pieces from $0$ to $1$ or vice versa. Then self-composing $k$ times yields a function with $4^k$ linear pieces. In this way composing multiple layers creates \"self-folding\" which can create exponentially many facets; however each \"sublayer\" is a mirror image of other layers, making the layers related to each other in convoluted/difficult ways. In general, we can optimally get $D^K$ linear regions, but then they're entangled annoyingly.","title":"Intuitional Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#syntax-notes_1","text":"The object np.zeros(b,a) creates a mapping $\\RR^a\\to\\RR^b.$ Beware malformed np shapes!","title":"Syntax Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#chap-5","text":"So-called \"recipe\" for loss functions: - For your inference domain, pick a distribution represented by parameters $\\theta$ outputted by the model - For each data point, cost is $-\\log(\\Pr(y_i\\mid \\theta))$ - Finally optimize parameters $\\phi$ that yield parameters $\\theta$ - For inference, take prediction that maximizes probability according to $\\theta$ And finally, take negative log everywhere, cuz of course.","title":"Chap 5"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#examples","text":"Gaussian for univariate (yields squared-loss); variance can also be an output parameter Remap using $\\exp$ and divide for multiclass Random other distros (beta, poisson, von Mises, exponential/gamma, Plackett-Luce)","title":"Examples"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%201/#intuitional-notes_2","text":"In general we want prob distributions since we're good Bayesians We helpfully give structure to our model by describing the distro with these parameters Then evaluate based on log-sum-score of our guesses $\\newcommand{\\RR}{\\mathbb R}$","title":"Intuitional Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/","text":"Variational autoencoders have two paradigms: probabilistic and regularization. Preliminary: KL divergence From $Q$ to $P$ defined as $$ \\begin{align} D_{KL}(P \\parallel Q) &= \\int P(x)\\log \\left( \\frac{P(x)}{Q(x)} \\right) \\, dx \\ &= - \\int P(x)\\log \\left( \\frac{Q(x)}{P(x)} \\right) \\, dx. \\end{align} $$ Chap 17: VAEs Probabilistic Latent $z$ drawn from normal Gaussian (prior), and a NN $f[z,\\phi]\\to \\bar{x}$ generates samples, then add another normal Gaussian for noise. Then loss is $$ L(x,\\phi)=-\\log \\left[ Pr(f=x) \\right] =-\\log \\left[ \\int Pr(z,x|\\phi) \\, dz \\right] .$$ Integral is intractable. Let's make another NN $q[x,\\theta]\\to\\bar{z}$ generating mean and covariance that models the posterior for $z$, and then $$ \\begin{align} \\log \\left(\\int Pr(x,z|\\phi) \\, dz\\right) &=\\log \\left( \\int q(z|x,\\theta) \\frac{{Pr(x,z|\\phi)}}{q(z|x,\\theta)} \\, dz \\right) \\ &\\geq \\int q(z|x,\\theta)\\log \\left( \\frac{{Pr(z)Pr(x|z,\\phi)}}{q(z|x,\\theta)}\\right) \\, dz \\ &=\\int q(z|x,\\theta)\\log \\left( Pr(x|z,\\phi) \\right) \\, dz + \\int q(z|x,\\theta) \\log \\left( \\frac{{Pr(z|\\phi)}}{q(z|x,\\theta)} \\right) \\, dz \\ &= \\mathbb{E} {q(z|x)}\\log(Pr(x|z))-D {KL}\\left[ q(z|x,\\theta)\\lvert \\rvert Pr(z) \\right] . \\end{align}$$ The term after Jensen's is called Evidence Lower Bound. Recall that $Pr(z)$ the prior is unit normal. Then in this final form, the first term can be approximated with a fixed sample $z,$ and the second term can be explicitly computed (both are known Gaussians). For gradient descent, fix a unit normal \"source of randomness\" $\\epsilon$ and then rewrite to deterministic, get sample etc. Regularization Our network is just $$ x\\underset{g=q}{\\to}(\\bar{z},\\sigma(z))\\underset{\\text{sample}}{\\to} z^ \\underset{f,\\phi}{\\to} x^{ }$$ The trained $\\sigma$ adds noise to our sample. In theory we could just do $x\\to z\\to x^*,$ but then it would just spit out what we gave in. Furthermore the latent variable has been compressed. The final expression in earlier derivation makes sense; if $q(z|x,\\theta)$ truly resembles $Pr(z)$ then the first term is exactly what we wanted to compute. This ELBO allows us to integrate over not-quite-the-prior by making us pay for distance from the prior. Hence, we can add hyperparameter $\\beta$ as the coefficient of the KL-divergence. The placeholder $q(z|x,\\theta)$ is not an inverse/discriminator in the conventional sense; it just serves as an implicit compromise mechanism for computing the true negative log likelihood. Chap 18: Diffusion Models Now shuffle GAN/VAE/Normalizing, and multiple by 100. Given input $x$ we have a diffusion kernel defined: $z_{0}=x,$ $z_{t}=z_{t-1}\\sqrt{ 1-\\beta_{t} }+\\epsilon\\sqrt{ \\beta }$ for unit normal $\\epsilon.$ Notice that if $\\sigma(z_{0})=1$ then by sum-of-squares $\\sigma(z_{t})=1,$ so we can infer the distribution for any $t$ as $z_{t}=z_{0}\\sqrt{ \\prod_{i}\\left( 1-\\beta_{i} \\right) }+\\epsilon \\sqrt{ 1- \\prod_{i}\\left( 1-\\beta_{i} \\right) }$. Call $\\alpha_{t}=\\prod_{i}1-\\beta_{i}$ so that $z_{t}=z_{0}\\sqrt{ \\alpha_{t} }+\\epsilon \\sqrt{ 1-\\alpha_{t} }$. Eventually, we get unit normal back. Now our goal is to create NNs $f_{t}(z_{t},\\phi_{t})$ that represent the mean of the conditional distribution $q(z_{t-1}|z_{t})$. Specifically $f_{T}$ has input (unit normal, $\\phi_{T}$) outputting the new mean. We apply the same trick as earlier: $$ \\begin{align} \\log \\left( Pr(x=z_{0}) \\right) &=\\log \\left( \\int P(x=z_{0},z_{i}|\\phi) \\, dz_{i} \\right) \\ &= \\log \\left( \\int q(z_{i}|x) \\frac{Pr(x=z_{0},z_{i})}{q(z_{i}|x)} \\, dz_{i} \\right) \\ &\\geq \\int q(z_{i}|x) \\log \\left(\\frac{Pr(x=z_{0},z_{i})}{q(z_{i}|x)} \\right) \\, dz_{i} \\ &= \\int q(z_{i}|x) \\left( \\log \\left[ Pr(z_{0}=x|z_{1}) \\right] +\\sum_{i=2}^{T} \\log \\left[ \\frac{{Pr(z_{i-1}|z_{i})}}{q(z_{i-1}|z_{i},x)} \\right]\\right) \\, dz_{i} \\ &= \\mathbb{E} {q(z {1}|x)} \\log \\left[ Pr(z_{0}=x|z_{1}) \\right] - \\sum_{i=2}^{T} \\mathbb{E} {q(z {i}|x)} D_{KL}\\left(q(z_{i-1}|z_{i},x) \\parallel Pr(z_{i-1}|z_{i}) \\right) . \\end{align} $$ Once again the terms of the KL-divergence are known normals, so we can express literally. Implementation Now, if we reparameterize by setting $g(z_{i-1}|z_{i},x)$ to find the noise $\\epsilon=\\frac{{z_{t} -z_{t-1}\\sqrt{ 1-\\beta_{t} }}}{\\sqrt{ \\beta }}$, then we get a cute and nice expression. Finally, taking the loss over all evidence we get double sum over evidence and layer. Then for batching, just grab a few evidence and a few layer, and go. Conditional Same as VAEs, add a label $c$ and input into every $q=f_{t}.$","title":"UDL   Week 10"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#preliminary-kl-divergence","text":"From $Q$ to $P$ defined as $$ \\begin{align} D_{KL}(P \\parallel Q) &= \\int P(x)\\log \\left( \\frac{P(x)}{Q(x)} \\right) \\, dx \\ &= - \\int P(x)\\log \\left( \\frac{Q(x)}{P(x)} \\right) \\, dx. \\end{align} $$","title":"Preliminary: KL divergence"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#chap-17-vaes","text":"","title":"Chap 17: VAEs"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#probabilistic","text":"Latent $z$ drawn from normal Gaussian (prior), and a NN $f[z,\\phi]\\to \\bar{x}$ generates samples, then add another normal Gaussian for noise. Then loss is $$ L(x,\\phi)=-\\log \\left[ Pr(f=x) \\right] =-\\log \\left[ \\int Pr(z,x|\\phi) \\, dz \\right] .$$ Integral is intractable. Let's make another NN $q[x,\\theta]\\to\\bar{z}$ generating mean and covariance that models the posterior for $z$, and then $$ \\begin{align} \\log \\left(\\int Pr(x,z|\\phi) \\, dz\\right) &=\\log \\left( \\int q(z|x,\\theta) \\frac{{Pr(x,z|\\phi)}}{q(z|x,\\theta)} \\, dz \\right) \\ &\\geq \\int q(z|x,\\theta)\\log \\left( \\frac{{Pr(z)Pr(x|z,\\phi)}}{q(z|x,\\theta)}\\right) \\, dz \\ &=\\int q(z|x,\\theta)\\log \\left( Pr(x|z,\\phi) \\right) \\, dz + \\int q(z|x,\\theta) \\log \\left( \\frac{{Pr(z|\\phi)}}{q(z|x,\\theta)} \\right) \\, dz \\ &= \\mathbb{E} {q(z|x)}\\log(Pr(x|z))-D {KL}\\left[ q(z|x,\\theta)\\lvert \\rvert Pr(z) \\right] . \\end{align}$$ The term after Jensen's is called Evidence Lower Bound. Recall that $Pr(z)$ the prior is unit normal. Then in this final form, the first term can be approximated with a fixed sample $z,$ and the second term can be explicitly computed (both are known Gaussians). For gradient descent, fix a unit normal \"source of randomness\" $\\epsilon$ and then rewrite to deterministic, get sample etc.","title":"Probabilistic"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#regularization","text":"Our network is just $$ x\\underset{g=q}{\\to}(\\bar{z},\\sigma(z))\\underset{\\text{sample}}{\\to} z^ \\underset{f,\\phi}{\\to} x^{ }$$ The trained $\\sigma$ adds noise to our sample. In theory we could just do $x\\to z\\to x^*,$ but then it would just spit out what we gave in. Furthermore the latent variable has been compressed. The final expression in earlier derivation makes sense; if $q(z|x,\\theta)$ truly resembles $Pr(z)$ then the first term is exactly what we wanted to compute. This ELBO allows us to integrate over not-quite-the-prior by making us pay for distance from the prior. Hence, we can add hyperparameter $\\beta$ as the coefficient of the KL-divergence. The placeholder $q(z|x,\\theta)$ is not an inverse/discriminator in the conventional sense; it just serves as an implicit compromise mechanism for computing the true negative log likelihood.","title":"Regularization"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#chap-18-diffusion-models","text":"Now shuffle GAN/VAE/Normalizing, and multiple by 100. Given input $x$ we have a diffusion kernel defined: $z_{0}=x,$ $z_{t}=z_{t-1}\\sqrt{ 1-\\beta_{t} }+\\epsilon\\sqrt{ \\beta }$ for unit normal $\\epsilon.$ Notice that if $\\sigma(z_{0})=1$ then by sum-of-squares $\\sigma(z_{t})=1,$ so we can infer the distribution for any $t$ as $z_{t}=z_{0}\\sqrt{ \\prod_{i}\\left( 1-\\beta_{i} \\right) }+\\epsilon \\sqrt{ 1- \\prod_{i}\\left( 1-\\beta_{i} \\right) }$. Call $\\alpha_{t}=\\prod_{i}1-\\beta_{i}$ so that $z_{t}=z_{0}\\sqrt{ \\alpha_{t} }+\\epsilon \\sqrt{ 1-\\alpha_{t} }$. Eventually, we get unit normal back. Now our goal is to create NNs $f_{t}(z_{t},\\phi_{t})$ that represent the mean of the conditional distribution $q(z_{t-1}|z_{t})$. Specifically $f_{T}$ has input (unit normal, $\\phi_{T}$) outputting the new mean. We apply the same trick as earlier: $$ \\begin{align} \\log \\left( Pr(x=z_{0}) \\right) &=\\log \\left( \\int P(x=z_{0},z_{i}|\\phi) \\, dz_{i} \\right) \\ &= \\log \\left( \\int q(z_{i}|x) \\frac{Pr(x=z_{0},z_{i})}{q(z_{i}|x)} \\, dz_{i} \\right) \\ &\\geq \\int q(z_{i}|x) \\log \\left(\\frac{Pr(x=z_{0},z_{i})}{q(z_{i}|x)} \\right) \\, dz_{i} \\ &= \\int q(z_{i}|x) \\left( \\log \\left[ Pr(z_{0}=x|z_{1}) \\right] +\\sum_{i=2}^{T} \\log \\left[ \\frac{{Pr(z_{i-1}|z_{i})}}{q(z_{i-1}|z_{i},x)} \\right]\\right) \\, dz_{i} \\ &= \\mathbb{E} {q(z {1}|x)} \\log \\left[ Pr(z_{0}=x|z_{1}) \\right] - \\sum_{i=2}^{T} \\mathbb{E} {q(z {i}|x)} D_{KL}\\left(q(z_{i-1}|z_{i},x) \\parallel Pr(z_{i-1}|z_{i}) \\right) . \\end{align} $$ Once again the terms of the KL-divergence are known normals, so we can express literally.","title":"Chap 18: Diffusion Models"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#implementation","text":"Now, if we reparameterize by setting $g(z_{i-1}|z_{i},x)$ to find the noise $\\epsilon=\\frac{{z_{t} -z_{t-1}\\sqrt{ 1-\\beta_{t} }}}{\\sqrt{ \\beta }}$, then we get a cute and nice expression. Finally, taking the loss over all evidence we get double sum over evidence and layer. Then for batching, just grab a few evidence and a few layer, and go.","title":"Implementation"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2010/#conditional","text":"Same as VAEs, add a label $c$ and input into every $q=f_{t}.$","title":"Conditional"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/","text":"Chapter 19: RL So we have partially observable Markov decision processes , where - at each timestep world state encoded in $s$, fixes a set of actions - each action $a$ has distribution describing subsequent state - actions can have reward $r$ - observations $o$ drawn from distribution based on state A policy $\\pi[a|s]$ is the distribution of next actions by state. We estimate the value of future rewards with a reverse interest rate $\\gamma,$ and so can define the value of a sequence of actions or a policy from a given starting point. Also define $q$, the value from given state with given action. Relations are $$ \\begin{align} v[s_{t}]&=\\sum_{a_{t}}\\pi[a_{t}|s_{t}]q[s_{t},a_{t}], \\ q[s_{t},a_{t}]&=r[s_{t},a_{t}]+\\gamma \\sum Pr(s_{t+1}|s_{t},a_{t})v[s_{t+1}]. \\end{align} $$ Markov Decision Problems (full state known) Direct DP Now, if we know the entire action->state distribution for all states, we can initialize values for $v$ randomly, sweep through by plugging in $q$ into the first equation once, and then greedily update $\\pi$. This eventually converges. Monte Carlo Set policy randomly, then after each run of experimentation, for each pair $a,s$ update the policy to value of $a$ that yielded the best measured average $q$ over all runs. However, this requires us to see every $a,s$ pair many times, which is not always possible. Temporal Difference Live updating while doing Monte Carlo, so that the new value of $q[s_{t},a_{t}]$ is the old value plus $\\alpha$ times the delta in expectation. SARSA is $(r[s_{t},a_{t}]+\\gamma\\cdot q[s_{t+1},a_{t+1}])-q[s_{t},a_{t}].$ Instead we could do off-policy , meaning the policy that is acting is \"behavioral\" whereas optimal \"target\" is being learned. Typically, behavioral is stochastic but target is deterministic. Behavioral can be modified by e.g. epsilon greedy where pick greedy with chance $1-\\epsilon,$ random otherwise. Then we have some behavioral, but then $q[s_{t+1},a_{t+1}]$ is replaced with the best move (target policy), $\\max_{a}(q[s_{t+1},a]).$ This is called Q-learning , however faces the same size issue as Monte Carlo. Fitted Q-learning Represent state $s_{t}$ as a vector, put into model $q$ with parameters $\\phi$. Then loss $L(\\phi)$ is the average TD delta $\\left( r[s_{t},a_{t}]+\\gamma\\cdot \\max_{a}(q[s_{t+1},a]) \\right) -q[s_{t},a_{t}]$ squared. Then $\\phi$ gets updated using partial of this. Abandon Action Values Let's just do estimation of policy effectiveness. That would be expected discounted reward over the whole trajectory (measuring state and action). Then after taking $I$ many samples and gradient ascent, we get the addition to $\\theta$ of $$ \\begin{align} \\alpha\\cdot \\frac{1}{I}\\sum_{i}^I \\frac{r[\\tau_{i}]}{Pr(\\tau_{i}|\\theta)} \\frac{{\\partial Pr(\\tau_{i}|\\theta)}}{\\partial\\theta} &= \\alpha \\cdot Mean \\left[ r[\\tau]\\frac{{\\partial \\ln(Pr[\\tau|\\theta])}}{\\partial\\theta} \\right] \\ &=\\alpha \\cdot Mean\\left[ r[\\tau] \\sum_{i}\\frac{\\partial \\ln(Pr[s_{i+1}|s_{i},\\theta])}{\\partial\\theta} \\right] . \\end{align} $$ The REINFORCE algorithm does this for a discrete action space, using a softmaxxed neural net for probabilities. Walking through Monte-Carlo style and updating $\\theta$ does the trick. Another good thing to do is to just train a parallel value model that predicts value of a state $s_{t}$ and replace $r[\\tau_{it}]$ with $r[\\tau_{t}]-b[s_{t}].$ This compensates for some states just being better than others. To do this at each step using temporal difference, we can update $\\theta$ using $\\alpha(r[s_{t},a_{t}]+\\gamma v(s_{t+1})-v(s_{t}))$, where $v(s,\\phi)$ is a parallel network predicting each state's value. Now $\\pi[s_{t},\\theta]$ is the \"actor\" and $v[s_{t},\\phi]$ is the \"critic\". Offline RL Where we want to build a model based on past, noninteractive transcripts with the environment. This is new, and rather hard. The above is one paradigm; sequence prediction with transformers or LSTM is another, called \"decision transformer,\" which tries to predict the reward for each action.","title":"UDL   Week 11"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#chapter-19-rl","text":"So we have partially observable Markov decision processes , where - at each timestep world state encoded in $s$, fixes a set of actions - each action $a$ has distribution describing subsequent state - actions can have reward $r$ - observations $o$ drawn from distribution based on state A policy $\\pi[a|s]$ is the distribution of next actions by state. We estimate the value of future rewards with a reverse interest rate $\\gamma,$ and so can define the value of a sequence of actions or a policy from a given starting point. Also define $q$, the value from given state with given action. Relations are $$ \\begin{align} v[s_{t}]&=\\sum_{a_{t}}\\pi[a_{t}|s_{t}]q[s_{t},a_{t}], \\ q[s_{t},a_{t}]&=r[s_{t},a_{t}]+\\gamma \\sum Pr(s_{t+1}|s_{t},a_{t})v[s_{t+1}]. \\end{align} $$","title":"Chapter 19: RL"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#markov-decision-problems-full-state-known","text":"","title":"Markov Decision Problems (full state known)"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#direct-dp","text":"Now, if we know the entire action->state distribution for all states, we can initialize values for $v$ randomly, sweep through by plugging in $q$ into the first equation once, and then greedily update $\\pi$. This eventually converges.","title":"Direct DP"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#monte-carlo","text":"Set policy randomly, then after each run of experimentation, for each pair $a,s$ update the policy to value of $a$ that yielded the best measured average $q$ over all runs. However, this requires us to see every $a,s$ pair many times, which is not always possible.","title":"Monte Carlo"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#temporal-difference","text":"Live updating while doing Monte Carlo, so that the new value of $q[s_{t},a_{t}]$ is the old value plus $\\alpha$ times the delta in expectation. SARSA is $(r[s_{t},a_{t}]+\\gamma\\cdot q[s_{t+1},a_{t+1}])-q[s_{t},a_{t}].$ Instead we could do off-policy , meaning the policy that is acting is \"behavioral\" whereas optimal \"target\" is being learned. Typically, behavioral is stochastic but target is deterministic. Behavioral can be modified by e.g. epsilon greedy where pick greedy with chance $1-\\epsilon,$ random otherwise. Then we have some behavioral, but then $q[s_{t+1},a_{t+1}]$ is replaced with the best move (target policy), $\\max_{a}(q[s_{t+1},a]).$ This is called Q-learning , however faces the same size issue as Monte Carlo.","title":"Temporal Difference"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#fitted-q-learning","text":"Represent state $s_{t}$ as a vector, put into model $q$ with parameters $\\phi$. Then loss $L(\\phi)$ is the average TD delta $\\left( r[s_{t},a_{t}]+\\gamma\\cdot \\max_{a}(q[s_{t+1},a]) \\right) -q[s_{t},a_{t}]$ squared. Then $\\phi$ gets updated using partial of this.","title":"Fitted Q-learning"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#abandon-action-values","text":"Let's just do estimation of policy effectiveness. That would be expected discounted reward over the whole trajectory (measuring state and action). Then after taking $I$ many samples and gradient ascent, we get the addition to $\\theta$ of $$ \\begin{align} \\alpha\\cdot \\frac{1}{I}\\sum_{i}^I \\frac{r[\\tau_{i}]}{Pr(\\tau_{i}|\\theta)} \\frac{{\\partial Pr(\\tau_{i}|\\theta)}}{\\partial\\theta} &= \\alpha \\cdot Mean \\left[ r[\\tau]\\frac{{\\partial \\ln(Pr[\\tau|\\theta])}}{\\partial\\theta} \\right] \\ &=\\alpha \\cdot Mean\\left[ r[\\tau] \\sum_{i}\\frac{\\partial \\ln(Pr[s_{i+1}|s_{i},\\theta])}{\\partial\\theta} \\right] . \\end{align} $$ The REINFORCE algorithm does this for a discrete action space, using a softmaxxed neural net for probabilities. Walking through Monte-Carlo style and updating $\\theta$ does the trick. Another good thing to do is to just train a parallel value model that predicts value of a state $s_{t}$ and replace $r[\\tau_{it}]$ with $r[\\tau_{t}]-b[s_{t}].$ This compensates for some states just being better than others. To do this at each step using temporal difference, we can update $\\theta$ using $\\alpha(r[s_{t},a_{t}]+\\gamma v(s_{t+1})-v(s_{t}))$, where $v(s,\\phi)$ is a parallel network predicting each state's value. Now $\\pi[s_{t},\\theta]$ is the \"actor\" and $v[s_{t},\\phi]$ is the \"critic\".","title":"Abandon Action Values"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%2011/#offline-rl","text":"Where we want to build a model based on past, noninteractive transcripts with the environment. This is new, and rather hard. The above is one paradigm; sequence prediction with transformers or LSTM is another, called \"decision transformer,\" which tries to predict the reward for each action.","title":"Offline RL"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/","text":"Chap 6: Gradient descent, momentum Ok, we've got normal gradient descent. $\\phi :=\\phi- \\alpha \\frac{\\partial L}{\\partial \\phi}.$ But this can be \"unstable\", so we have some empirical improvements: Momentum Have a $\\beta$-rate time decay on the influence of all previous gradient descents, i.e. we move in the direction $$m=(1-\\beta)\\sum_{i=0} \\beta^i g_i.$$ More specifically, $$m_i=(1-\\beta)g+\\beta m_{i-1}.$$Don't forget the $\\alpha$ coefficient. Nesterov Momentum Instead of updating by gradient here , let's update by gradient over there . $$m_{i+1} = (1-\\beta)\\left(-\\alpha\\left. \\frac{\\partial L}{\\partial \\phi}\\right| {\\phi-\\alpha m {i}}\\right)+\\beta m_i.$$ Basically, correct our momentum by an assumed future position. ADAptive Momentum (ADAM) Try moving uniform magnitude in each coordinate; i.e. $\\Delta\\phi=-\\alpha\\frac{g}{|g|}.$ We don't like absolute value; instead let's set $v=g^2,$ so $\\Delta \\phi=-\\alpha \\frac{g}{\\sqrt v}.$ Of course, normalize with $\\frac g{\\sqrt v+\\epsilon}.$ This (Adaptive Gradient) works well, but let's add momentum! Let $m$ be the momentum of $g$ i.e. $m=\\beta m_{i-1}+(1-\\beta)g,$ and $u$ the momentum of $v:=g^2$ i.e. $u=\\gamma u_{i-1}+(1-\\gamma)g^2.$ Then $\\Delta g = -\\alpha\\frac{m}{\\sqrt u+\\epsilon}.$ Don't forget to \"normalize\" our truncated infinite geometric series by taking $\\tilde m=\\frac{m}{1-\\beta^t}$ and $\\tilde u=\\frac u{1-\\gamma^t},$ and actually using $\\tilde m,\\tilde u$ in place of $m,u$. This is because $\\gamma$ is usually large. And of course, we can mashup ADAM with Nesterov if we want, by taking $g$ from $\\phi+\\Delta\\phi$. SGD Instead of taking gradient with the entire training dataset, we chop training into batches and gradient descent by these. Going through an entire permutation (set of batches) corresponds to one epoch . This can be mashup'd with any of the above. Intuitional Notes Momentum is a geometric time-average of gradients ADAM moves coordinate-wise, mashed up with momentum, scaling for geometric truncation Nesterov takes gradient from where you \"would\" be, like moving double and stepping back SGD is \"orthogonal\" to these descent strategies by adjusting batch Chap 7: Backprop Fun part! We'll work with only deep neural nets, of course. Assume one input, one output. $K$ layers means $K+1$ sets of linear maps composed with activations (last activation dropped). We'll take $h$ for hidden (post-activation) and $f$ for pre-activation from $0-K$. One \"unit\" is from $h\\to f$ through linear map, and then to $h$ by activation. Counterintuitively, by this index notation, the hidden layer comes first . Then $h[0]$ is input, $f[k]$ is output, and $$f_l=\\beta_l+\\Omega_lh_l$$ (where $l\\in[0,K]$ is layer). Total Derivative Chain Rule The total derivative of $f: \\RR^m\\to\\RR^n$ is the matrix $M:\\RR^m\\to\\RR^n$ corresponding to a \"linear approximation\". By this logic, it's perfectly clear that $$D(f\\circ g)=Df\\circ Dg.$$ Backprop Then using activation function $a$ and our above index notation, we have $$ \\begin{align } \\frac{\\partial L}{\\partial \\beta_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial \\beta_i}=\\frac{\\partial L}{\\partial f_i},\\ \\frac{\\partial L}{\\partial h_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial h_i}=\\frac{\\partial L}{\\partial f_i}\\circ \\Omega_i=\\Omega_i^T\\frac{\\partial L}{\\partial f_i},\\ \\frac{\\partial L}{\\partial \\Omega_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial \\Omega_i}=\\frac{\\partial L}{\\partial f_i} h_i^T.\\ \\frac{\\partial L}{\\partial f_{i-1}}&=\\frac{\\partial L}{\\partial h_{i}}\\circ\\frac{\\partial h_{i}}{\\partial f_{i-1}}=\\frac{\\partial L}{\\partial h_{i}}\\odot a'(f_{i-1}). \\end{align } $$ Note the distinction between a linear map from $\\RR^n\\to\\RR$ (linear functional) v.s. the vector whose dot product encodes said functional. This is the source of the transpose. More generally, we are taking inner product coefficients instead of total derivatives. And of course, $\\frac{\\partial L}{\\partial f_K}=\\frac{\\partial (f_K-y)^2}{\\partial f_K}=2(f_K-y).$ If we shorten to only the ones we care about, we get $$ \\begin{align } \\frac{\\partial L}{\\partial \\beta_{i}}&=\\left(\\Omega_{i+1}^T\\frac{\\partial L}{\\partial \\beta_{i+1}}\\right)\\odot a'(f_i),\\ \\frac{\\partial L}{\\partial \\Omega_i}&=\\frac{\\partial L}{\\partial \\beta_i} h_i^T. \\end{align } $$ This also means we can go layer by layer, dropping the values from previous layers, and update weights on each layer iteration. Intuitional Notes Vector calc stuff, total derivative chain rule; note that we're grabbing inner product duals Backprop biases first; chain rule: first Hadamard the activation derivative then next-layer weights transpose, to \"amplify\" next-layer bias derivatives Backprop weights: Literally just $\\frac{\\partial L}{\\partial \\beta_i}\\otimes h_i$ (remember to match shape) $\\newcommand{\\RR}{\\mathbb R}$","title":"UDL   Week 2"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#chap-6-gradient-descent-momentum","text":"Ok, we've got normal gradient descent. $\\phi :=\\phi- \\alpha \\frac{\\partial L}{\\partial \\phi}.$ But this can be \"unstable\", so we have some empirical improvements:","title":"Chap 6: Gradient descent, momentum"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#momentum","text":"Have a $\\beta$-rate time decay on the influence of all previous gradient descents, i.e. we move in the direction $$m=(1-\\beta)\\sum_{i=0} \\beta^i g_i.$$ More specifically, $$m_i=(1-\\beta)g+\\beta m_{i-1}.$$Don't forget the $\\alpha$ coefficient.","title":"Momentum"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#nesterov-momentum","text":"Instead of updating by gradient here , let's update by gradient over there . $$m_{i+1} = (1-\\beta)\\left(-\\alpha\\left. \\frac{\\partial L}{\\partial \\phi}\\right| {\\phi-\\alpha m {i}}\\right)+\\beta m_i.$$ Basically, correct our momentum by an assumed future position.","title":"Nesterov Momentum"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#adaptive-momentum-adam","text":"Try moving uniform magnitude in each coordinate; i.e. $\\Delta\\phi=-\\alpha\\frac{g}{|g|}.$ We don't like absolute value; instead let's set $v=g^2,$ so $\\Delta \\phi=-\\alpha \\frac{g}{\\sqrt v}.$ Of course, normalize with $\\frac g{\\sqrt v+\\epsilon}.$ This (Adaptive Gradient) works well, but let's add momentum! Let $m$ be the momentum of $g$ i.e. $m=\\beta m_{i-1}+(1-\\beta)g,$ and $u$ the momentum of $v:=g^2$ i.e. $u=\\gamma u_{i-1}+(1-\\gamma)g^2.$ Then $\\Delta g = -\\alpha\\frac{m}{\\sqrt u+\\epsilon}.$ Don't forget to \"normalize\" our truncated infinite geometric series by taking $\\tilde m=\\frac{m}{1-\\beta^t}$ and $\\tilde u=\\frac u{1-\\gamma^t},$ and actually using $\\tilde m,\\tilde u$ in place of $m,u$. This is because $\\gamma$ is usually large. And of course, we can mashup ADAM with Nesterov if we want, by taking $g$ from $\\phi+\\Delta\\phi$.","title":"ADAptive Momentum (ADAM)"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#sgd","text":"Instead of taking gradient with the entire training dataset, we chop training into batches and gradient descent by these. Going through an entire permutation (set of batches) corresponds to one epoch . This can be mashup'd with any of the above.","title":"SGD"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#intuitional-notes","text":"Momentum is a geometric time-average of gradients ADAM moves coordinate-wise, mashed up with momentum, scaling for geometric truncation Nesterov takes gradient from where you \"would\" be, like moving double and stepping back SGD is \"orthogonal\" to these descent strategies by adjusting batch","title":"Intuitional Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#chap-7-backprop","text":"Fun part! We'll work with only deep neural nets, of course. Assume one input, one output. $K$ layers means $K+1$ sets of linear maps composed with activations (last activation dropped). We'll take $h$ for hidden (post-activation) and $f$ for pre-activation from $0-K$. One \"unit\" is from $h\\to f$ through linear map, and then to $h$ by activation. Counterintuitively, by this index notation, the hidden layer comes first . Then $h[0]$ is input, $f[k]$ is output, and $$f_l=\\beta_l+\\Omega_lh_l$$ (where $l\\in[0,K]$ is layer).","title":"Chap 7: Backprop"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#total-derivative-chain-rule","text":"The total derivative of $f: \\RR^m\\to\\RR^n$ is the matrix $M:\\RR^m\\to\\RR^n$ corresponding to a \"linear approximation\". By this logic, it's perfectly clear that $$D(f\\circ g)=Df\\circ Dg.$$","title":"Total Derivative Chain Rule"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#backprop","text":"Then using activation function $a$ and our above index notation, we have $$ \\begin{align } \\frac{\\partial L}{\\partial \\beta_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial \\beta_i}=\\frac{\\partial L}{\\partial f_i},\\ \\frac{\\partial L}{\\partial h_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial h_i}=\\frac{\\partial L}{\\partial f_i}\\circ \\Omega_i=\\Omega_i^T\\frac{\\partial L}{\\partial f_i},\\ \\frac{\\partial L}{\\partial \\Omega_i}&=\\frac{\\partial L}{\\partial f_i}\\circ\\frac{\\partial f_i}{\\partial \\Omega_i}=\\frac{\\partial L}{\\partial f_i} h_i^T.\\ \\frac{\\partial L}{\\partial f_{i-1}}&=\\frac{\\partial L}{\\partial h_{i}}\\circ\\frac{\\partial h_{i}}{\\partial f_{i-1}}=\\frac{\\partial L}{\\partial h_{i}}\\odot a'(f_{i-1}). \\end{align } $$ Note the distinction between a linear map from $\\RR^n\\to\\RR$ (linear functional) v.s. the vector whose dot product encodes said functional. This is the source of the transpose. More generally, we are taking inner product coefficients instead of total derivatives. And of course, $\\frac{\\partial L}{\\partial f_K}=\\frac{\\partial (f_K-y)^2}{\\partial f_K}=2(f_K-y).$ If we shorten to only the ones we care about, we get $$ \\begin{align } \\frac{\\partial L}{\\partial \\beta_{i}}&=\\left(\\Omega_{i+1}^T\\frac{\\partial L}{\\partial \\beta_{i+1}}\\right)\\odot a'(f_i),\\ \\frac{\\partial L}{\\partial \\Omega_i}&=\\frac{\\partial L}{\\partial \\beta_i} h_i^T. \\end{align } $$ This also means we can go layer by layer, dropping the values from previous layers, and update weights on each layer iteration.","title":"Backprop"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%202/#intuitional-notes_1","text":"Vector calc stuff, total derivative chain rule; note that we're grabbing inner product duals Backprop biases first; chain rule: first Hadamard the activation derivative then next-layer weights transpose, to \"amplify\" next-layer bias derivatives Backprop weights: Literally just $\\frac{\\partial L}{\\partial \\beta_i}\\otimes h_i$ (remember to match shape) $\\newcommand{\\RR}{\\mathbb R}$","title":"Intuitional Notes"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%203/","text":"Chap 8 Basically, test error data comes from three sources, and this makes sense. For given input $x$ there can be multiple outputs $y$; determined by a mean $\\mu(x)$ with constant noise $\\sigma,$ i.e. $\\text{Var}(y|_x)=\\sigma, \\mathbb E(y|_x)=\\mu(x).$ For a given model architecture, the function $\\mu$ can be fit best by $f_\\mu.$ Clearly $\\mu, f_\\mu$ are determined if architecture is given. Then we have a final source of error, which is $\\mathbb E([f(x,\\Phi[\\mathcal D])-f_\\mu]^2).$ In other words, the error between the model that we're able to train and the best possible model, due to the random sampling as a result of incompleteness in training data. In this way, we can see source of noise as $$y(x)\\leftarrow \\mu(x) \\leftrightarrow f_\\mu(x)\\to f(x,\\Phi[\\mathcal D]).$$ Because we're only taking variances, we can directly write $$\\mathbb E(L)=\\mathbb E([f(x,\\Phi[\\mathcal D])-y(x)]^2)=\\sigma^2+(\\mu-f_\\mu)^2+\\mathbb E([f(x,\\Phi[\\mathcal D])-f_\\mu]^2).$$ These terms are called noise , bias , and variance , respectively. They measure - how noisy training data is from the true pattern - capacity of our model to fit the true pattern - our model's error from the best fitting, given our training data The Double Descent phenomenon is when as capacity increases, test performance decreases, increases until the point where the model memorizes the data, then decreases again. ![[Pasted image 20240303130044.png]] This results from variance increasing as the model begins to memorize the training data, because it also begins to memorize noise. Then as capacity increases the model begins to \"smooth out\" its interpolation and learns again. Something like this; people don't really know. Hypertuning is done by - Picking the hyperparameters that maximize scores on a validation set - Then testing the entire system on a test set - Performance on test is indicative of real-word performance","title":"UDL   Week 3"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%203/#chap-8","text":"Basically, test error data comes from three sources, and this makes sense. For given input $x$ there can be multiple outputs $y$; determined by a mean $\\mu(x)$ with constant noise $\\sigma,$ i.e. $\\text{Var}(y|_x)=\\sigma, \\mathbb E(y|_x)=\\mu(x).$ For a given model architecture, the function $\\mu$ can be fit best by $f_\\mu.$ Clearly $\\mu, f_\\mu$ are determined if architecture is given. Then we have a final source of error, which is $\\mathbb E([f(x,\\Phi[\\mathcal D])-f_\\mu]^2).$ In other words, the error between the model that we're able to train and the best possible model, due to the random sampling as a result of incompleteness in training data. In this way, we can see source of noise as $$y(x)\\leftarrow \\mu(x) \\leftrightarrow f_\\mu(x)\\to f(x,\\Phi[\\mathcal D]).$$ Because we're only taking variances, we can directly write $$\\mathbb E(L)=\\mathbb E([f(x,\\Phi[\\mathcal D])-y(x)]^2)=\\sigma^2+(\\mu-f_\\mu)^2+\\mathbb E([f(x,\\Phi[\\mathcal D])-f_\\mu]^2).$$ These terms are called noise , bias , and variance , respectively. They measure - how noisy training data is from the true pattern - capacity of our model to fit the true pattern - our model's error from the best fitting, given our training data The Double Descent phenomenon is when as capacity increases, test performance decreases, increases until the point where the model memorizes the data, then decreases again. ![[Pasted image 20240303130044.png]] This results from variance increasing as the model begins to memorize the training data, because it also begins to memorize noise. Then as capacity increases the model begins to \"smooth out\" its interpolation and learns again. Something like this; people don't really know. Hypertuning is done by - Picking the hyperparameters that maximize scores on a validation set - Then testing the entire system on a test set - Performance on test is indicative of real-word performance","title":"Chap 8"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%204/","text":"Chap 9 Regularization At the end of the day, what we want is to maximize performance on the test dataset. We can try to make our model interpolate more smoothly between data points. Essentially, we can add extra terms to the loss function to make our model parameters \"care\" about certain properties. Naively, we can add $\\lambda |\\phi|^2,$ a constant multiple of the magnitudes of the parameters, to our loss function. Making parameters small $\\to$ smoother model. This happens to be equivalent to taking $\\phi\\to(1-\\gamma)\\phi$ each descent step. Some training methods have implicit terms, compared to the base $\\frac{d\\phi}{dt}=\\frac{dL}{d\\phi}.$ For instance, training with gradient descent distance $\\alpha$ is actually adding $\\frac{\\alpha}4|\\frac{dL}{d\\phi}|^2.$ This can be derived from a perturbation expansion in $\\alpha$ for the \"real\" $\\frac{d\\phi}{dt}.$ Extending the above, training with SGD adds $\\frac1B\\sum_i |\\frac{dL_i}{d\\phi}|^2=|\\frac{dL}{d\\phi}|^2+\\text{Var}(\\frac{dL_i}{d\\phi}).$ In other words, take the above implicit term, and then add another term for the variance of gradients among all batches. Hence smaller batches (higher variance) can yield better models Implicit term encourages consistency across data That's all for the math. ![[Pasted image 20240317201439.png]] Ensembling: run several models and take an average Bayesian: maintain a distribution of parameters and update given data Dropout: drop out weights randomly during training, akin to applying random noise Early stopping Label smoothing: move labels around randomly to make model less confident Data augmentation: generating new data via transformations Transfer: pretraining Multi-task: several models that learn sequential tasks","title":"UDL   Week 4"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%204/#chap-9-regularization","text":"At the end of the day, what we want is to maximize performance on the test dataset. We can try to make our model interpolate more smoothly between data points. Essentially, we can add extra terms to the loss function to make our model parameters \"care\" about certain properties. Naively, we can add $\\lambda |\\phi|^2,$ a constant multiple of the magnitudes of the parameters, to our loss function. Making parameters small $\\to$ smoother model. This happens to be equivalent to taking $\\phi\\to(1-\\gamma)\\phi$ each descent step. Some training methods have implicit terms, compared to the base $\\frac{d\\phi}{dt}=\\frac{dL}{d\\phi}.$ For instance, training with gradient descent distance $\\alpha$ is actually adding $\\frac{\\alpha}4|\\frac{dL}{d\\phi}|^2.$ This can be derived from a perturbation expansion in $\\alpha$ for the \"real\" $\\frac{d\\phi}{dt}.$ Extending the above, training with SGD adds $\\frac1B\\sum_i |\\frac{dL_i}{d\\phi}|^2=|\\frac{dL}{d\\phi}|^2+\\text{Var}(\\frac{dL_i}{d\\phi}).$ In other words, take the above implicit term, and then add another term for the variance of gradients among all batches. Hence smaller batches (higher variance) can yield better models Implicit term encourages consistency across data That's all for the math. ![[Pasted image 20240317201439.png]] Ensembling: run several models and take an average Bayesian: maintain a distribution of parameters and update given data Dropout: drop out weights randomly during training, akin to applying random noise Early stopping Label smoothing: move labels around randomly to make model less confident Data augmentation: generating new data via transformations Transfer: pretraining Multi-task: several models that learn sequential tasks","title":"Chap 9 Regularization"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%205/","text":"Chap 10: CNNs right ok so a convlayer is convolution (some large tensor (nested ordered list)) usually 2D, in \"channels\" i.e. 3D stacks Then we have \"kernel sizes\" i.e. $$h_j=\\sum_i w_ix_{i+j}$$ Good for images because we can impose translational invariance and get this structure as a symmetric reduction Then if kernel is size $K\\times K,$ has $C_i$ channels in and $C_o$ channels out the convlayer has $K^2C_iC_o$ weights. We can also define parameters on convlayers: - stride : how dense our \"convolution sampling\" is (dimension goes down) - dilation : the value of $d$ in $x_{i+dj}$, i.e. \"dilating\" the convolution - padding : whether we add zeros on the boundary, or cut everything off to only take \"valid\" full convolutions Heuristics Intersperse convlayers with downsampling by two Preserve \"size of data\" by doubling number of channels At some point the \"receptive field\" of a single value is the entire image, so we can now flatten out into two layers or so of FCNN Channels $\\iff$ parallel feature extraction For object detection/semantic segmentation: Add max-unpool and deconvolutions (basically inversions of conv) to generate a new image of the same size, with classifications for each pixel Then use a heuristic greedy algorithm to label collections of pixels with sufficient evidence together ![[Pasted image 20240317231859.png]] Don't forget to use other \"good ideas\" such as dropout, SGD, data augmentation, so on. Data can be augmented \"unsupervised\" by blacking out certain boxes etc.","title":"UDL   Week 5"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%205/#chap-10-cnns","text":"right ok so a convlayer is convolution (some large tensor (nested ordered list)) usually 2D, in \"channels\" i.e. 3D stacks Then we have \"kernel sizes\" i.e. $$h_j=\\sum_i w_ix_{i+j}$$ Good for images because we can impose translational invariance and get this structure as a symmetric reduction Then if kernel is size $K\\times K,$ has $C_i$ channels in and $C_o$ channels out the convlayer has $K^2C_iC_o$ weights. We can also define parameters on convlayers: - stride : how dense our \"convolution sampling\" is (dimension goes down) - dilation : the value of $d$ in $x_{i+dj}$, i.e. \"dilating\" the convolution - padding : whether we add zeros on the boundary, or cut everything off to only take \"valid\" full convolutions","title":"Chap 10: CNNs"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%205/#heuristics","text":"Intersperse convlayers with downsampling by two Preserve \"size of data\" by doubling number of channels At some point the \"receptive field\" of a single value is the entire image, so we can now flatten out into two layers or so of FCNN Channels $\\iff$ parallel feature extraction For object detection/semantic segmentation: Add max-unpool and deconvolutions (basically inversions of conv) to generate a new image of the same size, with classifications for each pixel Then use a heuristic greedy algorithm to label collections of pixels with sufficient evidence together ![[Pasted image 20240317231859.png]] Don't forget to use other \"good ideas\" such as dropout, SGD, data augmentation, so on. Data can be augmented \"unsupervised\" by blacking out certain boxes etc.","title":"Heuristics"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/","text":"Chap 11: Residual Networks Residual blocks so convnet is pretty good, and deep networks are pretty good but really deep networks have \"shattered gradients\" aka multiple orders of derivative have high Lipschitz so we introduce \"residual blocks,\" aka taking $$h_3=f(h_2,\\phi)+h_2.$$ Then after a bunch of composition, the final expression is $$h_n=f(h_{n-1})+f(h_{n-2})+\\dots+h_1.$$ In other words, residual networks are actually, more realistically, an ensemble of correlated shallow networks, each of which take different \"short\" paths through the network. Batchnorm Each hidden node activation $h_i$ gets rescaled. Inside of the batch, take the statistical mean and variance, and then rescale so it has mean $\\sigma_i$ and variance $\\gamma_i$. Then make $\\sigma_i,\\gamma_i$ additional learned parameters for each node. After training and before testing, take statistical parameters across the entire training dataset and \"lock them in.\" In theory this helps because it lets us directly control/regularize for mean/variance at nodes that might otherwise be hard to control. Batchnorm Variants: Ghostnorm, Layernorm, Groupnorm, Instancenorm Ghostnorm: take some samples from this batch randomly Layernorm: go instance by instance, taking samples from this entire layer, as opposed to from the whole batch (better for text/images sometimes) Groupnorm: Normalize within groups of channels, as opposed to by layers of channels Instancenorm: normalize within each channel separately, taking stats only from pixels in that one channel U-Nets, Hourglass Take the convolutional encoder/decoder from last time, but now make it residual. Specific properties may have been lost through the \"compression\" in the middle, so when re-upscaling, concatenate the \"original image\" with the same resolution from the other side before mapping. If only valid convolutions are taken then cropping is necessary. Iterate this several times for \"stacked hourglass networks\". Chap 12: Transformers !! Premise Long input sequence of long vectors Sequence of vectors out that carry information in some way Desirable Properties Works no matter sequence length Inputs can talk to each other/interact with each other/determine each other's \"context\" Basic Implementation Matrices $\\Omega_k,\\Omega_q,\\Omega_v$ for key, query, value 1. $\\Omega_v$ maps input vectors to their values 2. $\\Omega_q,\\Omega_v$ maps input vectors to their key embeddings and query embeddings 3. Final embedding of input $x_i$ is softmax linear combination of values $v_i$ determined by dot product $k_j\\cdot q_i$ Position can be encoded by adding to the inputs In matrix form: - $X$ has input as column values - Matrices for values, keys, queries being $\\Omega_i X$ - As a reminder: - $X\\colon \\mathbb{R}^N\\to \\mathbb{R}^D$ with $N$ inputs, embedding length $D$ - $\\Omega_i\\colon \\mathbb{R}^D\\to\\mathbb{R}^l$ mapping to key/query vectors with length $l$ - Attention coeffs are $A=softmax_col(K^TQ)$ - Final output is $V\\cdot A$ We can do multihead meaning several such transformers are running at once. A normal transformer layer is ![[Pasted image 20240324220323.png]] Finally, we train GPT3 using encoder-decoder . ![[Pasted image 20240324220421.png]] We can train GPT3 very cheaply and unsupervised-ly: ![[Pasted image 20240324220524.png]] Anddd that's it.","title":"UDL   Week 6"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#chap-11-residual-networks","text":"","title":"Chap 11: Residual Networks"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#residual-blocks","text":"so convnet is pretty good, and deep networks are pretty good but really deep networks have \"shattered gradients\" aka multiple orders of derivative have high Lipschitz so we introduce \"residual blocks,\" aka taking $$h_3=f(h_2,\\phi)+h_2.$$ Then after a bunch of composition, the final expression is $$h_n=f(h_{n-1})+f(h_{n-2})+\\dots+h_1.$$ In other words, residual networks are actually, more realistically, an ensemble of correlated shallow networks, each of which take different \"short\" paths through the network.","title":"Residual blocks"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#batchnorm","text":"Each hidden node activation $h_i$ gets rescaled. Inside of the batch, take the statistical mean and variance, and then rescale so it has mean $\\sigma_i$ and variance $\\gamma_i$. Then make $\\sigma_i,\\gamma_i$ additional learned parameters for each node. After training and before testing, take statistical parameters across the entire training dataset and \"lock them in.\" In theory this helps because it lets us directly control/regularize for mean/variance at nodes that might otherwise be hard to control.","title":"Batchnorm"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#batchnorm-variants-ghostnorm-layernorm-groupnorm-instancenorm","text":"Ghostnorm: take some samples from this batch randomly Layernorm: go instance by instance, taking samples from this entire layer, as opposed to from the whole batch (better for text/images sometimes) Groupnorm: Normalize within groups of channels, as opposed to by layers of channels Instancenorm: normalize within each channel separately, taking stats only from pixels in that one channel","title":"Batchnorm Variants: Ghostnorm, Layernorm, Groupnorm, Instancenorm"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#u-nets-hourglass","text":"Take the convolutional encoder/decoder from last time, but now make it residual. Specific properties may have been lost through the \"compression\" in the middle, so when re-upscaling, concatenate the \"original image\" with the same resolution from the other side before mapping. If only valid convolutions are taken then cropping is necessary. Iterate this several times for \"stacked hourglass networks\".","title":"U-Nets, Hourglass"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#chap-12-transformers","text":"","title":"Chap 12: Transformers !!"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#premise","text":"Long input sequence of long vectors Sequence of vectors out that carry information in some way","title":"Premise"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#desirable-properties","text":"Works no matter sequence length Inputs can talk to each other/interact with each other/determine each other's \"context\"","title":"Desirable Properties"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%206/#basic-implementation","text":"Matrices $\\Omega_k,\\Omega_q,\\Omega_v$ for key, query, value 1. $\\Omega_v$ maps input vectors to their values 2. $\\Omega_q,\\Omega_v$ maps input vectors to their key embeddings and query embeddings 3. Final embedding of input $x_i$ is softmax linear combination of values $v_i$ determined by dot product $k_j\\cdot q_i$ Position can be encoded by adding to the inputs In matrix form: - $X$ has input as column values - Matrices for values, keys, queries being $\\Omega_i X$ - As a reminder: - $X\\colon \\mathbb{R}^N\\to \\mathbb{R}^D$ with $N$ inputs, embedding length $D$ - $\\Omega_i\\colon \\mathbb{R}^D\\to\\mathbb{R}^l$ mapping to key/query vectors with length $l$ - Attention coeffs are $A=softmax_col(K^TQ)$ - Final output is $V\\cdot A$ We can do multihead meaning several such transformers are running at once. A normal transformer layer is ![[Pasted image 20240324220323.png]] Finally, we train GPT3 using encoder-decoder . ![[Pasted image 20240324220421.png]] We can train GPT3 very cheaply and unsupervised-ly: ![[Pasted image 20240324220524.png]] Anddd that's it.","title":"Basic Implementation"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%207/","text":"Chap 13: GNNs Applicable for - maps, molecules, social networks, paper citations Setup Embed nodes with long vectors associated with each node Edges embedded in 1/0 adjacency matrix, sometimes dual edge graph adjacency matrices and edge embeddings We want the model to output new embeddings for each node that describes its context in the graph Motivation We want GNNs to respect relevant symmetries, specifically permutation of nodes As a result, the best we can do is sums of neighbor embeddings At each layer, for each node: - take sums of embeddings for neighbors and self - linear transform and activation So each subsequent layer will be $$H_{k+1}=a(\\beta_k 1^T+\\Omega_kH_k(A+I))$$ Each $H_k$ has $N$ columns of length $D$, for $D$-dimensional embeddings, $N$ nodes Tricks Batching is difficult for monolithic fill-in-the-blank graphs, it can be done by - sampling subset of neighbors randomly, for the receptive field from each layer - dividing graph deterministically to minimize lost edges With multiple graphs of similar size, zero-padding to use tensor trick","title":"UDL   Week 7"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%207/#chap-13-gnns","text":"Applicable for - maps, molecules, social networks, paper citations","title":"Chap 13: GNNs"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%207/#setup","text":"Embed nodes with long vectors associated with each node Edges embedded in 1/0 adjacency matrix, sometimes dual edge graph adjacency matrices and edge embeddings We want the model to output new embeddings for each node that describes its context in the graph","title":"Setup"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%207/#motivation","text":"We want GNNs to respect relevant symmetries, specifically permutation of nodes As a result, the best we can do is sums of neighbor embeddings At each layer, for each node: - take sums of embeddings for neighbors and self - linear transform and activation So each subsequent layer will be $$H_{k+1}=a(\\beta_k 1^T+\\Omega_kH_k(A+I))$$ Each $H_k$ has $N$ columns of length $D$, for $D$-dimensional embeddings, $N$ nodes","title":"Motivation"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%207/#tricks","text":"Batching is difficult for monolithic fill-in-the-blank graphs, it can be done by - sampling subset of neighbors randomly, for the receptive field from each layer - dividing graph deterministically to minimize lost edges With multiple graphs of similar size, zero-padding to use tensor trick","title":"Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/","text":"Chap 14: Unsupervised Learning ![[Pasted image 20240502173833.png]] For generative models, we have some parameters that determine how something gets generated. In the case of random gen models, the loss function is likelihood of generating the training data $\\sum_{x}-\\log(Pr(x_{i}|\\phi))$. Unsupervised models learn unstructured info, i.e. \"latent variables,\" on unlabelled data. Chap 15: GANs Setup Simple as it sounds. Generator $g(z_{j},\\theta)$ creates a sample given normally drawn $z_{j},$ and then discriminator $f(x,\\phi)$ determines $Pr(x\\text{ real}).$ Generator wants its samples to be marked as real; discriminator wants to be correct. Given fixed $\\phi,\\theta,$ the params $\\theta$ move in the direction that maximizes discriminator logscore given random $z_{j}$, while $\\phi$ moves in direction that minimizes logscore given generated $x_{j}$. Difficulty If the discriminator becomes \"too smart\" or \"too stupid\" there are vanishing gradients. Theory The optimal discriminator says, \"$x$ is either real or generated. Then given distributions of real and generated objs, I can determine likelihood of being real: $Pr(\\text{real}|x)=\\frac{{Pr(x)}}{Pr(x)+Pr(x^{*})}.$ In particular, the loss $L(\\phi)$ can be expressed as Jenson-Shannon divergence $D_{JS}\\left[ Pr(x^{ })\\lvert \\rvert Pr(x) \\right].$ Vanishing gradient is due to this divergence behaving poorly, so instead pick Wasserstein earth-mover's distance*, i.e. map probability masses, and integrate total mass-distance. Defined as $$ D[Pr(x),q(x)]=\\max_{{f}}\\left[ \\int Pr(x)f[x] \\, dx -\\int q(x)f[x] \\, dx \\right], $$ where $\\lvert \\nabla f \\rvert\\leq 1.$ Then new loss for the discriminator $$ L[\\phi]=\\sum_{j}f[g[z_{j},\\theta],\\phi]-\\sum_{j}f[x_{i},\\phi].$$ Lipschitz constraint guaranteed by clipping weights, or regularization. Implementation Tricks Progressive growing GAN generator usually CNN. First train to generate 4x4, and then add more to go to 16x16, etc. Mini-batch Create stats across activations e.g. in discriminator, add regularization to force generator to create diversity Manipulating latent variables Self-evident Conditional Pass a vector into both the generator and discriminator, and give matching conditional vector for real samples in discriminator. For text, vector is transformer embedding. Conversely, discriminator can be called to predict attributes, called InfoGAN. Usual tricks Careful regularization, step scheduling, ADAM+SGD","title":"UDL   Week 8"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#chap-14-unsupervised-learning","text":"![[Pasted image 20240502173833.png]] For generative models, we have some parameters that determine how something gets generated. In the case of random gen models, the loss function is likelihood of generating the training data $\\sum_{x}-\\log(Pr(x_{i}|\\phi))$. Unsupervised models learn unstructured info, i.e. \"latent variables,\" on unlabelled data.","title":"Chap 14: Unsupervised Learning"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#chap-15-gans","text":"","title":"Chap 15: GANs"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#setup","text":"Simple as it sounds. Generator $g(z_{j},\\theta)$ creates a sample given normally drawn $z_{j},$ and then discriminator $f(x,\\phi)$ determines $Pr(x\\text{ real}).$ Generator wants its samples to be marked as real; discriminator wants to be correct. Given fixed $\\phi,\\theta,$ the params $\\theta$ move in the direction that maximizes discriminator logscore given random $z_{j}$, while $\\phi$ moves in direction that minimizes logscore given generated $x_{j}$.","title":"Setup"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#difficulty","text":"If the discriminator becomes \"too smart\" or \"too stupid\" there are vanishing gradients.","title":"Difficulty"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#theory","text":"The optimal discriminator says, \"$x$ is either real or generated. Then given distributions of real and generated objs, I can determine likelihood of being real: $Pr(\\text{real}|x)=\\frac{{Pr(x)}}{Pr(x)+Pr(x^{*})}.$ In particular, the loss $L(\\phi)$ can be expressed as Jenson-Shannon divergence $D_{JS}\\left[ Pr(x^{ })\\lvert \\rvert Pr(x) \\right].$ Vanishing gradient is due to this divergence behaving poorly, so instead pick Wasserstein earth-mover's distance*, i.e. map probability masses, and integrate total mass-distance. Defined as $$ D[Pr(x),q(x)]=\\max_{{f}}\\left[ \\int Pr(x)f[x] \\, dx -\\int q(x)f[x] \\, dx \\right], $$ where $\\lvert \\nabla f \\rvert\\leq 1.$ Then new loss for the discriminator $$ L[\\phi]=\\sum_{j}f[g[z_{j},\\theta],\\phi]-\\sum_{j}f[x_{i},\\phi].$$ Lipschitz constraint guaranteed by clipping weights, or regularization.","title":"Theory"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#implementation-tricks","text":"","title":"Implementation Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#progressive-growing","text":"GAN generator usually CNN. First train to generate 4x4, and then add more to go to 16x16, etc.","title":"Progressive growing"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#mini-batch","text":"Create stats across activations e.g. in discriminator, add regularization to force generator to create diversity","title":"Mini-batch"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#manipulating-latent-variables","text":"Self-evident","title":"Manipulating latent variables"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#conditional","text":"Pass a vector into both the generator and discriminator, and give matching conditional vector for real samples in discriminator. For text, vector is transformer embedding. Conversely, discriminator can be called to predict attributes, called InfoGAN.","title":"Conditional"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%208/#usual-tricks","text":"Careful regularization, step scheduling, ADAM+SGD","title":"Usual tricks"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%209/","text":"Chap 16: Normalizing Flows Creates probability distribution for whole space, starting with unit normal sample and reversibly creating object sample. In this invertible model it's easy to compute likelihood that any given sample is generated, so loss function is straightforward. ![[Pasted image 20240502184111.png]] $g$ is some nonlinear function composed with linear map $\\phi,$ could be Leaky ReLU or sigmoid. Student-Teacher However, this form of inverse mapping is time-expensive. Instead we define loss as reverse KL divergence between a generated \"student\" distribution and known/sample-able \"teacher\" distribution.","title":"UDL   Week 9"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%209/#chap-16-normalizing-flows","text":"Creates probability distribution for whole space, starting with unit normal sample and reversibly creating object sample. In this invertible model it's easy to compute likelihood that any given sample is generated, so loss function is straightforward. ![[Pasted image 20240502184111.png]] $g$ is some nonlinear function composed with linear map $\\phi,$ could be Leaky ReLU or sigmoid.","title":"Chap 16: Normalizing Flows"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20-%20Week%209/#student-teacher","text":"However, this form of inverse mapping is time-expensive. Instead we define loss as reverse KL divergence between a generated \"student\" distribution and known/sample-able \"teacher\" distribution.","title":"Student-Teacher"},{"location":"Material%20Knowledge/Sciences/Coding/ML/udl_book/UDL%20Notes/","text":"![[UDL - Week 1]] ![[UDL - Week 2]] ![[UDL - Week 3]] ![[UDL - Week 4]] ![[UDL - Week 5]] ![[UDL - Week 6]] ![[UDL - Week 7]] ![[UDL - Week 8]] ![[UDL - Week 9]] ![[UDL - Week 10]] ![[UDL - Week 11]]","title":"UDL Notes"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/","text":"Rust is a parallel language to C++, with many similar ideas (static typing, compiled language) but also many new ones (very strong compiler guarantees, safe/unsafe code). Here are some the of the key ideas. Rust Compiler Stronk Ownership Every item is owned by some variable name. The code must always \"know\" what each object is owned by. For instance, a vector owns all of its constituents. These are the permitted operations: - Immutable borrow (must be \"returned\") - Mutable borrow (can write, nobody else can borrow, must be \"returned\") - Move (change ownership, previous variable gets \"consumed\") When passing &var or &mut var in a function signature, that's a borrow. But when directly passing var , that's a move, and the original variable cannot be accessed anymore. Consuming can be a good API for when an object should not be used anymore after a function call, i.e. all of the important guts got \"moved\" out. There's lots of work that goes into moving objects that can't be clone() , such as remove() and swap() . Lifetimes There are also lifetimes, which the compiler usually infers but sometimes special things need to be done. - Suppose A has a skateboard, B borrows the skateboard from A, and C borrows a sticker on the skateboard from B. Then C gives the sticker to D (function return). - The first borrow must last at least as long as the second borrow, i.e. the reference must live at least as long. This is only used when change of scope happens and the second reference moves out of the scope e.g. function calls. If the sticker is borrowed in \"inline\" code the skateboard reference is immediately required to live that long. Safety paradigms C++ makes the choice of making the programmer check guarantees at runtime . The Rust compiler enforces certain guarantees at compile-time so that those bugs can be found earlier by the compiler, rather than in production. The errors thrown at compiletime are usually panics , such as division by zero, unwrapping a None value, Typing choices Traits Rust borrows a lot from type theory, while mixing in some of the compiled-language efficiency of C++. Namely, all objects have static types, defined with the struct keyword. Traits are a \"property\" that multiple types can satisfy. Traits can provide \"associated types\" and methods. Functions have their arguments be a certain type, but they can also have arguments satisfy certain traits. Generics Syntactically, generic types are integrated into function signatures and traits in a way different from other languages. Rust's rigorous type checking encourages frequent use of generics. Generics can also be required to have trait bounds, so that e.g. an argument assumed to be of a certain type satisfying a given trait can call the methods from that trait, and Rust will know those methods exist. Traits with different generics become different traits, so MyTrait<i32> and MyTrait<bool> can both be implemented by the same type. Enums An enum is syntactic sugar for some type to be one of a list of hard-coded types. There is special match syntax that allows for concise control flow depending on the type of an enum. The Option<T> type The Null type is just horrible. Hence, the Option<T> , which is an enum for either None or T . Calling None.unwrap() will panic!() . Trait objects There is a special &dyn Trait keyword that allows for pointers to a set of objects that satisfy a given trait. Then these objects could be any type that implements the trait, allowing for expandable features. This is implemented under the hood with a vtable matching types to function pointers. Smart pointers There's Box , Mutex , Rc , Arc , when the reference doesn't suffice. Just google if necessary. Specifically, the Box moves its contents to heap and adds a pointer to the object on the stack, returning the pointer. It puts the object in a box. Other Rust specifics Iterators Oh, my god! these are awesome. Calling .iter() on a collection will create an object satisfying the Iterator trait, i.e. it has next() etc. creating immutable borrow. Then there's iter_mut() which returns mutable borrows, and into_iter() creating iterators that own the respective objects. Unsafe code When exposing some key functionality such as splitting a vector (consuming a mutable slice and returning two halves of that slice) unsafe code must be used. It's wrapped in a unsafe code block, and it's up to the programmer to check that the contracts of that function are satisfied. Tests Rust has builtin configs such as #[cfg(test)] that makes unit tests and integration tests very easy. Routing Code is organized into crates. Crates are split into \"library\" denoted by src and \"binary\" denoted by bin . The binaries are meant for production, and libraries as APIs for binaries or other projects. Cyclic imports between crates is not allowed, but any imports within one crate is fine. Imports from other crates need to be declared in Cargo.toml , along with other compilation and testing options. mod creates a module, and use imports things. Obviously, the placement and naming of files can determine their import paths. Macros Every #[...] generates code of some kind, using a TokenStream of the code block below it. All the macro!() lines apply some pattern to generate code in place at compile time, in a way similar to functions, except the assembly is quite literally expanded. Parallelism Can initiate multiple threads. Object must implement Send if its ownership is going to be transferred to another thread, and must implement Sync if its references can be sent between threads (i.e. multiple threads can read/write). So constants are Send, and Arc/Mutex are Sync, etc.","title":"Rust notes"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#rust-compiler-stronk","text":"","title":"Rust Compiler Stronk"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#ownership","text":"Every item is owned by some variable name. The code must always \"know\" what each object is owned by. For instance, a vector owns all of its constituents. These are the permitted operations: - Immutable borrow (must be \"returned\") - Mutable borrow (can write, nobody else can borrow, must be \"returned\") - Move (change ownership, previous variable gets \"consumed\") When passing &var or &mut var in a function signature, that's a borrow. But when directly passing var , that's a move, and the original variable cannot be accessed anymore. Consuming can be a good API for when an object should not be used anymore after a function call, i.e. all of the important guts got \"moved\" out. There's lots of work that goes into moving objects that can't be clone() , such as remove() and swap() .","title":"Ownership"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#lifetimes","text":"There are also lifetimes, which the compiler usually infers but sometimes special things need to be done. - Suppose A has a skateboard, B borrows the skateboard from A, and C borrows a sticker on the skateboard from B. Then C gives the sticker to D (function return). - The first borrow must last at least as long as the second borrow, i.e. the reference must live at least as long. This is only used when change of scope happens and the second reference moves out of the scope e.g. function calls. If the sticker is borrowed in \"inline\" code the skateboard reference is immediately required to live that long.","title":"Lifetimes"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#safety-paradigms","text":"C++ makes the choice of making the programmer check guarantees at runtime . The Rust compiler enforces certain guarantees at compile-time so that those bugs can be found earlier by the compiler, rather than in production. The errors thrown at compiletime are usually panics , such as division by zero, unwrapping a None value,","title":"Safety paradigms"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#typing-choices","text":"","title":"Typing choices"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#traits","text":"Rust borrows a lot from type theory, while mixing in some of the compiled-language efficiency of C++. Namely, all objects have static types, defined with the struct keyword. Traits are a \"property\" that multiple types can satisfy. Traits can provide \"associated types\" and methods. Functions have their arguments be a certain type, but they can also have arguments satisfy certain traits.","title":"Traits"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#generics","text":"Syntactically, generic types are integrated into function signatures and traits in a way different from other languages. Rust's rigorous type checking encourages frequent use of generics. Generics can also be required to have trait bounds, so that e.g. an argument assumed to be of a certain type satisfying a given trait can call the methods from that trait, and Rust will know those methods exist. Traits with different generics become different traits, so MyTrait<i32> and MyTrait<bool> can both be implemented by the same type.","title":"Generics"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#enums","text":"An enum is syntactic sugar for some type to be one of a list of hard-coded types. There is special match syntax that allows for concise control flow depending on the type of an enum.","title":"Enums"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#the-optiont-type","text":"The Null type is just horrible. Hence, the Option<T> , which is an enum for either None or T . Calling None.unwrap() will panic!() .","title":"The Option&lt;T&gt; type"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#trait-objects","text":"There is a special &dyn Trait keyword that allows for pointers to a set of objects that satisfy a given trait. Then these objects could be any type that implements the trait, allowing for expandable features. This is implemented under the hood with a vtable matching types to function pointers.","title":"Trait objects"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#smart-pointers","text":"There's Box , Mutex , Rc , Arc , when the reference doesn't suffice. Just google if necessary. Specifically, the Box moves its contents to heap and adds a pointer to the object on the stack, returning the pointer. It puts the object in a box.","title":"Smart pointers"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#other-rust-specifics","text":"","title":"Other Rust specifics"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#iterators","text":"Oh, my god! these are awesome. Calling .iter() on a collection will create an object satisfying the Iterator trait, i.e. it has next() etc. creating immutable borrow. Then there's iter_mut() which returns mutable borrows, and into_iter() creating iterators that own the respective objects.","title":"Iterators"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#unsafe-code","text":"When exposing some key functionality such as splitting a vector (consuming a mutable slice and returning two halves of that slice) unsafe code must be used. It's wrapped in a unsafe code block, and it's up to the programmer to check that the contracts of that function are satisfied.","title":"Unsafe code"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#tests","text":"Rust has builtin configs such as #[cfg(test)] that makes unit tests and integration tests very easy.","title":"Tests"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#routing","text":"Code is organized into crates. Crates are split into \"library\" denoted by src and \"binary\" denoted by bin . The binaries are meant for production, and libraries as APIs for binaries or other projects. Cyclic imports between crates is not allowed, but any imports within one crate is fine. Imports from other crates need to be declared in Cargo.toml , along with other compilation and testing options. mod creates a module, and use imports things. Obviously, the placement and naming of files can determine their import paths.","title":"Routing"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#macros","text":"Every #[...] generates code of some kind, using a TokenStream of the code block below it. All the macro!() lines apply some pattern to generate code in place at compile time, in a way similar to functions, except the assembly is quite literally expanded.","title":"Macros"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#parallelism","text":"Can initiate multiple threads. Object must implement Send if its ownership is going to be transferred to another thread, and must implement Sync if its references can be sent between threads (i.e. multiple threads can read/write). So constants are Send, and Arc/Mutex are Sync, etc.","title":"Parallelism"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-30/","text":"source here rust new version of std::sort, called driftsort for small cases (<=20 elem) it does either insertion sort (for NonFreeze types that can be mutated during comparison, or Arc that can't be copied byte by byte) in general it does quicksort using a weird $n^{\\log_{8}3}$ recursive median of 3s algorithm they do a fun thing with incrementing lower pointer, decrementing larger pointer, swap when bump into a swap that can be made and if a $\\sqrt{ n }$ run is found it does some merging using mergesort takeaways: - lots of hardware-centric optimizations - branchless arithmetic computations - small compiled code for instruction-cache - different sorting algorithms depending on level of safety and then ofc quicksort is good for cache locality to begin with also learned about CMOVE which only moves data on a conditional, instead of branching on a conditional - branch mispredictions can be expensive there are some sort9/sort13 things hardcoded in for smallsort Quicksort there's two ways to execute quicksort partition - hoare, have left pointer coming right and right pointer coming left which swap - lomuto, have pointer coming right and \"last swapped\" pointer which is last below item, swap with first higher when encounter new lower https://github.com/Voultapher/sort-research-rs/blob/main/writeup/lomcyc_partition/text.md swap optimizations: - \"branchless\": instead of doing conditional branches (high overhead cuz of mispredict), just make copies of both and choose using arithmetic - \"cyclic permutation\": a swap is acutally \"store, move, move\" so amortized 3 per swap - if you just put a \"gap\" in and manage correctly then only amortized 2 per swap also about \"sort safety\", apparently C people think users should make sure their sort functions are weak strict sorting, as opposed to","title":"Nysrg 6 30"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-30/#quicksort","text":"there's two ways to execute quicksort partition - hoare, have left pointer coming right and right pointer coming left which swap - lomuto, have pointer coming right and \"last swapped\" pointer which is last below item, swap with first higher when encounter new lower https://github.com/Voultapher/sort-research-rs/blob/main/writeup/lomcyc_partition/text.md swap optimizations: - \"branchless\": instead of doing conditional branches (high overhead cuz of mispredict), just make copies of both and choose using arithmetic - \"cyclic permutation\": a swap is acutally \"store, move, move\" so amortized 3 per swap - if you just put a \"gap\" in and manage correctly then only amortized 2 per swap also about \"sort safety\", apparently C people think users should make sure their sort functions are weak strict sorting, as opposed to","title":"Quicksort"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-9%20%2B%206-16/","text":"SQL NoSQL NewSQL care about ACID, atomicity (transactions don't complete partially), consistency (always correct), isolation (txs don't inteerfere with each other), durability (data put now is there later) FoundationDB: a \"simple\" \"minimalistic\" key-value store that first built in 2009, Apple acquired, open-sourced in 2021 distributed database ![[Pasted image 20240616144010.png]] client talks to proxy, asking for a read-only snapshot, proxy returns read version e.g. tx A has rv 100, tx B has rv 101, then tx A will receive commit version 102 i.e. at v102 tx A's thing would be \"done\" there's a resolver that figures out what conflicts exist and what commit time is OK keeps a hybrid clock of some kind e.g. if tx B,C,D use rv 102, 103, 104, then tx A gets cv 105 actually scratch that client asks proxy for rv. greater than any previous issued cv then goes and asks storage system (through ls through proxy) for certain reads at that rv finally submits read/write sets to proxy, proxy asks sequencer for a commit version in ekzhang's head: - coordinators form a distributed quorum storing config, control, etc. - delegate a sequencer, ratekeeper, data distributor (?) -","title":"Nysrg 6 9 + 6 16"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%207-7/","text":"Okay, just finished reading Redis version 1.3.6 with ekzhang's NYSRG. Some of the things that I learned are... This person put an entire database, socket, query, protocol, all in basically a single 9000 C file with a bunch of internalizers. So, there's a picture of the server that will probably be placed here. But, basically it just has all the pieces that you need just put together, wrapped together. So, it supports virtual memory, so you can swap from memory onto disk. It supports flushing memory onto disk. It supports what's called RDB, Redis Database. So, it will just copy everything to disk periodically. It holds the entire database in memory, which is kind of weird. I suppose that's what makes it kind of easy slash small in the sense that it can be stored entirely in a 9000-line C file because everything is just kind of done in memory. Also, relative to its 2010 launch, it had the innovation of supporting lists, sets, large datasets. I guess this time reading code, I learned about paying attention to structs and paying attention to config files and comments. The comments in the config files can kind of direct your attention as to what to pay attention to. In particular, when trying to dissect large libraries, the important thing is to figure out which structs and methods are important and then frame your intuition around those structs. Create a mental model that frames around some key elements and then fill in details one at a time when reading other methods. file:///Users/alex-zhao/Coding/assorted/redis/doc/README.html ![[Pasted image 20240707205010.png]]","title":"Nysrg 7 7"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%20week%2031/","text":"mainframe -> aws compute warehouse -> disaggregated db warehouse model - several vms each with their own cpu, ram, disk, gpu, etc. - each separated, just rent your own - ISSUE: some tasks use only lots fo ram, lots of disk, lots of gpu (ML) etc. disaggregated model - large block of cpu, ram, disk that gets cut up into pieces for each job dynamically -","title":"Nysrg week 31"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/","text":"Recursion+DP Transition How do I build a state from a previous state? What information needs to be preserved? What information can be disposed of? How do I \"unpeel\" it by layers? Iterate? Backtracking If I want to explore multiple \"children\" of the current state, how do I return to the parent? how to keep track of the children? DFS traversal of the tree, in essence Memoization ==What computations are repeated?== What's a naive DP idea? How can I ==add info== to make a valid transition? Specific Tricks Permutations can be bitmasked Subarray DPs, such as digit substrings, or XORs from big-small index Use dx/dy for grid problems Utilize spatial locality If potential MLE discard DP layers no longer in use Almost always write iteratively Unless it's tree, in which case write recursively Data Structures Set, MultiSet are ordered BBST Unordered are hash tables ==Cheat with PBDS==, good for sliding window Fenwick Tree: ==last 2^n==, for range queries (not used) Segment Tree: just fill out to power of 2; consider \"lazy Segment\" or \"Segment of Segment\" Remember PURS, RUPF, if needed consult Plat for RURS Use explicit representation (in 1D flattened array) for random access Stack, Queue, Deque: much better constant-time performance than other containers Priority Queue maintained by Heap, maintaining greater than children Graph Reps Adjacency Matrix: weight[i][j] Adjacency List: adj[i] lists all neighbors and corresponding weights of i Union-Find Disjoint Set: rep. partition of a collection as a forest, root of each tree is representative for set Greedy with Dijkstra's or Kruskal's using ==PQ== Tree Augmented DFS Includes Tree DP and Euler Tour Always Root Use DP Mindset-what can I memoize? ==Start with naive recursion== $\\sqrt n$ Divide and Conquer Range queries: rep. as pairs $(a,b),$ sliding window by semantic order Basically Meet in Middle for $\\sqrt N$ time","title":"USACO Notes"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#recursiondp","text":"","title":"Recursion+DP"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#transition","text":"How do I build a state from a previous state? What information needs to be preserved? What information can be disposed of? How do I \"unpeel\" it by layers? Iterate?","title":"Transition"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#backtracking","text":"If I want to explore multiple \"children\" of the current state, how do I return to the parent? how to keep track of the children? DFS traversal of the tree, in essence","title":"Backtracking"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#memoization","text":"==What computations are repeated?== What's a naive DP idea? How can I ==add info== to make a valid transition?","title":"Memoization"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#specific-tricks","text":"Permutations can be bitmasked Subarray DPs, such as digit substrings, or XORs from big-small index Use dx/dy for grid problems Utilize spatial locality If potential MLE discard DP layers no longer in use Almost always write iteratively Unless it's tree, in which case write recursively","title":"Specific Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#data-structures","text":"Set, MultiSet are ordered BBST Unordered are hash tables ==Cheat with PBDS==, good for sliding window Fenwick Tree: ==last 2^n==, for range queries (not used) Segment Tree: just fill out to power of 2; consider \"lazy Segment\" or \"Segment of Segment\" Remember PURS, RUPF, if needed consult Plat for RURS Use explicit representation (in 1D flattened array) for random access Stack, Queue, Deque: much better constant-time performance than other containers Priority Queue maintained by Heap, maintaining greater than children","title":"Data Structures"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#graph-reps","text":"Adjacency Matrix: weight[i][j] Adjacency List: adj[i] lists all neighbors and corresponding weights of i Union-Find Disjoint Set: rep. partition of a collection as a forest, root of each tree is representative for set Greedy with Dijkstra's or Kruskal's using ==PQ==","title":"Graph Reps"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#tree-augmented-dfs","text":"Includes Tree DP and Euler Tour Always Root Use DP Mindset-what can I memoize? ==Start with naive recursion==","title":"Tree Augmented DFS"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#sqrt-n-divide-and-conquer","text":"Range queries: rep. as pairs $(a,b),$ sliding window by semantic order Basically Meet in Middle for $\\sqrt N$ time","title":"$\\sqrt n$ Divide and Conquer"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/Syntax%20Details/","text":"Pre/Post Increment: --x, x-- (first in cmdname happens first) struct : class with all public attributes Initialize Stick to template for segtree Cool template goto bits/stdc++.h.gch use lambdas creatively, e.g. sorting int *ptr creates pointer ptr to int object (automatic reference for C-type array) 1-index for graph or tree problems! And 1-index segtrees","title":"Syntax Details"},{"location":"Material%20Knowledge/Sciences/MCB/1%20Intro/Chem%20Basics/","text":"Covalent/ionic bonds Covalent bonds are polar/nonpolar Hydrogen bonds are attractions between partial charges from polar bonds Hydrogen bonding occurs between FON atoms and the hydrogens they are connected to","title":"Chem Basics"},{"location":"Material%20Knowledge/Sciences/MCB/1%20Intro/Macromolecules/","text":"Proteins composed of amino acids Serve as enzymes, hormones, storage, motor, structure, receptor, transport, defense Most important function as catalytic enzyme, allowing reactions to happen, acting as workhorse Amino acid is alpha C attached to H, Amino (NH 2 ), Carboxyl (COOH) and R group C-terminus (last carboxyl) and N-terminus (last amino) Nonpolar R groups result in hydrophobic, polar/partially ionized R groups result in hydrophilic Peptide bonds dehydration bond amino to carboxyl Folding happens spontaneously when polypeptide formed Four levels of structure: Primary: peptides in the chain Secondary: hydrogen bonding within the chain that self-stabilizes, particularly into alpha-helices and beta-sheets Tertiary: Further self-stabilization with additional side chains, with disulfide bridges Quaternary (Epic only): Two or more polypeptides together, with disulfide bridges, H/dispersion forces Denaturation, from chemical or heat input X-ray crystallagraphy used to read protein shape Carbohydrates molecules that are equal numbers of carbon and hydrogen E.g. CH 2 O Glucose is 6 of these arrange in a ring Except one ring node replaced with an O, other \"piece\" moved onto node adjacent to O Carbons in ring are numbered 1-6 starting from O, ending on moved piece Cellulose is glucoses attached in alternating style Starch is glucoses attached 1-4 in direct style Glycogen is direct starch-like bonds with strange complex branches Nucleic Acid molecules such as RNA, DNA composed of nucleotides Ribose Nucleic Acid (Ribose is a sugar) Deoxy (no oxygen in the ribose) Ribose Nucleic Acid Bonds are called \"phosphodiester\" Polynucleotide/nucleic acid, ribosomes create proteins formed from amino acids informed by mRNA formed from DNA Nucleotide structure Sugar (ribose or 2-deoxyribose) Nitrogenous group (purine, A or G, vs pyrimidine, C, G, or U [RNA only]) A-T, G-C Some number of phosphate groups Ends indexed by (start) phosphate+5' to (end) 3' Double helix held together by hydrogen bonding Lipids are not polymers; fats or steroids Fats often have glycerol membrane with fatty acids (carbon-hydrogen chains) If carbons are double-bonded, chain is bent and fats cannot pack tightly (unsaturated, liquid) Otherwise all single-bonded, chain is straight and fats pack tightly (saturated, solid) Phospholipids have two fatty acids and one phosphate group","title":"Macromolecules"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Central%20Dogma%E2%80%94DNA/","text":"DNA becomes mRNA, becomes protein through ribosome Replication OrdoOp Origin of replication: specific DNA marker, eukaryotes can have 100s \"Bubbles\" form at each origin, and they grow at both ends (called replication forks) until two strands Helicase at the rep. fork unwind and separate the DNA Synthesis started by mRNA primer DNA is separated into two strands, the two strands each get another complete DNA built on top of it Nucleotides added to 3' end Polymerase adds triphosphate nucleotides to DNA Pyrophosphates come off Attached onto 3' end of strand One strand grows directly, called leading One strand grows in the opposite direction with Okazaki fragments (RNA) Attached to previous fragments by ligase Changed into DNA by pol 1 Polymerase-3 attaches to leading and adds nucleotides Come in as triphosphates; pyrophosphate comes out Maintenance and stability Sliding clamp keeps the two strands together Note the antiparallelity Polymerase Replication Correction Every 100k, wrong nucleotide matched First proofreading gets to every 10B If mistake persists, scanning proteins find \"bulges\" to get fixed by polymerase Telomerase The new strand in replication doesn't have stuff on it Each time duplication happens DNA gets shorter Telomerase rebuilds telomeres on the ends of DNA","title":"Central Dogma\u2014DNA"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Central%20Dogma%E2%80%94DNA/#replication-ordoop","text":"Origin of replication: specific DNA marker, eukaryotes can have 100s \"Bubbles\" form at each origin, and they grow at both ends (called replication forks) until two strands Helicase at the rep. fork unwind and separate the DNA Synthesis started by mRNA primer DNA is separated into two strands, the two strands each get another complete DNA built on top of it Nucleotides added to 3' end Polymerase adds triphosphate nucleotides to DNA Pyrophosphates come off Attached onto 3' end of strand One strand grows directly, called leading One strand grows in the opposite direction with Okazaki fragments (RNA) Attached to previous fragments by ligase Changed into DNA by pol 1 Polymerase-3 attaches to leading and adds nucleotides Come in as triphosphates; pyrophosphate comes out Maintenance and stability Sliding clamp keeps the two strands together Note the antiparallelity","title":"Replication OrdoOp"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Central%20Dogma%E2%80%94DNA/#polymerase","text":"","title":"Polymerase"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Central%20Dogma%E2%80%94DNA/#replication-correction","text":"Every 100k, wrong nucleotide matched First proofreading gets to every 10B If mistake persists, scanning proteins find \"bulges\" to get fixed by polymerase","title":"Replication Correction"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Central%20Dogma%E2%80%94DNA/#telomerase","text":"The new strand in replication doesn't have stuff on it Each time duplication happens DNA gets shorter Telomerase rebuilds telomeres on the ends of DNA","title":"Telomerase"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Gene%20structure%2C%20expression/","text":"Gene Expression 5' to 3' on top, \"coding\"; 3' to 5' on bottom, \"template\" Polymerase acts on the template, promoters tell polymerase where to bind TATA box in the promoter Many transcription factors interacting with promoter, allowing for actual gene expression Post Transcription Certain substrings denote ends of introns, exon ![[Pasted image 20231107144818.png]] Introns get spliced out by spliceosomes, a mix of polypeptides and RNA that identify splice sites Then guanine added to 5' side, poly-A tails of adenine s to 3' side Tail protects from degradation Translation Ribosome builds proteins from mRNA In general \"ribozymes\" or proteins that contain RNA (portm. ribo-, enzyme)","title":"Gene structure, expression"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Gene%20structure%2C%20expression/#gene-expression","text":"5' to 3' on top, \"coding\"; 3' to 5' on bottom, \"template\" Polymerase acts on the template, promoters tell polymerase where to bind TATA box in the promoter Many transcription factors interacting with promoter, allowing for actual gene expression","title":"Gene Expression"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Gene%20structure%2C%20expression/#post-transcription","text":"Certain substrings denote ends of introns, exon ![[Pasted image 20231107144818.png]] Introns get spliced out by spliceosomes, a mix of polypeptides and RNA that identify splice sites Then guanine added to 5' side, poly-A tails of adenine s to 3' side Tail protects from degradation","title":"Post Transcription"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Gene%20structure%2C%20expression/#translation","text":"Ribosome builds proteins from mRNA In general \"ribozymes\" or proteins that contain RNA (portm. ribo-, enzyme)","title":"Translation"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Genes%20and%20Enzymes/","text":"Beadle & Tatum '41 x-ray'd a simple bread mold Experimented with various mutants that \"needed\" extra nutrients Theorized biological pathway Orthinine -> Citrulline -> Arginine Recall Codon Wheel , translating 3-nucleotide strings into amino acids","title":"Genes and Enzymes"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/PCR%20Process/","text":"Lyse worms Place into lysis buffer (detergent and proteinase K) Frozen to crack open worms 60C to allow protein to break down cells and proteins in cells 95C for 15m to denature proteinase PCR Add Forward/Reverse Primer (length 18bp preferred) Master Mix (buffer, polymerase, nucleotides) As primer gets built, it's actually RNA (that means U and not T) Gel Electrophoresis Restriction enzymes cut DNA into bits at very specific locations, matching certain subseqs Each piece placed in gel with uniform electric potential field DNA phosphate negatively charged Large pieces will move more slowly towards positive side Bars on the gel can be revealed by staining and placing under UV","title":"PCR Process"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/PCR%20Process/#lyse-worms","text":"Place into lysis buffer (detergent and proteinase K) Frozen to crack open worms 60C to allow protein to break down cells and proteins in cells 95C for 15m to denature proteinase","title":"Lyse worms"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/PCR%20Process/#pcr","text":"Add Forward/Reverse Primer (length 18bp preferred) Master Mix (buffer, polymerase, nucleotides) As primer gets built, it's actually RNA (that means U and not T)","title":"PCR"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/PCR%20Process/#gel-electrophoresis","text":"Restriction enzymes cut DNA into bits at very specific locations, matching certain subseqs Each piece placed in gel with uniform electric potential field DNA phosphate negatively charged Large pieces will move more slowly towards positive side Bars on the gel can be revealed by staining and placing under UV","title":"Gel Electrophoresis"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Protein%20Synthesis/","text":"Ribosomal subunits formed in nucleolus (small nuclear RNA) Transcription to mRNA from pre-mRNA in nucleus mRNA exits from pore in nucleus Small \"red dots\" in cytoplasm are ribosom[al] RNA, translate mRNA using tRNA Alternately, bound ribosomes on rough ER translate secreted proteins Reminder on mutations Silent, Misssense (amino changes), Nonsense (STOP or START inserted) Change splicing Deletions/insertions can create frameshifts that create nonsense","title":"Protein Synthesis"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Protein%20Synthesis/#reminder-on-mutations","text":"Silent, Misssense (amino changes), Nonsense (STOP or START inserted) Change splicing Deletions/insertions can create frameshifts that create nonsense","title":"Reminder on mutations"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Testing%20Gene%20Expression/","text":"Get a sample, extract the mRNA Run out the mRNAs on electrophoresis Have a \"probe\" that binds/dyes chosen mRNA segment and labels it Measure amount of probe that ends up attached to each part","title":"Testing Gene Expression"},{"location":"Material%20Knowledge/Sciences/MCB/2%20DNA%20%2B%20Protein%20Synthesis/Translation%20Process/","text":"tRNA binds to mRNA carrying amino acids mRNA has start codon, end codon, that's where ribosome works Codon wheel translates nucleotides into amino acids Amino acids attached to tRNA by aminoacyl-tRNA synthethase Checks that the amino acid matches anticodon One for each amino acid Initiation, Elongation, Termination Initiator Methionine \"intiation complex\" for tRNA: A site has mRNA nucleotides paired, in opposite order AUG are STOP codons (anticodon CAU) UAG, UAA, UGA are STOP codons (anticodons CUA, UUA, UCA) Nucleotides covalently bonded to tRNA Slots are E, P, A New tRNA steps in A, then mRNA chain moves, E breaks off Release factor fits into P site for STOP","title":"Translation Process"},{"location":"Material%20Knowledge/Sciences/MCB/3%20Gene%20Expression/Epigenetics/","text":"\"Second genome\" on top of the genome that modifies expression - Chromatin (rolled up structure) can be another variable in gene expression - Histone Modifications - Histones are the protein balls DNA wrap around; DNA negative, histones positive - Acetylation tail -> open DNA (euchromatin), neutralizes lysine positive charge on histone - Methylation tail -> compact DNA, more permanent shutoff (heterochromatin) - Hereditary (each daughter has half strand, update afterwards) - Both -> reversible enzyme-catalyzed chemical rxns","title":"Epigenetics"},{"location":"Material%20Knowledge/Sciences/MCB/3%20Gene%20Expression/MicroRNA/","text":"a non-protein-coding gene makes an RNA that can \"fold\" This small RNA can: inhibit translation on mRNA degrade mRNA form chromatins Noted study: lin-4, lin-14","title":"MicroRNA"},{"location":"Material%20Knowledge/Sciences/MCB/3%20Gene%20Expression/Reporter%20Genes/","text":"Insert a new gene that produces a fluorescent protein Link it with the same promoter to the gene you want to measure Then can measure gene expression by measuring fluorescence","title":"Reporter Genes"},{"location":"Material%20Knowledge/Sciences/MCB/3%20Gene%20Expression/Repressors%2C%20Operons%2C%20Pathways/","text":"In bact., genes (1 protein each) can be promoted together, called operon (e.g. pathway to make tryptophan) Operator (transcription binding region) Regulatory gene builds repressor With tryptophan in environment, repressor binds to operator, repressing it Coordinatedly controlled genes have same control elements (binding regions) Anabolic pathway: build stuff (=ON) (repressor=OFF) repressible e.g. building tryptophan without trp in environ, negative control only Catabolic pathway: build enzymes to break stuff (=OFF) (repressor=ON) inducible e.g. breaking down lactose with lactate if lactose in environ, negative control no lactose, positive control if no glucose Negative control through repressor (lack/presence controls repressor in both ways) Positive control through activator (lack/presence controls regulatory protein, must bind to activator) Control elements are pieces that control regulation Enhancer is a region made up of distal control elements, genes coding for proteins that control activators, increasing transcription Proximal control elements are controlled by repressors, decreasing transcription","title":"Repressors, Operons, Pathways"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Cytoplasmic%20Determinants/","text":"Hox genes revisisted Define body plan (master regulators, coordinate control etc.) Mutations can result in new body parts Cytoplasmic Determinants Proteins+mRNAs that aren't homogenous in early divisions , first differentiation E.g. bicoid mRNA makes bicoid protein which signifies the end Evo-Devo Evolutionary-Devolopmentary Bio E.g. C. elegans slomo; sperm/egg nucleus combine, and then split, but two resulting cells are asymmetric","title":"Cytoplasmic Determinants"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Cytoplasmic%20Determinants/#hox-genes-revisisted","text":"Define body plan (master regulators, coordinate control etc.) Mutations can result in new body parts","title":"Hox genes revisisted"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Cytoplasmic%20Determinants/#cytoplasmic-determinants","text":"Proteins+mRNAs that aren't homogenous in early divisions , first differentiation E.g. bicoid mRNA makes bicoid protein which signifies the end","title":"Cytoplasmic Determinants"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Cytoplasmic%20Determinants/#evo-devo","text":"Evolutionary-Devolopmentary Bio E.g. C. elegans slomo; sperm/egg nucleus combine, and then split, but two resulting cells are asymmetric","title":"Evo-Devo"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Intro/","text":"time-lapse of a salamander Cell Division Cell Movement (morphogenesis) cells take on \"specific\" fates (differentiation) Inductive Signals: transplanted tissue to surrounding cells Master Regulatory Genes: - Coordinate control - May self-stimulate production - Produce transcription factors for differentiation - Can often change other fully differentiated cells into their own type","title":"Intro"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Intro/#time-lapse-of-a-salamander","text":"Cell Division Cell Movement (morphogenesis) cells take on \"specific\" fates (differentiation) Inductive Signals: transplanted tissue to surrounding cells Master Regulatory Genes: - Coordinate control - May self-stimulate production - Produce transcription factors for differentiation - Can often change other fully differentiated cells into their own type","title":"time-lapse of a salamander"},{"location":"Material%20Knowledge/Sciences/MCB/4%20Developmental%20Biology/Key%20Takeaways/","text":"Inductive signalling (signal gets passed down) Cell Signaling: Ligand binds to receptor (signal reception) Intracellular signal transduction (exponential protein copying) Cell response Combinatorial control (small number of factors controlling large # genes) Complex stripy patterns in (embryo?) HOX genes define \"body segments,\" these r huge master regulatory genes","title":"Key Takeaways"},{"location":"Material%20Knowledge/Sciences/MCB/5%20Genetic%20Engineering/Multiplex%20PCR%20Genotyping/","text":"For a single strand and several similar-length disjoint segments, we can parallelize PCR in the same tube Similarly, if we have DNA change at one point we can have two pairs of primers, one creating a long segment in unedited case, one creating a short segment in edited case (e.g.)","title":"Multiplex PCR Genotyping"},{"location":"Material%20Knowledge/Sciences/MCB/5%20Genetic%20Engineering/Plasmid%20Engineering/","text":"By differential centrifugation and PCR, get a bunch of bacterial plasmids (small DNA circles) and gene fragments coding for insulin Restriction enzyme (EcoR1 from E. coli) will cut DNA at GAATTC (between G and A) on both sides Mix in the desired insulin fragments with overlapping gaps; RNA will bind Add DNA ligase to seal the gaps Restriction Digestion Analysis: If both sides are cut with same restriction enzyme, then test with another restriction enzyme that's in the middle of the segment, to check for orientation Experimental Procedure After plasmids created, heatshock bacteria to absorb DNA; then reproduce, centrifuge, etc. Add an antibiotic resistance gene, and cull those who didn't take it with antibiotics Purify from individual colonies Finally, take out to check done correctly (locating restriction locations; can bar test)","title":"Plasmid Engineering"},{"location":"Material%20Knowledge/Sciences/MCB/5%20Genetic%20Engineering/Plasmid%20Engineering/#experimental-procedure","text":"After plasmids created, heatshock bacteria to absorb DNA; then reproduce, centrifuge, etc. Add an antibiotic resistance gene, and cull those who didn't take it with antibiotics Purify from individual colonies Finally, take out to check done correctly (locating restriction locations; can bar test)","title":"Experimental Procedure"},{"location":"Material%20Knowledge/Sciences/MCB/6%20Cells%20%2B%20Metabolism/ATP%2C%20ADP/","text":"Adenosine diphosphate + inorganic phosphate $\\iff$ Adenosine triphosphate Phosphates are negatively charged and stable, so ATPs hold lots of energy Mitochondria has inner membrane which covered with ATP synthase-s, inside is Matrix, in between is inter-membrane space ATP synthase powered by the wheel being turned by proton pump stuff On each synthase, protons flow from intermembrane $\\to$ egress Glucose + 6O2 $\\to$ 6 Carbon dioxide and 6 water 2H becomes 2H+ and 2e-, following electron transport chain producing ATP NADH$\\to$NAD+ pump protons out of the matrix, then the gradient turns the turbine which creates ATP","title":"ATP, ADP"},{"location":"Material%20Knowledge/Sciences/MCB/6%20Cells%20%2B%20Metabolism/Cell%20organelles/","text":"Nucleus , holds DNA has pores ER - Rough ER, studded with ribosomes, produces proteins meant to exit the cell - Soft ER, produces phospholipids, also processes toxins etc. Golgi repackages proteins, attaches carbohydrate tags to mark proteins Lysosome contains chemicals that can destroy stuff, used for cell lysis and for killing food Peroxisome synthesizes H2O2 Cell Membrane has phospholipid membrane, with regulators in the membrane that determine whether stuff can come in/out Extracellular Matrix is composed of collagen fibers, tying cells together/doing stuff Cell Junctions between adjacent cells, usually plants, allows ions/small molecules to flow through","title":"Cell organelles"},{"location":"Material%20Knowledge/Sciences/MCB/6%20Cells%20%2B%20Metabolism/Photosynthesis/","text":"$$C_6H_{12}O_6\\implies 6CO_2+6H_2O+E$$ $$6CO_2+12H_2O+P \\implies C_6H_{12}O_6 + 6H_2O$$ $O_2$ becomes reduced (gain electron) to become $H_2O,$ and vice versa (oxidized to lose) Light hits chlorophyll, excited electron in chlorophyll that gets transferred through redox After PSII, cytochrome complex splits water and creates proton gradient. NADPH is final electron acceptor.","title":"Photosynthesis"},{"location":"Material%20Knowledge/Sciences/MCB/Genetics/Cell%20Cycle/","text":"![[Pasted image 20240401143011.png]] ![[Pasted image 20240401143428.png]] ![[Pasted image 20240401150537.png]] ![[Pasted image 20240403144513.png]] ![[Pasted image 20240403144601.png]]","title":"Cell Cycle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Important%20Equations/","text":"[!important] GOOD TO REMEMBER $$E^2-p^2c^2=m^2c^4,\\qquad E=\\frac{p^2}{2m}.$$ $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda,$$ $$E_0=mc^2=h\\nu=h\\frac c\\lambda=\\hbar\\omega,\\text{ (Compton)}$$ $$p=\\hbar k,\\qquad\\lambda=\\frac{2\\pi}k=\\frac hp.\\text{ (de Broglie)}$$ $$\\Psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ $$\\hat p=\\frac{\\hbar}{i}\\frac{\\partial}{\\partial x},\\qquad \\hat E=\\frac{\\hat p^2}{2m}=i\\hbar\\frac{\\partial}{\\partial t}.$$ $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+V\\right)\\Psi.$$ $$J=\\frac{\\hbar}m\\text{Im}(\\Psi^*\\nabla\\Psi),\\qquad \\frac{\\partial \\rho}{\\partial t}+\\nabla\\cdot J=0.$$ $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ $$(\\psi,\\varphi)=\\int \\psi\\bar\\varphi,\\qquad (\\psi,\\hat A\\varphi)=(\\hat A^\\dagger\\psi,\\varphi).$$ $$ \\begin{align } f(x)=\\frac1{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{ikx}\\tilde f(k)\\,dk,\\qquad\\tilde f(k)=\\frac1{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{ikx}f(x)\\,dk \\end{align } $$ $$\\delta(x)=0 \\iff x\\neq 0,\\qquad\\int\\delta \\,dx=1$$ $$ \\begin{align } \\langle F\\rangle=\\mathbb E(F)=\\int \\Psi^2(x)F(x)\\,dx\\ \\Delta x=\\text{Var}(x)=\\sqrt{\\langle x^2\\rangle-\\langle x\\rangle^2}\\ [\\hat A,\\hat B]=\\hat A\\hat B-\\hat B\\hat A\\ \\hat p=-i\\hbar\\frac{\\partial}{\\partial x}\\ \\hat E \\,\\Psi(x,t)=i\\hbar\\frac\\partial{\\partial t}\\Psi(x,t)=\\frac1{2m}\\hat p^2+V(x)\\ (f|g) = \\int f^ g\\,dx \\end{align*} $$","title":"Important Equations"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04%20Cleaned/","text":"![[8.04.A]] ![[8.04.B]] ![[8.04.C]] ![[8.04.D]]","title":"8.04 Cleaned"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/","text":"Fun with States States are $\\mathbb C$-vectors. Mach-Zehdner interferometer ![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood. E-V bombs Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation. Schrodinger's Photon Energy [!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$. Matter wave and Schrodinger's We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$. Manipulating States Interpretation of States $\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$. Probability Flow $J$ Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point. Hermitian Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$ Fourier In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum.","title":"8.04.A"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#fun-with-states","text":"States are $\\mathbb C$-vectors.","title":"Fun with States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#mach-zehdner-interferometer","text":"![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood.","title":"Mach-Zehdner interferometer"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#e-v-bombs","text":"Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation.","title":"E-V bombs"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#schrodingers","text":"","title":"Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#photon-energy","text":"[!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$.","title":"Photon Energy"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#matter-wave-and-schrodingers","text":"We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$.","title":"Matter wave and Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#manipulating-states","text":"","title":"Manipulating States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#interpretation-of-states","text":"$\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$.","title":"Interpretation of States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#probability-flow-j","text":"Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point.","title":"Probability Flow $J$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#hermitian","text":"Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$","title":"Hermitian"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#fourier","text":"In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum.","title":"Fourier"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/","text":"Inner Product Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$ Expectation ROC $$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$ Problem Solving [!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$ 1D Problem Solving Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$ Free Particle on Circle On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$ Square Well(s) Now we have domain $\\mathbb{R}$ and experiment with $V(x)$. Infinite case $$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$. Finite case $$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states. Delta(s) Simple case $$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$. Simple Harmonic Oscillator Solving Setup $$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree. Factorization Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$","title":"8.04.B"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#inner-product","text":"Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$","title":"Inner Product"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#expectation-roc","text":"$$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$","title":"Expectation ROC"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#problem-solving","text":"[!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$","title":"Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#1d-problem-solving","text":"Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$","title":"1D Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#free-particle-on-circle","text":"On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$","title":"Free Particle on Circle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#square-wells","text":"Now we have domain $\\mathbb{R}$ and experiment with $V(x)$.","title":"Square Well(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#infinite-case","text":"$$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$.","title":"Infinite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#finite-case","text":"$$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states.","title":"Finite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#deltas","text":"","title":"Delta(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#simple-case","text":"$$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$.","title":"Simple case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#simple-harmonic-oscillator","text":"","title":"Simple Harmonic Oscillator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#solving-setup","text":"$$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree.","title":"Solving Setup"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#factorization","text":"Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$","title":"Factorization"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/","text":"Unidirectional transmission/scattering Set up exponential ansatz for each domain and solve BCs. Example: Step Potential Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying). Example: Finite square well, resonance Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well. Reflective half-domain scattering Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$. Wavepacket $\\Delta t$ Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$ Levinson's Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states. Some weird experimentation/construction We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense. Weird magic If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance.","title":"8.04.C"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#unidirectional-transmissionscattering","text":"Set up exponential ansatz for each domain and solve BCs.","title":"Unidirectional transmission/scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#example-step-potential","text":"Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying).","title":"Example: Step Potential"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#example-finite-square-well-resonance","text":"Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well.","title":"Example: Finite square well, resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#reflective-half-domain-scattering","text":"Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$.","title":"Reflective half-domain scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#wavepacket-delta-t","text":"Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$","title":"Wavepacket $\\Delta t$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#levinsons","text":"Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states.","title":"Levinson's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#some-weird-experimentationconstruction","text":"We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense.","title":"Some weird experimentation/construction"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#weird-magic","text":"If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance.","title":"Weird magic"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/","text":"Hydrogen atom Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$ Angular momentum Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist. Radial ansatz Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"8.04.D"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#hydrogen-atom","text":"Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$","title":"Hydrogen atom"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#angular-momentum","text":"Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist.","title":"Angular momentum"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#radial-ansatz","text":"Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"Radial ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04%20Final%20Notes/","text":"![[Important Equations]] ![[8.04.1 Superposition, norms, M-F interferometer, E-V bombs]] ![[8.04.2 Dualities of Light and Matter]] ![[8.04.3 Matter Waves]] ![[8.04.4 Schrodinger's and Wavefunctions]] ![[8.04.5 Wavefunction as Probability]] ![[8.04.6 Uncertainty and Fourier Inverse]] ![[8.04.7 Momentum space, Expectation, Uncertainty 2]] ![[8.04.8 Eigenstates, Hermitian, Observables, Measurement]] ![[8.04.9 Time Independence, Free Particle on Circle]] ![[8.04.10 Square Well]] ![[8.04.11 1D Potentials]] ![[8.04.12 Dirac Delta, Node Theorem, SHO]] ![[8.04.13 Factorizing SHO]] ![[8.04.14 Step Potential]] ![[8.04.15 Resonance, Half-Domain Scattering]] ![[8.04.16 Levinson's, Resonance, Complex k-plane]] ![[8.04.17 Angular Momentum, Radial Ansatz]] ![[8.04.18 Solving Hydrogen]]","title":"8.04 Final Notes"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.1%20Superposition%2C%20norms%2C%20M-F%20interferometer%2C%20E-V%20bombs/","text":"While physical theories have not been made compatible, quantum mechanics is a framework for operating on other fields, such as classical mechanics and EM. One nice thing is linearity; the time evolution operator $\\hat H$ in Schrodinger's is assumed to be linear, so solutions are all linear. This operator is somewhat arbitrary, by the way. States superimpose linearly, we can represent them as vectors in some abstract $\\mathbb C$-valued vector space, and if outcomes are finite we can represent as a unit vector in $\\CC^k$. Then the probability of any event is the norm of its coefficient. Because of this, states are invariant on constant scaling . Also recall entanglement; we can use $\\otimes$ to denote one outcome with several measurements; if two measurements are not independent in some state, they're entangled. Let's examine one case study: the Mach-Zehnder interferometer . ![[Pasted image 20230907105403.png]] Represent photon states with unit $\\CC^2$ column vectors. The splitters are probabilistic, they split one way vs the other with amplitudes $s,t$ satisfying $s^2+t^2=1.$ But that's only from one direction; it can also split differently depending on input from the other direction. Then each BS is a $2\\times 2$ complex matrix. It must also be an orthogonal matrix. Considering two \"correct\" orthogonal matrices with $\\frac1{\\sqrt 2},$ their product is actually $\\begin{bmatrix} 0&1\\-1&0\\end{bmatrix}.$ Then in this above setup where the initial state is all from bottom, it will end up all going to detector 0, the top. We can use this to get some \"uncanny measurement\" in an EV bomb. If we stick a bomb in the bottom path between the two splitters, and the detector does work, then it is $50\\%$ a photon ends at D1, and independently $50\\%$ a photon goes through the top path. Then given a live bomb, with $25\\%$ probability we discover that it is live without setting it off. Final note: phase shifters work by multiplying the probability amplitude by $e^{i\\delta}.$ No clue why this works. $\\require{mhchem}\\newcommand{\\CC}{\\mathbb C}$-latex","title":"8.04.1 Superposition, norms, M F interferometer, E V bombs"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/","text":"Def'n, Motivation First we'll treat unrigorously, with a potential that is a well with infinite walls. Then we'll take a limit. Inf Square Well So we have $V(x)=0$ if $x\\in(0,a),$ $V(x)=\\infty$ else. Then we must have $\\psi(x)=0$ for $x\\in(-\\infty,0]\\cup [a,\\infty)$ by continuity of $\\psi$ and energy conservation. Then in $[0,a]$ there is no potential and we just get 1D Schrodinger free equation $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi.$$ Then we do the usual, set $k^2=\\frac{2mE}{\\hbar^2},$ then $\\psi=e^{\\pm ikx}.$ After imposing boundary conditions we get $\\psi_n(x)=\\sin(\\frac{n\\pi x}{a}).$ The normalization is $\\sqrt{\\frac 2a}.$ In other words, we can take any wavefunction on this domain and Fourier expand to obtain distribution among the quantized energy distribution. Finite Square Well Let's reframe as $V(x)=-V_0$ if $|x|\\leq a,$ and $V(x)=0$ otherwise. Note that the energy eigenvalue $-V_0<E<0,$ otherwise we get a non-normalizable plane wave for $|x|>a,$ and we also want positive KE in the well. (With $E<0$ we get a decaying exponential, below) Then within the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2m(V_0-|E|)}{\\hbar^2}\\psi.$$ Outside the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi=\\frac{2m|E|}{\\hbar^2}\\psi.$$ Then with $$k=\\sqrt{\\frac{2m}{\\hbar^2}(V_0-|E|)},\\qquad\\kappa=\\sqrt{\\frac{2m|E|}{\\hbar^2}}.$$ we get $\\psi(x)=Ae^{-\\kappa|x|}$ outside the well, $e^{\\pm ikx}$ inside the well. Taking the even/odd solutions for inside yields $\\cos kx$ or $\\sin kx$. Introduce the nondimensionalized $\\eta=ka,\\xi=\\kappa a,$ then in fact $$\\eta^2+\\xi^2=\\frac{2mV_0a^2}{\\hbar^2}.$$ Then imposing $\\psi,\\psi'$ continuity, for the even case at $x>0$ we get $$\\cos \\eta=Ae^{-\\xi},-k\\sin\\eta=-\\kappa Ae^{-\\xi}\\implies k\\tan\\eta=\\kappa\\iff \\eta\\tan\\eta=\\xi.$$ For the odd case we similarly get $-\\eta\\cot\\eta=\\xi.$ Considering the constraint above, there are $O\\left(\\sqrt{\\frac{2mV_0a^2}{\\hbar^2}}\\right)$ many stationary states, one for each period of $\\eta\\tan\\eta$. Now it's clear that in the limit $k$ is the same set of values as before, and $\\kappa$ goes to infinity. General qualitative properties include - Higher bound energy is faster oscillation in well, slower decay outside well","title":"8.04.10 Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#defn-motivation","text":"First we'll treat unrigorously, with a potential that is a well with infinite walls. Then we'll take a limit.","title":"Def'n, Motivation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#inf-square-well","text":"So we have $V(x)=0$ if $x\\in(0,a),$ $V(x)=\\infty$ else. Then we must have $\\psi(x)=0$ for $x\\in(-\\infty,0]\\cup [a,\\infty)$ by continuity of $\\psi$ and energy conservation. Then in $[0,a]$ there is no potential and we just get 1D Schrodinger free equation $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi.$$ Then we do the usual, set $k^2=\\frac{2mE}{\\hbar^2},$ then $\\psi=e^{\\pm ikx}.$ After imposing boundary conditions we get $\\psi_n(x)=\\sin(\\frac{n\\pi x}{a}).$ The normalization is $\\sqrt{\\frac 2a}.$ In other words, we can take any wavefunction on this domain and Fourier expand to obtain distribution among the quantized energy distribution.","title":"Inf Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#finite-square-well","text":"Let's reframe as $V(x)=-V_0$ if $|x|\\leq a,$ and $V(x)=0$ otherwise. Note that the energy eigenvalue $-V_0<E<0,$ otherwise we get a non-normalizable plane wave for $|x|>a,$ and we also want positive KE in the well. (With $E<0$ we get a decaying exponential, below) Then within the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2m(V_0-|E|)}{\\hbar^2}\\psi.$$ Outside the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi=\\frac{2m|E|}{\\hbar^2}\\psi.$$ Then with $$k=\\sqrt{\\frac{2m}{\\hbar^2}(V_0-|E|)},\\qquad\\kappa=\\sqrt{\\frac{2m|E|}{\\hbar^2}}.$$ we get $\\psi(x)=Ae^{-\\kappa|x|}$ outside the well, $e^{\\pm ikx}$ inside the well. Taking the even/odd solutions for inside yields $\\cos kx$ or $\\sin kx$. Introduce the nondimensionalized $\\eta=ka,\\xi=\\kappa a,$ then in fact $$\\eta^2+\\xi^2=\\frac{2mV_0a^2}{\\hbar^2}.$$ Then imposing $\\psi,\\psi'$ continuity, for the even case at $x>0$ we get $$\\cos \\eta=Ae^{-\\xi},-k\\sin\\eta=-\\kappa Ae^{-\\xi}\\implies k\\tan\\eta=\\kappa\\iff \\eta\\tan\\eta=\\xi.$$ For the odd case we similarly get $-\\eta\\cot\\eta=\\xi.$ Considering the constraint above, there are $O\\left(\\sqrt{\\frac{2mV_0a^2}{\\hbar^2}}\\right)$ many stationary states, one for each period of $\\eta\\tan\\eta$. Now it's clear that in the limit $k$ is the same set of values as before, and $\\kappa$ goes to infinity. General qualitative properties include - Higher bound energy is faster oscillation in well, slower decay outside well","title":"Finite Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/","text":"Reminder that we're only working with stationary states, i.e. eigenstates of the Hamiltonian. Thus there's no time derivative. Our equation is $$\\psi''+\\frac{2m}{\\hbar^2}(E-V(x))\\psi=0.$$ Properties No degenerate bound states All states are full-real These are a result of being able to solve the above diffEq uniquely given $\\lim_{x\\to\\infty}\\psi(x)=0.$ Energy eigenvalue $E\\geq\\min_x V(x).$ If $E-V(x)<0$ everywhere then $\\psi$ will either be always positive + concave up or always neg + concave down. Just find somewhere satisfying $\\psi'(x)=0.$ If potential is even, we can even specify - Eigenstates must be even or odd . Slowly varying potentials Letting $E-V=K$ for KE, we get $\\psi''=-\\frac{2m}{\\hbar^2}K=-\\frac{p^2}{\\hbar^2}\\psi$ by some degen semi-classical allusion. Then $\\psi\\sim\\cos\\left(\\frac{2\\pi}{\\lambda}x\\right),$ the deBroglie wavelength $\\lambda=\\frac hp$ of $\\psi$. Superposition of momentum $\\pm p$. We make the assumption that $\\lambda(x)\\frac{dV}{dx}\\ll V(x).$ Consider classically a ball sliding around on a roller coaster; KE at a point is $E-V$ at that point; with larger KE it's moving faster, and less likely to be found there. Indeed, we find in semiclassical approximation that $$\\text{Amp}(\\psi(x))\\sim\\sqrt{\\lambda(x)}\\sim\\frac1{\\sqrt{p(x)}}.$$ The semiclassical approximation assumes $\\psi$ oscillates a lot before $V$ changes, so we can locally average out the oscillation in $\\psi$ (that's quantization for ya). You can sketch wavefunctions by analyzing sign of $\\psi''$ and intercepts &c. Shooting Method In this example, used for even/odd potentials. Our boundary condition is $\\Psi(\\pm\\infty)=0.$ Basically, binary search on possible energies. Begin with $\\Psi(0)=1,\\Psi'(0)=0$ for even states and $\\Psi(0)=0,\\Psi'(0)=1$ for odd states.","title":"8.04.11 1D Potentials"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#properties","text":"No degenerate bound states All states are full-real These are a result of being able to solve the above diffEq uniquely given $\\lim_{x\\to\\infty}\\psi(x)=0.$ Energy eigenvalue $E\\geq\\min_x V(x).$ If $E-V(x)<0$ everywhere then $\\psi$ will either be always positive + concave up or always neg + concave down. Just find somewhere satisfying $\\psi'(x)=0.$ If potential is even, we can even specify - Eigenstates must be even or odd .","title":"Properties"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#slowly-varying-potentials","text":"Letting $E-V=K$ for KE, we get $\\psi''=-\\frac{2m}{\\hbar^2}K=-\\frac{p^2}{\\hbar^2}\\psi$ by some degen semi-classical allusion. Then $\\psi\\sim\\cos\\left(\\frac{2\\pi}{\\lambda}x\\right),$ the deBroglie wavelength $\\lambda=\\frac hp$ of $\\psi$. Superposition of momentum $\\pm p$. We make the assumption that $\\lambda(x)\\frac{dV}{dx}\\ll V(x).$ Consider classically a ball sliding around on a roller coaster; KE at a point is $E-V$ at that point; with larger KE it's moving faster, and less likely to be found there. Indeed, we find in semiclassical approximation that $$\\text{Amp}(\\psi(x))\\sim\\sqrt{\\lambda(x)}\\sim\\frac1{\\sqrt{p(x)}}.$$ The semiclassical approximation assumes $\\psi$ oscillates a lot before $V$ changes, so we can locally average out the oscillation in $\\psi$ (that's quantization for ya). You can sketch wavefunctions by analyzing sign of $\\psi''$ and intercepts &c.","title":"Slowly varying potentials"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#shooting-method","text":"In this example, used for even/odd potentials. Our boundary condition is $\\Psi(\\pm\\infty)=0.$ Basically, binary search on possible energies. Begin with $\\Psi(0)=1,\\Psi'(0)=0$ for even states and $\\Psi(0)=0,\\Psi'(0)=1$ for odd states.","title":"Shooting Method"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.12%20Dirac%20Delta%2C%20Node%20Theorem%2C%20SHO/","text":"More analysis of solving 1D potentials. Dirac Delta is a \"function\" (actually it's a functional) that satisfies ... Node Theorem: the n th lowest-energy bound state has $n-1$ nodes, for $\\lim_{x\\to\\infty}V=\\infty$. Proof: Take an infinitesimally thin \"screen\" outside of which everything is 0. Then we get $n-1$ nodes for the $n$th state, because that's just square well, but infinitesimal. Then as we stretch the screen out, the $n$th state can never gain more nodes. At the endpoints of the screen we have $\\psi=0$ and so the sign of $\\psi'$ at those endpoints can't change. Also we can't have a \"double node\" getting created; if the wavefunction is tangent to the $x$-axis then $\\psi=\\psi'=0\\implies\\psi\\equiv 0.$ Done! Harmonic Oscillator: $V(x)=\\frac 12m\\omega^2 x^2.$ To solve this, we nondimensionalize the usual eigenstate equation to $$\\frac{d^2\\varphi}{du^2}=(u^2-\\mathcal E)\\varphi.$$ Substituting $\\varphi=e^{\\frac{-x^2}2}h$ is completely motivated, and after doing that we can solve for $h$ as some finite-degree polynomial. Tada!","title":"8.04.12 Dirac Delta, Node Theorem, SHO"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.13%20Factorizing%20SHO/","text":"Recall the Hermitian conjugate of an operator $A$ satisfies $$(\\varphi,A\\psi)=(A^\\dagger\\varphi,\\psi).$$ A Hermitian operator is one whose conjugate is itself. Given some potential, if we can factor the resulting Hamiltonian $\\hat H=\\hat A^\\dagger\\hat A+E_0\\,\\bf 1,$ where $\\bf 1$ is the identity operator, then we get that $\\hat H\\varphi=E\\varphi\\implies E\\geq E_0.$ Usually $\\hat A$ will be a first-order differential operator, and the ground energy state will satisfy $\\hat A\\varphi_0=0.$ We call the operator $\\hat N=\\hat A^\\dagger \\hat A$ the \"number operator.\" Simple Harmonic Oscillator Take $V=\\frac12 m\\omega x^2$ as before, then $$\\hat H=\\frac12m\\omega^2(\\hat x^2+\\frac{\\hat p^2}{m^2\\omega^2}).$$ Try $V,V^\\dagger=\\hat x\\pm \\frac{i\\hat p}{m\\omega}.$ Then notice $$[V,V^\\dagger]=-\\frac{2i}{m\\omega}[\\hat x,\\hat p]=\\frac{2\\hbar}{m\\omega}\\,\\bf1.$$ Take the dimensionless $a=V\\sqrt{\\frac{m\\omega}{2\\hbar}}$ to get [!important] Dimensionless Factorization $$\\hat H=\\hbar\\omega(a^\\dagger a+\\frac12),[a,a^\\dagger]=1.$$ Explicitly, $\\hat a =\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat x+\\frac{i\\hat p}{\\omega}\\right),$ and $\\hat a^\\dagger$ is with $-$. Equivalently (and usefully) $\\hat x=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger),$ $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Now the ground energy state $\\varphi_0$ satisfies $\\hat a\\varphi_0=0,$ since $$E=[\\varphi,E\\varphi]=[\\varphi,\\hat H\\varphi]=E_0+[a\\varphi,a\\varphi]\\geq E_0.$$ That's why we call $\\hat a$ the annihilation operator. Now note that $$[\\hat N,(a^\\dagger)^k]=a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^ka^\\dagger a.$$ Then $-a^\\dagger a=-aa^\\dagger+1,$ so we get $$a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^k a a^\\dagger+(a^\\dagger)^k=[\\hat N,(a^\\dagger)^{k-1}]a^\\dagger+(a^\\dagger)^k=k(a^\\dagger)^k.$$ Thus, [!important] Number Operator $$[\\hat N,(a^\\dagger)^k]=k(a^\\dagger)^k,[\\hat N,a^k]=-ka^k.$$ Now if we take $\\varphi_1=a^\\dagger\\varphi_0,$ and generally, $\\varphi_i=(a^\\dagger)^i\\varphi_0,$ $$\\hat N\\varphi_{i}=[\\hat N,(a^\\dagger)^i]\\varphi_0=i(a^\\dagger)^i\\varphi_0.$$ Thus $\\varphi_i$ has energy $E=\\hbar\\omega(i+\\frac12).$ Thus $a^\\dagger$ is called the creation operator. Finally notice that $$a\\varphi_n=[a,(a^\\dagger)^n]\\varphi_0=n(a^\\dagger)^{n-1}\\varphi_0=n\\varphi_{n-1}.$$ Thus [!important] Creation and Destruction $\\hat a\\varphi_{i+1}=\\sqrt{i+1}\\cdot\\varphi_i$ and $\\hat a^\\dagger\\varphi_i=\\sqrt{n+1}\\varphi_{i+1},$ not considering scaling. $a\\varphi_0=\\bf 0.$","title":"8.04.13 Factorizing SHO"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.13%20Factorizing%20SHO/#simple-harmonic-oscillator","text":"Take $V=\\frac12 m\\omega x^2$ as before, then $$\\hat H=\\frac12m\\omega^2(\\hat x^2+\\frac{\\hat p^2}{m^2\\omega^2}).$$ Try $V,V^\\dagger=\\hat x\\pm \\frac{i\\hat p}{m\\omega}.$ Then notice $$[V,V^\\dagger]=-\\frac{2i}{m\\omega}[\\hat x,\\hat p]=\\frac{2\\hbar}{m\\omega}\\,\\bf1.$$ Take the dimensionless $a=V\\sqrt{\\frac{m\\omega}{2\\hbar}}$ to get [!important] Dimensionless Factorization $$\\hat H=\\hbar\\omega(a^\\dagger a+\\frac12),[a,a^\\dagger]=1.$$ Explicitly, $\\hat a =\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat x+\\frac{i\\hat p}{\\omega}\\right),$ and $\\hat a^\\dagger$ is with $-$. Equivalently (and usefully) $\\hat x=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger),$ $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Now the ground energy state $\\varphi_0$ satisfies $\\hat a\\varphi_0=0,$ since $$E=[\\varphi,E\\varphi]=[\\varphi,\\hat H\\varphi]=E_0+[a\\varphi,a\\varphi]\\geq E_0.$$ That's why we call $\\hat a$ the annihilation operator. Now note that $$[\\hat N,(a^\\dagger)^k]=a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^ka^\\dagger a.$$ Then $-a^\\dagger a=-aa^\\dagger+1,$ so we get $$a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^k a a^\\dagger+(a^\\dagger)^k=[\\hat N,(a^\\dagger)^{k-1}]a^\\dagger+(a^\\dagger)^k=k(a^\\dagger)^k.$$ Thus, [!important] Number Operator $$[\\hat N,(a^\\dagger)^k]=k(a^\\dagger)^k,[\\hat N,a^k]=-ka^k.$$ Now if we take $\\varphi_1=a^\\dagger\\varphi_0,$ and generally, $\\varphi_i=(a^\\dagger)^i\\varphi_0,$ $$\\hat N\\varphi_{i}=[\\hat N,(a^\\dagger)^i]\\varphi_0=i(a^\\dagger)^i\\varphi_0.$$ Thus $\\varphi_i$ has energy $E=\\hbar\\omega(i+\\frac12).$ Thus $a^\\dagger$ is called the creation operator. Finally notice that $$a\\varphi_n=[a,(a^\\dagger)^n]\\varphi_0=n(a^\\dagger)^{n-1}\\varphi_0=n\\varphi_{n-1}.$$ Thus [!important] Creation and Destruction $\\hat a\\varphi_{i+1}=\\sqrt{i+1}\\cdot\\varphi_i$ and $\\hat a^\\dagger\\varphi_i=\\sqrt{n+1}\\varphi_{i+1},$ not considering scaling. $a\\varphi_0=\\bf 0.$","title":"Simple Harmonic Oscillator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.14%20Step%20Potential/","text":"The step potential is $0$ for $x<0,$ $V_0$ for $x>0$. Take a functional or whatever if you prefer. Once again we've got piecewise constant potentials, which is just linear combinations of exponentials. Specifically, with $k^2=\\frac{2mE}{\\hbar^2}, \\bar k^2=\\frac{2m(E-V_0)}{\\hbar^2},$ we have $\\Psi=Ae^{ikx}+Be^{-ikx}$ for $x<0,$ $C e^{i\\bar kx}$ for $x>0.$ Once again analyzing continuity of $\\psi$ and $\\psi'$ at $x=0$ we can solve for $A,B,C$ in terms of $k,\\bar k.$ With $E<V_0$ we get imaginary $\\bar k$ resulting in a decaying exponential in forbidden region, and some ghastly phase-shifted sine for $x<0.$ The time factor is $e^{iEt/\\hbar}.$ Then with the energy eigenstate we can compose these states into a normalizable wavepacket. The three terms corresponding to $A,B,C$ are like the incident, reflected, and transmitted wave. Using the stationary phase paradigm we can determine the speed of the wavepacket components. For $E>V_0$ the incoming and reflected both have speed $\\frac{\\hbar k_0}{m}$ and reflected has speed $\\frac{\\hbar \\bar k_0}m.$ For $E<V_0$ the observed phase shift in the sine because a time delay in the reflected component. Finally, a funny heuristic argument: Suppose we saw a particle in the forbidden region. Then because of the decaying exponential it's about $\\frac1k,$ where $\\kappa^2=\\frac{2m(V_0-E)}{\\hbar^2}$, of the way in. The position uncertainty must be at most $\\kappa,$ so $p\\geq\\frac{\\hbar}{\\nabla x}=\\hbar\\kappa.$ Then this new momentum contributes to an amount of energy $\\frac{p^2}{2m}=\\frac{\\hbar^2\\kappa^2}{2m}=V_0-E,$ so the final energy is in fact at least $V_0$ probably.","title":"8.04.14 Step Potential"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/","text":"Problem 1 Explore the double-well potential $V=-g\\delta(x-a)-g\\delta(x+a).$ Here $\\delta$ is Dirac delta, $a,g$ are positive constants. Even bound state is $\\cosh kx$ in the middle, $Ae^{-kx}$ on the right side. Then $A=1+e^{2ak}.$ Because we have Dirac delta justified in not matching derivatives, but $-\\frac{\\hbar^2}{2m}\\Delta \\psi'|_a=g\\psi(a).$ Then $\\psi'_L=k\\sinh ak, \\psi'_R=-kAe^{-ak}=-k\\cosh ak,$ so $\\Delta\\psi'|_a=-2ke^{ak}.$ Then $2ke^{ak}\\cdot\\frac{\\hbar^2}{2m}=g\\cosh ak,$ i.e. $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1+e^{-2ak}).$ Odd bound state is $\\sinh kx$ in the middle. Similar computation yields $\\psi'_L=k\\cosh ak,$ $\\psi'_R=-k\\sinh ak,$ so $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1-e^{-2ak}).$ Recall $k^2=\\frac{2mE}{\\hbar^2}.$ In the limit of large $a,$ the LHS goes to 0. We're solving $C=\\frac{1\\pm e^{-2z}}{z}$ for $z$ as $C\\to 0.$ Then $z\\approx \\frac1C,$ to one extra degree $z_1\\approx\\frac{1+e^{-2/C}}C,z_2\\approx\\frac{1-e^{-2/C}}C.$ Then $k_1-k_2=\\frac{2e^{-2/C}}{aC}=\\frac{2e^{-2amg/\\hbar^2}}{\\hbar^2/mg}=\\frac{2mge^{-2amg/\\hbar^2}}{\\hbar^2}.$ Problem 2 Skipped. Problem 3 Calculate probabilities that quantum SHO has position greater than amplitude in classical SHO for $\\mathcal E=1,3,5.$ We can work entirely in non-dimensional coordinates. We get the polynomials $1,x,x^2-\\frac12$ as solutions to $u''-2xu'+(\\mathcal E-1)u=0.$ The classical energy potential $V=x^2.$ Then, non-normalized we get - $\\psi=e^{-x^2/2}$ corresponding to $L=1.$ Get $\\approx 0.078649.$ - $\\psi=xe^{-x^2/2}$ corresponding to $L=\\sqrt 3.$ Get $\\approx 0.0558051.$ - $\\psi=(2x^2-1)e^{-x^2/2}$ corresponding to $L=\\sqrt 5.$ Get $\\approx 0.04753471774.$ Problem 4 Compute: (a) Expectation of $x^4$ on $n$th SHO eigenstate (b) $\\Delta x,\\Delta p,\\Delta x\\Delta p$ on $n$th SHO eigenstate Finally, (c) Check that $$e^{-s^2+2s\\xi}=\\sum_{n=0}H_n(\\xi)\\frac{s^n}{n!}$$ satisfies $H_n''-2\\xi H_n'+2nH_n=0.$ Recall that $\\hat x=(\\hat a+\\hat a^\\dagger)\\sqrt{\\frac{\\hbar}{2m\\omega}}.$ Then $$ \\begin{align } \\langle \\psi_n,\\hat x^4\\psi_n\\rangle &=\\langle \\hat x^2\\psi_n,\\hat x^2\\psi_n\\rangle\\ &=\\frac{\\hbar^2}{4m^2\\omega^2}\\langle(\\hat a^2+\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a+\\hat a^\\dagger\\hat a^\\dagger)\\psi_n,\\cdot \\rangle. \\end{align } $$ Note that $\\hat a$ is destruction, $\\hat a^\\dagger$ is construction so we can split by \"degree\". Reference L14-15, p7. $$\\langle\\hat a^2\\psi_n,\\cdot\\rangle=\\langle \\sqrt{n(n-1)}\\phi_{n-2},\\cdot\\rangle=n(n-1).$$ Similarly, $\\langle (\\hat a^\\dagger)^2\\psi_n,\\cdot\\rangle=(n+1)(n+2).$ Finally $(\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a)\\psi_n=(n+n+1)\\psi_n,$ so our final answer is $n(n-1)+(n+1)(n+2)+(2n+1)^2=6n^2+6n+3.$ Adding our units back in, we get an answer of $$(6n^2+6n+3)\\frac{\\hbar^2}{4m^2\\omega^2}.$$ (b) is similar. $(\\Delta x)^2=\\frac{\\hbar}{2m\\omega}(2n+1).$ Now $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Then clearly $\\langle \\psi_n,\\hat p\\psi_n=0$ (also true from potential symmetry). Finally $$ \\begin{align } \\langle \\psi_n,\\hat p^2\\psi_n\\rangle &=-\\frac{m\\omega\\hbar}2\\langle\\psi_n,C_1\\psi_{n+2}+C_2\\psi_{n-2}-(2n+1)\\psi_n\\rangle\\ &=(2n+1)\\frac{m\\omega\\hbar}2. \\end{align } $$ Thus $\\Delta x\\Delta p=(n+\\frac12)\\hbar.$ (c) Taking the $s$-derivative of the whole generating function yields $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=1}H_n \\frac{s^{n-1}}{(n-1)!}=\\sum_{n=0}H_{n+1}\\frac{s^n}{n!}.$$ Then $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=0}2H_{n}\\frac{s^{n+1}}{n!}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=1}2H_{n-1}n\\frac{s^n}{n!}.$$ Thus $$H_{n+1}=2\\xi H_n-2nH_{n-1}.$$ Taking the $\\xi$-derivative yields $$\\sum_{n=1}2nH_{n-1}\\frac{s^n}{n!}=2se^{-s^2+2s\\xi}=\\sum_{n=0}H_n'\\frac{s^n}{n!}.$$ Thus $2nH_{n-1}=H_n',$ so $$H_n''-2\\xi H_n'+2nH_n=4n(n-1)H_{n-2}-4\\xi nH_{n-1}+2nH_n=2n(H_n-2\\xi H_{n-1}+2(n-1)H_{n-2})=0.$$ Yay! Problem 5 Harmonic oscillator, except $V(x)=\\infty$ for $x<0$. What are the possible energy levels? Basically we need the polynomial solution to $H''-2xH+2nH=0$ to satisfy $H(0)=0.$ We established $H_{n+1}=2xH_n-2nH_{n-1},$ so $H_{n+1}(0)=-2nH_{n-1}(0).$ Since $H_0=1, H_1=x,$ we get $n$ must be odd so the possible energy levels are $\\mathcal E=4n+3.$ Re-dimensionalizing yields $E=(4n+3)\\hbar\\omega.$ Problem 6 Given $V=m\\omega^2x^2,$ $\\Psi(x,0)=\\frac1{\\sqrt2}\\left(\\varphi_0(x)+\\varphi_1(x)\\right)$: - Find $\\Psi(x,t),|\\Psi(x,t)|^2$ - Find $\\langle x\\rangle(t).$ - Find $\\langle p\\rangle(t).$ - Show that for any such $\\Psi,$ $|\\Psi(x,t)|^2=|\\Psi(x,t+T)|^2$ where $T=\\frac{2\\pi}\\omega.$ $E(\\varphi_0)=\\hbar\\omega, E(\\varphi_1)=3\\hbar\\omega,$ so $$\\Psi(x,t)=\\frac1{\\sqrt2}(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1).$$ Then $$\\Psi\\Psi^*=\\frac12\\left(\\varphi_0^2+\\varphi_1^2+2\\varphi_0\\varphi_1\\cos(2t\\omega)\\right).$$ Then $\\langle x\\rangle=\\cos(2t\\omega)\\int x\\varphi_0\\varphi_1.$ Turns out that $\\hat x\\varphi_0=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger)\\varphi_0=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\varphi_1.$ As a result $\\langle x\\rangle=\\cos(2t\\omega)\\sqrt{\\frac\\hbar{2m\\omega}}.$ Next, $$ \\begin{align } \\langle \\hat p\\rangle &=\\int \\Psi\\hat p\\Psi\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a)(e^{it\\omega}\\varphi_0+e^{3it\\omega}\\varphi_1)\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(-e^{3it\\omega}\\varphi_0+e^{it\\omega}\\varphi_1)\\ &=\\frac i2\\sqrt{\\frac{m\\omega\\hbar}2}\\int -e^{2it\\omega}\\varphi_0^2+e^{-2it\\omega}\\varphi_1^2\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\frac1{2i}(e^{2it\\omega}-e^{-2it\\omega})\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\sin(2t\\omega). \\end{align } $$ Finally, $E=N\\hbar\\omega,$ so all $e^{itE/\\hbar}$ terms are periodic in $t$ every $\\frac{2\\pi}{E/\\hbar}=\\frac{2\\pi}{N\\omega}.$ Thus if we increment $t$ by $\\frac{2\\pi}\\omega$ the value of $\\Psi$ is unchanged.","title":"8.04.15 Pset 7"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-1","text":"Explore the double-well potential $V=-g\\delta(x-a)-g\\delta(x+a).$ Here $\\delta$ is Dirac delta, $a,g$ are positive constants. Even bound state is $\\cosh kx$ in the middle, $Ae^{-kx}$ on the right side. Then $A=1+e^{2ak}.$ Because we have Dirac delta justified in not matching derivatives, but $-\\frac{\\hbar^2}{2m}\\Delta \\psi'|_a=g\\psi(a).$ Then $\\psi'_L=k\\sinh ak, \\psi'_R=-kAe^{-ak}=-k\\cosh ak,$ so $\\Delta\\psi'|_a=-2ke^{ak}.$ Then $2ke^{ak}\\cdot\\frac{\\hbar^2}{2m}=g\\cosh ak,$ i.e. $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1+e^{-2ak}).$ Odd bound state is $\\sinh kx$ in the middle. Similar computation yields $\\psi'_L=k\\cosh ak,$ $\\psi'_R=-k\\sinh ak,$ so $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1-e^{-2ak}).$ Recall $k^2=\\frac{2mE}{\\hbar^2}.$ In the limit of large $a,$ the LHS goes to 0. We're solving $C=\\frac{1\\pm e^{-2z}}{z}$ for $z$ as $C\\to 0.$ Then $z\\approx \\frac1C,$ to one extra degree $z_1\\approx\\frac{1+e^{-2/C}}C,z_2\\approx\\frac{1-e^{-2/C}}C.$ Then $k_1-k_2=\\frac{2e^{-2/C}}{aC}=\\frac{2e^{-2amg/\\hbar^2}}{\\hbar^2/mg}=\\frac{2mge^{-2amg/\\hbar^2}}{\\hbar^2}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-2","text":"Skipped.","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-3","text":"Calculate probabilities that quantum SHO has position greater than amplitude in classical SHO for $\\mathcal E=1,3,5.$ We can work entirely in non-dimensional coordinates. We get the polynomials $1,x,x^2-\\frac12$ as solutions to $u''-2xu'+(\\mathcal E-1)u=0.$ The classical energy potential $V=x^2.$ Then, non-normalized we get - $\\psi=e^{-x^2/2}$ corresponding to $L=1.$ Get $\\approx 0.078649.$ - $\\psi=xe^{-x^2/2}$ corresponding to $L=\\sqrt 3.$ Get $\\approx 0.0558051.$ - $\\psi=(2x^2-1)e^{-x^2/2}$ corresponding to $L=\\sqrt 5.$ Get $\\approx 0.04753471774.$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-4","text":"Compute: (a) Expectation of $x^4$ on $n$th SHO eigenstate (b) $\\Delta x,\\Delta p,\\Delta x\\Delta p$ on $n$th SHO eigenstate Finally, (c) Check that $$e^{-s^2+2s\\xi}=\\sum_{n=0}H_n(\\xi)\\frac{s^n}{n!}$$ satisfies $H_n''-2\\xi H_n'+2nH_n=0.$ Recall that $\\hat x=(\\hat a+\\hat a^\\dagger)\\sqrt{\\frac{\\hbar}{2m\\omega}}.$ Then $$ \\begin{align } \\langle \\psi_n,\\hat x^4\\psi_n\\rangle &=\\langle \\hat x^2\\psi_n,\\hat x^2\\psi_n\\rangle\\ &=\\frac{\\hbar^2}{4m^2\\omega^2}\\langle(\\hat a^2+\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a+\\hat a^\\dagger\\hat a^\\dagger)\\psi_n,\\cdot \\rangle. \\end{align } $$ Note that $\\hat a$ is destruction, $\\hat a^\\dagger$ is construction so we can split by \"degree\". Reference L14-15, p7. $$\\langle\\hat a^2\\psi_n,\\cdot\\rangle=\\langle \\sqrt{n(n-1)}\\phi_{n-2},\\cdot\\rangle=n(n-1).$$ Similarly, $\\langle (\\hat a^\\dagger)^2\\psi_n,\\cdot\\rangle=(n+1)(n+2).$ Finally $(\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a)\\psi_n=(n+n+1)\\psi_n,$ so our final answer is $n(n-1)+(n+1)(n+2)+(2n+1)^2=6n^2+6n+3.$ Adding our units back in, we get an answer of $$(6n^2+6n+3)\\frac{\\hbar^2}{4m^2\\omega^2}.$$ (b) is similar. $(\\Delta x)^2=\\frac{\\hbar}{2m\\omega}(2n+1).$ Now $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Then clearly $\\langle \\psi_n,\\hat p\\psi_n=0$ (also true from potential symmetry). Finally $$ \\begin{align } \\langle \\psi_n,\\hat p^2\\psi_n\\rangle &=-\\frac{m\\omega\\hbar}2\\langle\\psi_n,C_1\\psi_{n+2}+C_2\\psi_{n-2}-(2n+1)\\psi_n\\rangle\\ &=(2n+1)\\frac{m\\omega\\hbar}2. \\end{align } $$ Thus $\\Delta x\\Delta p=(n+\\frac12)\\hbar.$ (c) Taking the $s$-derivative of the whole generating function yields $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=1}H_n \\frac{s^{n-1}}{(n-1)!}=\\sum_{n=0}H_{n+1}\\frac{s^n}{n!}.$$ Then $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=0}2H_{n}\\frac{s^{n+1}}{n!}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=1}2H_{n-1}n\\frac{s^n}{n!}.$$ Thus $$H_{n+1}=2\\xi H_n-2nH_{n-1}.$$ Taking the $\\xi$-derivative yields $$\\sum_{n=1}2nH_{n-1}\\frac{s^n}{n!}=2se^{-s^2+2s\\xi}=\\sum_{n=0}H_n'\\frac{s^n}{n!}.$$ Thus $2nH_{n-1}=H_n',$ so $$H_n''-2\\xi H_n'+2nH_n=4n(n-1)H_{n-2}-4\\xi nH_{n-1}+2nH_n=2n(H_n-2\\xi H_{n-1}+2(n-1)H_{n-2})=0.$$ Yay!","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-5","text":"Harmonic oscillator, except $V(x)=\\infty$ for $x<0$. What are the possible energy levels? Basically we need the polynomial solution to $H''-2xH+2nH=0$ to satisfy $H(0)=0.$ We established $H_{n+1}=2xH_n-2nH_{n-1},$ so $H_{n+1}(0)=-2nH_{n-1}(0).$ Since $H_0=1, H_1=x,$ we get $n$ must be odd so the possible energy levels are $\\mathcal E=4n+3.$ Re-dimensionalizing yields $E=(4n+3)\\hbar\\omega.$","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-6","text":"Given $V=m\\omega^2x^2,$ $\\Psi(x,0)=\\frac1{\\sqrt2}\\left(\\varphi_0(x)+\\varphi_1(x)\\right)$: - Find $\\Psi(x,t),|\\Psi(x,t)|^2$ - Find $\\langle x\\rangle(t).$ - Find $\\langle p\\rangle(t).$ - Show that for any such $\\Psi,$ $|\\Psi(x,t)|^2=|\\Psi(x,t+T)|^2$ where $T=\\frac{2\\pi}\\omega.$ $E(\\varphi_0)=\\hbar\\omega, E(\\varphi_1)=3\\hbar\\omega,$ so $$\\Psi(x,t)=\\frac1{\\sqrt2}(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1).$$ Then $$\\Psi\\Psi^*=\\frac12\\left(\\varphi_0^2+\\varphi_1^2+2\\varphi_0\\varphi_1\\cos(2t\\omega)\\right).$$ Then $\\langle x\\rangle=\\cos(2t\\omega)\\int x\\varphi_0\\varphi_1.$ Turns out that $\\hat x\\varphi_0=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger)\\varphi_0=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\varphi_1.$ As a result $\\langle x\\rangle=\\cos(2t\\omega)\\sqrt{\\frac\\hbar{2m\\omega}}.$ Next, $$ \\begin{align } \\langle \\hat p\\rangle &=\\int \\Psi\\hat p\\Psi\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a)(e^{it\\omega}\\varphi_0+e^{3it\\omega}\\varphi_1)\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(-e^{3it\\omega}\\varphi_0+e^{it\\omega}\\varphi_1)\\ &=\\frac i2\\sqrt{\\frac{m\\omega\\hbar}2}\\int -e^{2it\\omega}\\varphi_0^2+e^{-2it\\omega}\\varphi_1^2\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\frac1{2i}(e^{2it\\omega}-e^{-2it\\omega})\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\sin(2t\\omega). \\end{align } $$ Finally, $E=N\\hbar\\omega,$ so all $e^{itE/\\hbar}$ terms are periodic in $t$ every $\\frac{2\\pi}{E/\\hbar}=\\frac{2\\pi}{N\\omega}.$ Thus if we increment $t$ by $\\frac{2\\pi}\\omega$ the value of $\\Psi$ is unchanged.","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/","text":"Problem 1 Explore the SHO state $\\psi_\\alpha\\equiv N\\text{exp}(\\alpha \\hat a^\\dagger)\\varphi_0.$ Equivalently, $$\\psi_\\alpha=N\\sum_{i=0}\\alpha^i (\\hat a^\\dagger)^i\\varphi_0\\frac1{i!}=N\\sum_{i=0}\\alpha^i \\varphi_i\\frac1{\\sqrt{i!}}.$$ Then $$1=|\\psi_\\alpha|^2=N^2\\sum_i \\alpha^{2i}\\frac1{i!}=Ne^{\\alpha^2/2}.$$ Thus $N=e^{-\\alpha^2/2}.$ Now $$\\hat a\\psi_\\alpha=N\\sum_{i=0}\\alpha^i \\varphi_{i-1}\\frac1{\\sqrt{(i-1)!}}=\\alpha\\psi_\\alpha.$$ Then $$ \\begin{align } \\hat H\\psi_\\alpha &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}E_i\\ &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}(i+\\frac12)\\hbar\\omega\\ &=e^{-\\alpha^2/2}\\left(\\sum_{i=1}\\alpha^i\\varphi_i\\frac{\\sqrt i}{\\sqrt{(i-1)!}}+\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{2\\sqrt{i!}}\\right), \\end{align } $$ so $$\\langle\\hat H\\rangle=e^{-\\alpha^2}\\left(\\alpha^{}e^{\\alpha^2}+\\frac12e^{\\alpha^2}\\right)=\\alpha+\\frac12.$$ Also, $$\\langle\\hat H^2\\rangle =e^{-\\alpha^2}\\sum_{i=0}\\alpha^i\\frac{(i+\\frac12)^2}{i!}.$$ Then $(i+\\frac12)^2=i^2+i+\\frac14=i(i-1)+2i+\\frac14,$ so we get $$\\langle\\hat H^2\\rangle=e^{-\\alpha^2}\\left(\\alpha^2e^{\\alpha^2}+2\\alpha e^{\\alpha^2}+\\frac14 e^{\\alpha^2}\\right)=\\alpha^2+2\\alpha+\\frac14.$$ Then $(\\Delta\\hat H)^2=\\alpha^2+2\\alpha+\\frac14-(\\alpha^2+\\alpha+\\frac14)=\\alpha.$ Finally, we can compute $\\psi_\\alpha$. Abbreviating as $f$ for ease, $$\\alpha f=\\hat af=(\\sqrt{\\frac{m\\omega}{2\\hbar}})(x+\\frac{\\hbar}{m\\omega}\\frac{\\partial}{\\partial x})f.$$ Nondimensionalizing with $L=\\sqrt{\\frac{\\hbar}{m\\omega}}$ yields $$\\alpha f=\\frac1{\\sqrt2}(x+\\frac{\\partial}{\\partial x})f.$$ Taking $f=e^{-x^2/2}h$ yields $\\alpha h=\\frac1{\\sqrt 2}h',$ so $h=e^{\\alpha x\\sqrt 2}.$ Thus before normalization $\\psi_\\alpha=e^{-x^2/2+\\alpha x\\sqrt 2}.$ Problem 2 Analyze double-delta potential, again. Use an extra $V_r(x)=\\frac{\\beta g}{x}$ for some small $\\beta>0$. Exercise in non-dimensionalization (sorta)! Use $E_0=\\frac{mg^2}{2\\hbar^2}.$ Recall that $\\xi=\\kappa a,$ where $E/E_0=-\\xi^2,$ $\\frac\\xi{1+e^{-2\\xi}}=\\lambda,$ $\\lambda\\equiv\\frac{mag}{\\hbar^2}$ (recall $g$ is Joules). See that $E=-E_0\\xi^2.$ Also, $V_r=\\frac{\\beta g}{2a}=\\frac{\\beta E_0}{\\lambda}.$ Then we can plot $E_{tot}=E+V_r=E_0(\\frac{\\beta}{\\lambda}-\\xi^2).$ For $\\beta=0.31,$ we can find local minima for $E_{tot}$ given $\\lambda.$ That yields the optimal value for $a,$ distance between two $H^+$ ions! Problem 3 Analyze the energy in the forbidden region of the finite square well as it limits towards the infinite square well. From a previous analysis we've determined that as $z_0^2=\\frac{2ma^2V}{\\hbar^2}$ limits to $\\infty$ equivalent to $V$ limit to $\\infty$. In this limit, $\\eta=ka\\sim\\frac\\pi2(1-\\frac1{z_0}),$ $\\xi=\\kappa a\\sim z_0,$ and $A$ the constant factor on the outer exponential satisfies $A\\sim \\frac\\pi{2z_0}e^{z_0}.$ What is $\\langle\\hat K\\rangle$ from the forbidden region $x>a,$ given $K=\\frac{\\hat p^2}{2m}=-\\frac{\\hbar^2\\partial^2}{2m\\partial x^2}?$ In this region we have $\\psi=\\frac A{\\sqrt a}e^{-\\kappa x}.$ Then we just get $$ \\begin{align } \\int_a^\\infty \\frac{A^2}{a}e^{-2\\kappa x}\\left(-\\frac{\\hbar^2\\kappa^2}{2m}\\right) &=\\frac{A^2\\hbar^2\\kappa}{4am}e^{-2\\kappa a}\\ &\\approx\\frac{A^2\\hbar^2z_0}{4a^2m}e^{-2z_0}\\ &\\approx\\frac{\\pi^2\\hbar^2}{16a^2mz_0}. \\end{align } $$ Units check out. As $z_0$ limits to $\\infty$ this contribution limits to $0$. yay! Problem 4 Consider step potential $V=0$ for $x<0,$ $V=V_0$ else. Analyze time delay, using a Gaussian wavepacket distribution $\\Phi$ with width $\\beta$. First let's analyze scattering time delay. As computed earlier for incoming $1$ we get $A=\\frac{\\kappa+ik}{\\kappa-ik},$ so letting $\\tan(\\delta)=\\frac k{\\kappa}$ means $e^{2i\\delta}=-A.$ Now we use $\\Phi(k)=e^{-\\beta^2a^2(k-k_0)^2},$ $a=\\frac{\\hbar}{\\sqrt{mV_0}}.$ The incoming wave is $$\\Psi_{inc}(x)=\\sqrt a\\int_0^{\\hat k} \\Phi(k)e^{ikx}e^{-iE(k)t/\\hbar},$$ so the outgoing wave is $$\\Psi_{out}(x)=-\\sqrt a\\int_0^\\hat k\\Phi(k)e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar},$$ where $\\delta(E)=\\arctan(\\frac k\\kappa)$ as derived above. After dividing by $2i$ we can rewrite the sum $-e^{2i\\delta(E(k))-ikx}+e^{ikx}$ as $e^{i\\delta(E)}\\sin(kx-\\delta).$ Introduce nondimensionalized $k=\\frac Ka,$ $x=au,$ and $t=\\frac\\hbar{V_0}\\tau.$ Naturally get $K_0$ and $kx=Ku.$ Let's look at the group velocity. For given $(u,\\tau)$ we need the phase derivative in $K$ at $K_0$ to be $0$. The phase is $$kx-E(k)t/\\hbar=Ku-\\frac{t\\hbar k^2}{2m}=Ku-\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau.$$ Then we need $u=\\frac{\\hbar^2}{V_0a^2m}\\tau K_0,$ i.e. $\\frac{du}{d\\tau}=\\frac{\\hbar^2}{V_0 a^2m}K_0=\\frac{\\hbar^2 k_0}{V_0am}.$ Recall now that $\\Delta x\\Delta k=\\Delta u\\Delta K\\geq \\frac12.$ Our distribution for $K$ is now $\\Phi(K)=e^{-\\beta^2(K-K_0)^2},$ so the uncertainty $\\Delta K$ is known to be $\\frac 1{2\\beta^2}$ (simple-ish integral by parts) so $\\Delta u\\geq \\beta^2.$ Continuing the non-dimensionalization, $E(k)=\\frac{\\hbar^2 k^2}{2m}=\\frac{\\hbar^2}{2a^2m}K^2=\\frac{\\hbar^2}{2a^2mV_0} V_0 K^2.$ First coeff is unitless. Furthermore, $e^{2i\\delta(E)}=\\frac{\\kappa+ik}{\\kappa-ik}=((\\kappa^2-k^2)+2k\\kappa i)\\div(\\kappa^2+k^2).$ Then $\\kappa=\\sqrt{\\frac{(V_0-E(K))}{\\hbar^2}},$ and $k=\\sqrt{\\frac{2mE}{\\hbar^2}}=\\frac Ka.$ Plugging all this in yields $$e^{2i\\delta(E)}=1-\\frac{\\hbar^2}{a^2mV_0}K^2+K\\sqrt{\\frac{2\\hbar^2}{a^2mV_0}-\\frac{2\\hbar^4}{a^4m^2V_0}K^2}\\equiv\\omega(K).$$ Then $\\Delta t=2\\hbar\\delta'(E)$ as a derivative in $k$ yields $\\Delta \\tau=\\frac{V_0}\\hbar \\Delta t=2V_0\\delta'(E).$ Then $\\delta=\\arctan(\\sqrt{\\frac{E}{V_0-E}}).$ Recall that $\\frac d{dx}\\arctan x=\\frac1{x^2+1},$ so $\\delta'(E)=\\frac1{2\\sqrt{E(V_0-E)}}.$ Then $$ \\Delta \\tau =\\frac{V_0}{\\sqrt{E(V_0-E)}} =\\frac{1}{\\sqrt{E/V_0\\cdot(1-E/V_0)}}. $$ Recall that $E/V_0=\\frac{\\hbar^2}{2a^2mV_0}K^2,$ and $\\Delta\\tau$ is evaluated at $K=K_0$ so we get $$\\Delta\\tau=\\frac1{K_0\\sqrt{\\frac{\\hbar^2}{2a^2mV_0}(1-\\frac{\\hbar^2}{2a^2mV_0}K_0^2)}}=\\frac{2a^2mV_0}{K_0\\sqrt{2a^2mV_0\\hbar^2-K_0^2}}.$$ We've computed several uncertainties and the time delay. Now let's look at the wavefunction again. $$ \\begin{align } a^{\\frac12}\\Psi(u,\\tau) &=a\\int_0^{\\hat k}\\Phi(k)\\left(e^{ikx}e^{-iE(k)t/\\hbar}-e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar}\\right)dk\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}\\Phi(K)\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}e^{-\\beta^2(K-K_0)^2}\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK. \\end{align } $$ yay!! we're done!! Problem 5 Consider same step potential. Compute reflection/transmission coefficients, and comment. For $E<V_0,$ as usual set $k^2=\\frac{2mE}{\\hbar^2},$ and $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2}.$ Then with transmission $1,$ reflection $A$ transmitted $B$ we get $A+1=B,$ and $\\frac{ik-ikA}{A+1}=-\\kappa.$ As a result $|A|=1$ so reflection coefficient is $0$. For $E>V_0$ set $k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ With the same setup we get $A+1=B$ and $\\frac{ik-ikA}{1+A}=ik_2$ so $A=\\frac{k-k_2}{k+k_2},$ and $B=\\frac{2k}{k+k_2}.$ But since the wavelength changed, we actually have $T={\\frac{k_2}k}\\frac{B^2}{1^2}=\\frac{4kk_2}{(k_1+k_2)^2}.$ Wondrously, $T+A^2=\\frac{4kk_2+k^2-2kk_2+k_2^2}{k^2+2kk_2+k_2^2}=1$. We only get perfect transmission in the limit $E\\to\\infty.$ Higher $E$, better $T$. O and BTW, for $E=V_0$ we get $k_2=0$ so $A=1$ and $B=2$ a constant flat thing. Perfect reflection.","title":"8.04.15 Pset 8"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-1","text":"Explore the SHO state $\\psi_\\alpha\\equiv N\\text{exp}(\\alpha \\hat a^\\dagger)\\varphi_0.$ Equivalently, $$\\psi_\\alpha=N\\sum_{i=0}\\alpha^i (\\hat a^\\dagger)^i\\varphi_0\\frac1{i!}=N\\sum_{i=0}\\alpha^i \\varphi_i\\frac1{\\sqrt{i!}}.$$ Then $$1=|\\psi_\\alpha|^2=N^2\\sum_i \\alpha^{2i}\\frac1{i!}=Ne^{\\alpha^2/2}.$$ Thus $N=e^{-\\alpha^2/2}.$ Now $$\\hat a\\psi_\\alpha=N\\sum_{i=0}\\alpha^i \\varphi_{i-1}\\frac1{\\sqrt{(i-1)!}}=\\alpha\\psi_\\alpha.$$ Then $$ \\begin{align } \\hat H\\psi_\\alpha &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}E_i\\ &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}(i+\\frac12)\\hbar\\omega\\ &=e^{-\\alpha^2/2}\\left(\\sum_{i=1}\\alpha^i\\varphi_i\\frac{\\sqrt i}{\\sqrt{(i-1)!}}+\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{2\\sqrt{i!}}\\right), \\end{align } $$ so $$\\langle\\hat H\\rangle=e^{-\\alpha^2}\\left(\\alpha^{}e^{\\alpha^2}+\\frac12e^{\\alpha^2}\\right)=\\alpha+\\frac12.$$ Also, $$\\langle\\hat H^2\\rangle =e^{-\\alpha^2}\\sum_{i=0}\\alpha^i\\frac{(i+\\frac12)^2}{i!}.$$ Then $(i+\\frac12)^2=i^2+i+\\frac14=i(i-1)+2i+\\frac14,$ so we get $$\\langle\\hat H^2\\rangle=e^{-\\alpha^2}\\left(\\alpha^2e^{\\alpha^2}+2\\alpha e^{\\alpha^2}+\\frac14 e^{\\alpha^2}\\right)=\\alpha^2+2\\alpha+\\frac14.$$ Then $(\\Delta\\hat H)^2=\\alpha^2+2\\alpha+\\frac14-(\\alpha^2+\\alpha+\\frac14)=\\alpha.$ Finally, we can compute $\\psi_\\alpha$. Abbreviating as $f$ for ease, $$\\alpha f=\\hat af=(\\sqrt{\\frac{m\\omega}{2\\hbar}})(x+\\frac{\\hbar}{m\\omega}\\frac{\\partial}{\\partial x})f.$$ Nondimensionalizing with $L=\\sqrt{\\frac{\\hbar}{m\\omega}}$ yields $$\\alpha f=\\frac1{\\sqrt2}(x+\\frac{\\partial}{\\partial x})f.$$ Taking $f=e^{-x^2/2}h$ yields $\\alpha h=\\frac1{\\sqrt 2}h',$ so $h=e^{\\alpha x\\sqrt 2}.$ Thus before normalization $\\psi_\\alpha=e^{-x^2/2+\\alpha x\\sqrt 2}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-2","text":"Analyze double-delta potential, again. Use an extra $V_r(x)=\\frac{\\beta g}{x}$ for some small $\\beta>0$. Exercise in non-dimensionalization (sorta)! Use $E_0=\\frac{mg^2}{2\\hbar^2}.$ Recall that $\\xi=\\kappa a,$ where $E/E_0=-\\xi^2,$ $\\frac\\xi{1+e^{-2\\xi}}=\\lambda,$ $\\lambda\\equiv\\frac{mag}{\\hbar^2}$ (recall $g$ is Joules). See that $E=-E_0\\xi^2.$ Also, $V_r=\\frac{\\beta g}{2a}=\\frac{\\beta E_0}{\\lambda}.$ Then we can plot $E_{tot}=E+V_r=E_0(\\frac{\\beta}{\\lambda}-\\xi^2).$ For $\\beta=0.31,$ we can find local minima for $E_{tot}$ given $\\lambda.$ That yields the optimal value for $a,$ distance between two $H^+$ ions!","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-3","text":"Analyze the energy in the forbidden region of the finite square well as it limits towards the infinite square well. From a previous analysis we've determined that as $z_0^2=\\frac{2ma^2V}{\\hbar^2}$ limits to $\\infty$ equivalent to $V$ limit to $\\infty$. In this limit, $\\eta=ka\\sim\\frac\\pi2(1-\\frac1{z_0}),$ $\\xi=\\kappa a\\sim z_0,$ and $A$ the constant factor on the outer exponential satisfies $A\\sim \\frac\\pi{2z_0}e^{z_0}.$ What is $\\langle\\hat K\\rangle$ from the forbidden region $x>a,$ given $K=\\frac{\\hat p^2}{2m}=-\\frac{\\hbar^2\\partial^2}{2m\\partial x^2}?$ In this region we have $\\psi=\\frac A{\\sqrt a}e^{-\\kappa x}.$ Then we just get $$ \\begin{align } \\int_a^\\infty \\frac{A^2}{a}e^{-2\\kappa x}\\left(-\\frac{\\hbar^2\\kappa^2}{2m}\\right) &=\\frac{A^2\\hbar^2\\kappa}{4am}e^{-2\\kappa a}\\ &\\approx\\frac{A^2\\hbar^2z_0}{4a^2m}e^{-2z_0}\\ &\\approx\\frac{\\pi^2\\hbar^2}{16a^2mz_0}. \\end{align } $$ Units check out. As $z_0$ limits to $\\infty$ this contribution limits to $0$. yay!","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-4","text":"Consider step potential $V=0$ for $x<0,$ $V=V_0$ else. Analyze time delay, using a Gaussian wavepacket distribution $\\Phi$ with width $\\beta$. First let's analyze scattering time delay. As computed earlier for incoming $1$ we get $A=\\frac{\\kappa+ik}{\\kappa-ik},$ so letting $\\tan(\\delta)=\\frac k{\\kappa}$ means $e^{2i\\delta}=-A.$ Now we use $\\Phi(k)=e^{-\\beta^2a^2(k-k_0)^2},$ $a=\\frac{\\hbar}{\\sqrt{mV_0}}.$ The incoming wave is $$\\Psi_{inc}(x)=\\sqrt a\\int_0^{\\hat k} \\Phi(k)e^{ikx}e^{-iE(k)t/\\hbar},$$ so the outgoing wave is $$\\Psi_{out}(x)=-\\sqrt a\\int_0^\\hat k\\Phi(k)e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar},$$ where $\\delta(E)=\\arctan(\\frac k\\kappa)$ as derived above. After dividing by $2i$ we can rewrite the sum $-e^{2i\\delta(E(k))-ikx}+e^{ikx}$ as $e^{i\\delta(E)}\\sin(kx-\\delta).$ Introduce nondimensionalized $k=\\frac Ka,$ $x=au,$ and $t=\\frac\\hbar{V_0}\\tau.$ Naturally get $K_0$ and $kx=Ku.$ Let's look at the group velocity. For given $(u,\\tau)$ we need the phase derivative in $K$ at $K_0$ to be $0$. The phase is $$kx-E(k)t/\\hbar=Ku-\\frac{t\\hbar k^2}{2m}=Ku-\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau.$$ Then we need $u=\\frac{\\hbar^2}{V_0a^2m}\\tau K_0,$ i.e. $\\frac{du}{d\\tau}=\\frac{\\hbar^2}{V_0 a^2m}K_0=\\frac{\\hbar^2 k_0}{V_0am}.$ Recall now that $\\Delta x\\Delta k=\\Delta u\\Delta K\\geq \\frac12.$ Our distribution for $K$ is now $\\Phi(K)=e^{-\\beta^2(K-K_0)^2},$ so the uncertainty $\\Delta K$ is known to be $\\frac 1{2\\beta^2}$ (simple-ish integral by parts) so $\\Delta u\\geq \\beta^2.$ Continuing the non-dimensionalization, $E(k)=\\frac{\\hbar^2 k^2}{2m}=\\frac{\\hbar^2}{2a^2m}K^2=\\frac{\\hbar^2}{2a^2mV_0} V_0 K^2.$ First coeff is unitless. Furthermore, $e^{2i\\delta(E)}=\\frac{\\kappa+ik}{\\kappa-ik}=((\\kappa^2-k^2)+2k\\kappa i)\\div(\\kappa^2+k^2).$ Then $\\kappa=\\sqrt{\\frac{(V_0-E(K))}{\\hbar^2}},$ and $k=\\sqrt{\\frac{2mE}{\\hbar^2}}=\\frac Ka.$ Plugging all this in yields $$e^{2i\\delta(E)}=1-\\frac{\\hbar^2}{a^2mV_0}K^2+K\\sqrt{\\frac{2\\hbar^2}{a^2mV_0}-\\frac{2\\hbar^4}{a^4m^2V_0}K^2}\\equiv\\omega(K).$$ Then $\\Delta t=2\\hbar\\delta'(E)$ as a derivative in $k$ yields $\\Delta \\tau=\\frac{V_0}\\hbar \\Delta t=2V_0\\delta'(E).$ Then $\\delta=\\arctan(\\sqrt{\\frac{E}{V_0-E}}).$ Recall that $\\frac d{dx}\\arctan x=\\frac1{x^2+1},$ so $\\delta'(E)=\\frac1{2\\sqrt{E(V_0-E)}}.$ Then $$ \\Delta \\tau =\\frac{V_0}{\\sqrt{E(V_0-E)}} =\\frac{1}{\\sqrt{E/V_0\\cdot(1-E/V_0)}}. $$ Recall that $E/V_0=\\frac{\\hbar^2}{2a^2mV_0}K^2,$ and $\\Delta\\tau$ is evaluated at $K=K_0$ so we get $$\\Delta\\tau=\\frac1{K_0\\sqrt{\\frac{\\hbar^2}{2a^2mV_0}(1-\\frac{\\hbar^2}{2a^2mV_0}K_0^2)}}=\\frac{2a^2mV_0}{K_0\\sqrt{2a^2mV_0\\hbar^2-K_0^2}}.$$ We've computed several uncertainties and the time delay. Now let's look at the wavefunction again. $$ \\begin{align } a^{\\frac12}\\Psi(u,\\tau) &=a\\int_0^{\\hat k}\\Phi(k)\\left(e^{ikx}e^{-iE(k)t/\\hbar}-e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar}\\right)dk\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}\\Phi(K)\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}e^{-\\beta^2(K-K_0)^2}\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK. \\end{align } $$ yay!! we're done!!","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-5","text":"Consider same step potential. Compute reflection/transmission coefficients, and comment. For $E<V_0,$ as usual set $k^2=\\frac{2mE}{\\hbar^2},$ and $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2}.$ Then with transmission $1,$ reflection $A$ transmitted $B$ we get $A+1=B,$ and $\\frac{ik-ikA}{A+1}=-\\kappa.$ As a result $|A|=1$ so reflection coefficient is $0$. For $E>V_0$ set $k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ With the same setup we get $A+1=B$ and $\\frac{ik-ikA}{1+A}=ik_2$ so $A=\\frac{k-k_2}{k+k_2},$ and $B=\\frac{2k}{k+k_2}.$ But since the wavelength changed, we actually have $T={\\frac{k_2}k}\\frac{B^2}{1^2}=\\frac{4kk_2}{(k_1+k_2)^2}.$ Wondrously, $T+A^2=\\frac{4kk_2+k^2-2kk_2+k_2^2}{k^2+2kk_2+k_2^2}=1$. We only get perfect transmission in the limit $E\\to\\infty.$ Higher $E$, better $T$. O and BTW, for $E=V_0$ we get $k_2=0$ so $A=1$ and $B=2$ a constant flat thing. Perfect reflection.","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/","text":"Resonance Scattering state: Finite square well with incoming wave from left side. Then we have inc., refl. on left side, two oscillating in the well, and a single transmitted from the right. Specifically, $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ Solving in terms of $A$ by matching $\\Psi,\\Psi'$ at $x=\\pm a.$ As usual we have $k^2=\\frac{2mE}{\\hbar^2},$ $k_2^2=\\frac{2m(E+V_0)}{\\hbar^2}.$ As a result we get $$\\frac1T=\\frac{|A|^2}{|F|^2}=1+\\frac14\\frac{V^2}{E^2+EV}\\sin^2(2k_2a).$$ Evidently for \"arbitrary\" $E,K_2$ we get $T\\in(0,1)$ and also $T\\to 0$ as $E\\to 0,$ $T\\to\\infty$ as $E\\to\\infty$. But if we want perfect $T=1$ we can fix $\\sin(2k_2a)=0,$ i.e. the frequency $k_2$ fits itself perfectly within the well width $2a$. More specifically, given that $2k_2a=n\\pi,$ we can compute $$E+V=\\frac{n^2\\pi^2\\hbar^2}{2m(2a)^2,}$$ the same as the energy for the $n$th bound state in the inf. square well! This was first seen in the real world in the Ramsauer-Townsend effect, where the number of reflections when electrons were fired at a wall of noble gases had unexpected troughs regularly. Half-Domain Scattering We consider scattering states with potentials with an infinite wall at $x=0$ and $V=0$ for $x>r$. These are instigated by an incoming wave from $x=\\infty,$ so $x=e^{-ikx}$ for the usual $k^2=\\frac{2mE}{\\hbar^2}.$ In the case that $V=0$ for all $x>0$ the only resolution to $\\Psi(0)=0$ is $\\sin(kx),$ so we'll slap $\\frac1{2i}$ on everything as convention. For general $V$ let the outgoing wave be $e^{ikx+i2\\delta}$ for some $\\delta(x).$ For $x>r$ we actually need $\\delta$ constant. Also $\\delta$ must be real because of how probability flux works. Then we can write $$\\Psi(x)=\\frac1{2i}\\left(e^{ikx+2i\\delta}-e^{-ikx}\\right)=e^{i\\delta}\\sin(kx+\\delta).$$ Time Delay If we examine a wavepacket constructed from $g(k)$ then $$\\Psi_{inc}=\\int g(k)(e^{-ikx})(e^{-iE(k)t/\\hbar}).$$ The reflected wavepacket is $$\\Psi_{ref}=\\int g(k)(e^{ikx+2i\\delta(k)})(e^{-iE(k)t/\\hbar}).$$ Since $\\frac{\\partial P}{\\partial k}=0$ at the crest, we get $$x+2\\frac{\\partial\\delta}{\\partial k}-\\frac1\\hbar\\frac{\\partial E}{\\partial k}t=0$$ evaluated at $k=k_0.$ Pulling out a $\\frac{\\partial E}{\\partial k}=\\frac{\\hbar^2k}{m}$ from $\\frac{\\partial\\delta}{\\partial k}$ we get $$x=\\frac{\\hbar k_0}{m}(t-2\\hbar\\delta'(E(k_0))).$$ As a result the time delay is $$\\Delta t=2\\hbar\\delta'(E(k_0))$$ (with $\\delta$ as a function of $E$ ofc). Example Consider constant potential for the nonzero region. Then ansatz is $A\\sin(k_2x),Be^{i\\delta}\\sin(kx+\\delta)$ piecewise. Notice $\\psi(0)=0$ is naturally handled. Solve this for $\\delta.$","title":"8.04.15 Resonance, Half Domain Scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#resonance","text":"Scattering state: Finite square well with incoming wave from left side. Then we have inc., refl. on left side, two oscillating in the well, and a single transmitted from the right. Specifically, $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ Solving in terms of $A$ by matching $\\Psi,\\Psi'$ at $x=\\pm a.$ As usual we have $k^2=\\frac{2mE}{\\hbar^2},$ $k_2^2=\\frac{2m(E+V_0)}{\\hbar^2}.$ As a result we get $$\\frac1T=\\frac{|A|^2}{|F|^2}=1+\\frac14\\frac{V^2}{E^2+EV}\\sin^2(2k_2a).$$ Evidently for \"arbitrary\" $E,K_2$ we get $T\\in(0,1)$ and also $T\\to 0$ as $E\\to 0,$ $T\\to\\infty$ as $E\\to\\infty$. But if we want perfect $T=1$ we can fix $\\sin(2k_2a)=0,$ i.e. the frequency $k_2$ fits itself perfectly within the well width $2a$. More specifically, given that $2k_2a=n\\pi,$ we can compute $$E+V=\\frac{n^2\\pi^2\\hbar^2}{2m(2a)^2,}$$ the same as the energy for the $n$th bound state in the inf. square well! This was first seen in the real world in the Ramsauer-Townsend effect, where the number of reflections when electrons were fired at a wall of noble gases had unexpected troughs regularly.","title":"Resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#half-domain-scattering","text":"We consider scattering states with potentials with an infinite wall at $x=0$ and $V=0$ for $x>r$. These are instigated by an incoming wave from $x=\\infty,$ so $x=e^{-ikx}$ for the usual $k^2=\\frac{2mE}{\\hbar^2}.$ In the case that $V=0$ for all $x>0$ the only resolution to $\\Psi(0)=0$ is $\\sin(kx),$ so we'll slap $\\frac1{2i}$ on everything as convention. For general $V$ let the outgoing wave be $e^{ikx+i2\\delta}$ for some $\\delta(x).$ For $x>r$ we actually need $\\delta$ constant. Also $\\delta$ must be real because of how probability flux works. Then we can write $$\\Psi(x)=\\frac1{2i}\\left(e^{ikx+2i\\delta}-e^{-ikx}\\right)=e^{i\\delta}\\sin(kx+\\delta).$$","title":"Half-Domain Scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#time-delay","text":"If we examine a wavepacket constructed from $g(k)$ then $$\\Psi_{inc}=\\int g(k)(e^{-ikx})(e^{-iE(k)t/\\hbar}).$$ The reflected wavepacket is $$\\Psi_{ref}=\\int g(k)(e^{ikx+2i\\delta(k)})(e^{-iE(k)t/\\hbar}).$$ Since $\\frac{\\partial P}{\\partial k}=0$ at the crest, we get $$x+2\\frac{\\partial\\delta}{\\partial k}-\\frac1\\hbar\\frac{\\partial E}{\\partial k}t=0$$ evaluated at $k=k_0.$ Pulling out a $\\frac{\\partial E}{\\partial k}=\\frac{\\hbar^2k}{m}$ from $\\frac{\\partial\\delta}{\\partial k}$ we get $$x=\\frac{\\hbar k_0}{m}(t-2\\hbar\\delta'(E(k_0))).$$ As a result the time delay is $$\\Delta t=2\\hbar\\delta'(E(k_0))$$ (with $\\delta$ as a function of $E$ ofc).","title":"Time Delay"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#example","text":"Consider constant potential for the nonzero region. Then ansatz is $A\\sin(k_2x),Be^{i\\delta}\\sin(kx+\\delta)$ piecewise. Notice $\\psi(0)=0$ is naturally handled. Solve this for $\\delta.$","title":"Example"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/","text":"Context here: we're dealing just with the infinite wall, incoming wave, 1D potential thing. Levinson's What's the number of (countable) bound states for a potential? (Recall scattering states are infinite.) Let's take a second infinite wall whose distance $L$ limits to $\\infty$. We have $V$ which ends at $R$. Consider $V=0,$ then we got $\\sin(kx)$ coming in, the second wall creates $kL=n\\pi.$ Then for infinitesimal interval $dk$ we got $dn=\\frac L\\pi dk$ possibilities for $n$. For $V\\neq 0,$ we got $e^{i\\delta}\\sin(kx+\\delta)$ coming in, so BC yields $kL+\\delta=n'\\pi.$ Then same thing, $Ldk+\\frac{d\\delta}{d k}dk=\\pi dn'.$ Then $dn'=\\frac L\\pi dk + \\frac1\\pi \\frac{d\\delta}{dk}dk,$ so turning on $V$ costs us $dn-dn'=-\\frac1\\pi\\frac{d\\delta}{dk}dk$ bound states, over the interval $dk$. Then integrating over all $k$, we get a loss of $\\frac 1\\pi (\\delta(0)-\\delta(\\infty)).$ But as we slowly turn on $V$ we can't lose states, so all of these \"lost\" states become negative energy -> bound states! thonk. Modeling Resonance Clearly $E\\in (0,V_1).$ We want resonant, i.e. large amplitude in potential and large time delay. Recall $\\Delta t=\\frac{d\\delta}{dE},$ or something. We get bumps when $\\delta$ crosses $\\frac{n\\pi}2,$ cuz that's when stuff is \"resonant,\" I suppose. Waves matching with each other. Let's have a double-step potential, $V=-V_0$ for $x<a,$ $V=V_1$ for $x<2a,$ $V=0$ otherwise. Solve with a cute trick using $\\cosh(x-a),\\sinh(x-a)$ in the middle interval. ![[Pasted image 20240107222205.png]] We want large resonance, so we want $\\delta$ to quickly cross from $0$ to $\\pi$, so large derivative at $\\delta=\\frac\\pi 2$. We can achieve this with $$\\delta=\\tan^{-1}\\left(\\frac{\\alpha\\varepsilon}{\\alpha-k}\\right).$$ Using $\\beta=\\alpha\\varepsilon,$ ![[Pasted image 20240107222729.png]] We then calculate $\\Delta t\\sim \\frac1\\beta$ (using $\\frac{d\\delta}{dk}$ as heuristic) and our amplitude heuristic $|\\psi_s|^2=\\sin\\delta^2=\\frac{\\beta^2}{\\beta^2+(\\alpha-k)^2}.$ At $k\\sim \\alpha,$ we get $|\\psi_s|^2=\\frac{\\frac14\\Gamma^2}{(E-E_\\alpha)^2+\\frac14\\Gamma^2}$ where $\\Gamma=\\frac{2\\alpha\\beta\\hbar^2}{m}.$ Here $\\psi_s$ is basically a measure of scattering resonance sm. $\\Gamma$ is a measure of the width of the distribution of the wavefunction amplitude in the potential as a function of $k$. Taking the time $\\tau=\\frac\\hbar\\Gamma,$ we can evaluate $\\Delta t=4\\tau.$ Let's go a little wild. Take the $x>R$ wavefunction $e^{i\\delta}\\sin(kx+\\delta),$ so we care about $A_s=e^{i\\delta}\\sin\\delta.$ $A_s$ here is scattering amplitude. Then $A_s=\\frac{\\tan\\delta}{1-i\\tan\\delta}.$ So weirdly enough we'd like $\\tan\\delta=-i.$ Notice that our $|A_s|=1$ from earlier is just the best we can do with real $\\delta$, $\\delta\\to\\frac\\pi 2,$ i.e. $\\tan\\delta\\to\\infty.$ Now, continuing with our weird example... If we sub stuff back in we get $A_s=\\frac{\\frac{\\beta}{\\alpha-k}}{1-i\\frac{\\beta}{\\alpha-k}}=\\frac{\\beta}{(a-i\\beta)-k}.$ Then we want $k$ to be as close to $a-i\\beta$ as possible. If we allow complex $k$... we get arbitrarily big resonance at $\\tan(\\delta(k))=-i.$ Here, the imaginary part $\\beta$ refers us to the half-life (somehow, didnt get to that,) and the positive-imaginary-part poles refer us to the bound states! (somehow?? $E\\sim k^2<0,$ I suppose) so goes physics ![[Pasted image 20240107223209.png]]","title":"8.04.16 Levinson's, Resonance, Complex k plane"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/#levinsons","text":"What's the number of (countable) bound states for a potential? (Recall scattering states are infinite.) Let's take a second infinite wall whose distance $L$ limits to $\\infty$. We have $V$ which ends at $R$. Consider $V=0,$ then we got $\\sin(kx)$ coming in, the second wall creates $kL=n\\pi.$ Then for infinitesimal interval $dk$ we got $dn=\\frac L\\pi dk$ possibilities for $n$. For $V\\neq 0,$ we got $e^{i\\delta}\\sin(kx+\\delta)$ coming in, so BC yields $kL+\\delta=n'\\pi.$ Then same thing, $Ldk+\\frac{d\\delta}{d k}dk=\\pi dn'.$ Then $dn'=\\frac L\\pi dk + \\frac1\\pi \\frac{d\\delta}{dk}dk,$ so turning on $V$ costs us $dn-dn'=-\\frac1\\pi\\frac{d\\delta}{dk}dk$ bound states, over the interval $dk$. Then integrating over all $k$, we get a loss of $\\frac 1\\pi (\\delta(0)-\\delta(\\infty)).$ But as we slowly turn on $V$ we can't lose states, so all of these \"lost\" states become negative energy -> bound states! thonk.","title":"Levinson's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/#modeling-resonance","text":"Clearly $E\\in (0,V_1).$ We want resonant, i.e. large amplitude in potential and large time delay. Recall $\\Delta t=\\frac{d\\delta}{dE},$ or something. We get bumps when $\\delta$ crosses $\\frac{n\\pi}2,$ cuz that's when stuff is \"resonant,\" I suppose. Waves matching with each other. Let's have a double-step potential, $V=-V_0$ for $x<a,$ $V=V_1$ for $x<2a,$ $V=0$ otherwise. Solve with a cute trick using $\\cosh(x-a),\\sinh(x-a)$ in the middle interval. ![[Pasted image 20240107222205.png]] We want large resonance, so we want $\\delta$ to quickly cross from $0$ to $\\pi$, so large derivative at $\\delta=\\frac\\pi 2$. We can achieve this with $$\\delta=\\tan^{-1}\\left(\\frac{\\alpha\\varepsilon}{\\alpha-k}\\right).$$ Using $\\beta=\\alpha\\varepsilon,$ ![[Pasted image 20240107222729.png]] We then calculate $\\Delta t\\sim \\frac1\\beta$ (using $\\frac{d\\delta}{dk}$ as heuristic) and our amplitude heuristic $|\\psi_s|^2=\\sin\\delta^2=\\frac{\\beta^2}{\\beta^2+(\\alpha-k)^2}.$ At $k\\sim \\alpha,$ we get $|\\psi_s|^2=\\frac{\\frac14\\Gamma^2}{(E-E_\\alpha)^2+\\frac14\\Gamma^2}$ where $\\Gamma=\\frac{2\\alpha\\beta\\hbar^2}{m}.$ Here $\\psi_s$ is basically a measure of scattering resonance sm. $\\Gamma$ is a measure of the width of the distribution of the wavefunction amplitude in the potential as a function of $k$. Taking the time $\\tau=\\frac\\hbar\\Gamma,$ we can evaluate $\\Delta t=4\\tau.$ Let's go a little wild. Take the $x>R$ wavefunction $e^{i\\delta}\\sin(kx+\\delta),$ so we care about $A_s=e^{i\\delta}\\sin\\delta.$ $A_s$ here is scattering amplitude. Then $A_s=\\frac{\\tan\\delta}{1-i\\tan\\delta}.$ So weirdly enough we'd like $\\tan\\delta=-i.$ Notice that our $|A_s|=1$ from earlier is just the best we can do with real $\\delta$, $\\delta\\to\\frac\\pi 2,$ i.e. $\\tan\\delta\\to\\infty.$ Now, continuing with our weird example... If we sub stuff back in we get $A_s=\\frac{\\frac{\\beta}{\\alpha-k}}{1-i\\frac{\\beta}{\\alpha-k}}=\\frac{\\beta}{(a-i\\beta)-k}.$ Then we want $k$ to be as close to $a-i\\beta$ as possible. If we allow complex $k$... we get arbitrarily big resonance at $\\tan(\\delta(k))=-i.$ Here, the imaginary part $\\beta$ refers us to the half-life (somehow, didnt get to that,) and the positive-imaginary-part poles refer us to the bound states! (somehow?? $E\\sim k^2<0,$ I suppose) so goes physics ![[Pasted image 20240107223209.png]]","title":"Modeling Resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/","text":"Non-dimensionalized momentum $\\frac{i\\hat pa}{\\hbar}$ represents location shift; $e^{\\frac{i\\hat pa}\\hbar}=e^{a\\frac{\\partial}{\\partial x}},$ after expansion, $e^{a\\frac\\partial{\\partial x}}\\psi(x)=\\psi(x+a)$ representing shift left $-a$. Angular momentum operator Typically $\\bf L=\\bf r\\times\\bf p.$ We're using central angular momentum with spherical potential, so $\\bf r=\\bf x, \\bf p=\\hat p.$ - $\\bf\\hat L\\equiv\\bf\\hat r\\times\\bf\\hat p$ - $\\hat L_x=\\hat y\\hat p_z-\\hat z\\hat p_y$ - $\\hat L_i$ are Hermitian - Computing the commutator, $$ \\begin{align } [\\hat L_x,\\hat L_y] &=[yp_z-zp_y,zp_x-xp_z]\\ &=[yp_z,zp_x]+[zp_y,xp_z]\\ &=y[p_z,z]p_x+x[z,p_z]p_y\\ &=i\\hbar(xp_y-yp_x)\\ &=i\\hbar L_z. \\end{align } $$ - Commutators are ==algebraically cyclic==. Spin operators are similarly algebraically cyclic with commutator. - Thus no nontrivial eigenstate of $L_x,L_y$ simultaneously exists Expanding the original expansion yields $$\\hat L_x=\\frac\\hbar i(y\\frac d{dz}-z\\frac d{dy})=\\frac\\hbar i\\frac{\\partial}{\\partial \\phi_x}.$$ Here $\\phi_x=\\arctan(\\frac zy),$ the angle formed by the projection onto the 2D $yz$ plane, with $y$ as base. Expressing $L_x,L_y$ using standard spherical $\\phi,\\theta$ is trickier. Radial Ansatz Working with spherical potential, Schrodinger's becomes an equation in $\\bf L^2$ and $\\frac d{dr}.$ Now we care about $\\psi$ the eigenstate of $\\hat L_z,\\bf L^2.$ We specify $L_z\\psi=\\hbar m\\psi,{\\bf L^2}\\psi=\\hbar^2l(l+1)\\psi.$ Recall $\\psi(\\theta,\\phi)$ is $2\\pi$ periodic in both arguments. Solving first directly yields $\\frac d{d\\phi}\\psi=im\\psi,$ so $\\psi\\sim e^{im\\phi}.$ by $2\\pi$ periodicity $m\\in \\mathbb Z.$ Solving second and doing algebra yields $x=\\cos\\theta,$ $\\psi\\sim P,$ $$\\frac d{dx}\\left[(1-x)^2\\frac{dP}{dx}\\right]+\\left[l(l+1)-\\frac{m^2}{1-x^2}\\right]P=0.$$Here $P$ is Legendre polynomial $l,m$. Taking $m=0$, we can solve for coefficients of the series expansion, we get the Legendre polynomials $P_l=\\frac1{2^l l!}\\left(\\frac d{dx}\\right)^l(x^2-1)^l.$ Normalized form is $$Y_{l,m}(\\theta,\\phi)=\\sqrt{\\frac{2l+1}{4\\pi}\\frac{(l-m)!}{(l+m)!}}(-1)^me^{im\\phi}P_l^m(\\cos\\theta),$$with $P_l^m$ above Legendre polynomial. Resuming with radial ansatz, - Plugging in $\\psi(r,\\theta,\\phi)=R(r)Y(\\theta,\\phi)$ yields $u\\equiv rR,$ $$-\\frac{\\hbar^2}{2m}\\frac{d^2 u}{dr^2}+\\left(V(r)+\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u=Eu.$$ - We can match singular terms (the $r^{-2}$) as $r\\to 0,$ and determine $u\\sim r^{l+1}$ in that region.","title":"8.04.17 Angular Momentum, Radial Ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/#angular-momentum-operator","text":"Typically $\\bf L=\\bf r\\times\\bf p.$ We're using central angular momentum with spherical potential, so $\\bf r=\\bf x, \\bf p=\\hat p.$ - $\\bf\\hat L\\equiv\\bf\\hat r\\times\\bf\\hat p$ - $\\hat L_x=\\hat y\\hat p_z-\\hat z\\hat p_y$ - $\\hat L_i$ are Hermitian - Computing the commutator, $$ \\begin{align } [\\hat L_x,\\hat L_y] &=[yp_z-zp_y,zp_x-xp_z]\\ &=[yp_z,zp_x]+[zp_y,xp_z]\\ &=y[p_z,z]p_x+x[z,p_z]p_y\\ &=i\\hbar(xp_y-yp_x)\\ &=i\\hbar L_z. \\end{align } $$ - Commutators are ==algebraically cyclic==. Spin operators are similarly algebraically cyclic with commutator. - Thus no nontrivial eigenstate of $L_x,L_y$ simultaneously exists Expanding the original expansion yields $$\\hat L_x=\\frac\\hbar i(y\\frac d{dz}-z\\frac d{dy})=\\frac\\hbar i\\frac{\\partial}{\\partial \\phi_x}.$$ Here $\\phi_x=\\arctan(\\frac zy),$ the angle formed by the projection onto the 2D $yz$ plane, with $y$ as base. Expressing $L_x,L_y$ using standard spherical $\\phi,\\theta$ is trickier.","title":"Angular momentum operator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/#radial-ansatz","text":"Working with spherical potential, Schrodinger's becomes an equation in $\\bf L^2$ and $\\frac d{dr}.$ Now we care about $\\psi$ the eigenstate of $\\hat L_z,\\bf L^2.$ We specify $L_z\\psi=\\hbar m\\psi,{\\bf L^2}\\psi=\\hbar^2l(l+1)\\psi.$ Recall $\\psi(\\theta,\\phi)$ is $2\\pi$ periodic in both arguments. Solving first directly yields $\\frac d{d\\phi}\\psi=im\\psi,$ so $\\psi\\sim e^{im\\phi}.$ by $2\\pi$ periodicity $m\\in \\mathbb Z.$ Solving second and doing algebra yields $x=\\cos\\theta,$ $\\psi\\sim P,$ $$\\frac d{dx}\\left[(1-x)^2\\frac{dP}{dx}\\right]+\\left[l(l+1)-\\frac{m^2}{1-x^2}\\right]P=0.$$Here $P$ is Legendre polynomial $l,m$. Taking $m=0$, we can solve for coefficients of the series expansion, we get the Legendre polynomials $P_l=\\frac1{2^l l!}\\left(\\frac d{dx}\\right)^l(x^2-1)^l.$ Normalized form is $$Y_{l,m}(\\theta,\\phi)=\\sqrt{\\frac{2l+1}{4\\pi}\\frac{(l-m)!}{(l+m)!}}(-1)^me^{im\\phi}P_l^m(\\cos\\theta),$$with $P_l^m$ above Legendre polynomial. Resuming with radial ansatz, - Plugging in $\\psi(r,\\theta,\\phi)=R(r)Y(\\theta,\\phi)$ yields $u\\equiv rR,$ $$-\\frac{\\hbar^2}{2m}\\frac{d^2 u}{dr^2}+\\left(V(r)+\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u=Eu.$$ - We can match singular terms (the $r^{-2}$) as $r\\to 0,$ and determine $u\\sim r^{l+1}$ in that region.","title":"Radial Ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/","text":"Problem 1 Explore $f(\\lambda)=\\int_{-\\infty}^{\\infty}dx\\,e^{-100(x-2)^2}e^{i\\phi(\\lambda,x)}$ where $\\phi(\\lambda,x)=50(x-\\lambda\\frac1{32}x^4).$ Half-max width is at $e^{-100(x-2)^2}=\\frac12,$ i.e. $100(x-2)^2=\\ln 2,$ so $\\Delta=\\frac{\\sqrt {\\ln 2}}{5}.$ Integrating from 1 to 3 is thus completely fine, at 1 or 3 we've reached $e^{-100},$ wow. Stationary phase specifies $\\left.\\frac d{dx}\\phi\\right|_2=0,$ i.e. $1-\\frac18\\lambda 2^3=0,$ so $\\lambda=1.$ Then $50(x-\\frac1{32} x^4)$ expanded degree 2 round $x=2$ is $50(1.5-\\frac3{16}(x-2)^2).$ Using that approximation, within the half-max width range, the excursion of $\\phi$ is just $50\\cdot\\frac 3{16}\\frac{ln 2}{100}=\\left(\\frac{3\\ln 2}{32\\pi}\\right)\\pi$ radians. Not all that much! Integrating with constant $\\phi$ yields $\\frac{\\sqrt\\pi}{10}\\cdot e^{75i}.$ Integrating with quadratic approx. $\\phi$ yields $\\frac{\\sqrt\\pi}{\\sqrt{100+\\frac3{16}i}}\\cdot e^{75i}.$ Problem 2 Analyze limits for $\\tan(\\delta)=\\frac{1-\\frac{k'}{k}\\cot k'a\\tan ka}{\\tan ka+\\frac{k'}k\\cot k'a},$ with $k^2=\\frac{2mE}{\\hbar^2}, k'^2=\\frac{2m(V+E)}{\\hbar^2},$ and $z_0=\\frac{2mV_0a^2}{\\hbar^2}.$ As $E\\to 0,$ $k\\to 0$ and $k'\\to \\frac{\\sqrt{2mV}}{\\hbar}.$ Then we can rewrite as $$\\tan(\\delta)=\\frac{k-k'\\cot k'a\\tan ka}{k\\tan ka+k'\\cot k'a}.$$ From this it is clear top $\\to 0$ and bottom $\\to k'\\cot k'a\\neq 0,$ so $\\tan(\\delta)\\to 0.$ As $E\\to\\infty,$ $k'-k\\to 0$$ so $\\cot k'a\\tan ka\\to 1.$ Top $\\to 0$ while bottom $\\to 2k(\\tan ka+\\cot k'a)$ which has norm at least $\\frac14$. Thus $\\tan(\\delta)\\to 0.$ Finally, letting $u=ka$ we have $k'a=\\sqrt{u^2+z_0^2},$ so we get $$\\delta=\\arctan\\left(\\frac{u-\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}\\tan u}{u\\tan u+\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}}\\right).$$ Problem 3 Solve for phase shift in the potential $V$ equalling $V_0>0$ for $x\\in (0,a),$ $0$ for $x>a,$ and $\\infty$ otherwise. If $E(k)>V_0$ then we get $k^2=\\frac{2mE}{\\hbar^2}, k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ On the right we have $\\sin(kx+\\delta)$ and on the left $\\sin(k_2x)$ so the old $\\psi/\\psi'$ strat yields $$ \\begin{align } k_2\\tan(ak_2) &=k\\tan(ak+\\delta)\\ &=\\frac{k\\tan(ak)+k\\tan(\\delta)}{1-\\tan(ak)\\tan(\\delta)}. \\end{align } $$ Then it's plain to see (from sight expansion) that $$\\tan(\\delta)=\\frac{k_2\\tan(ak_2)-k\\tan(ak)}{k+k_2\\tan(ak_2)\\tan(ak)}.$$ If $E(k)<V_0$ then with $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2},$ left is $\\sinh(\\kappa x)$ and right is $\\sin(kx+\\delta)$ so same thing $$ \\begin{align } \\kappa\\tanh(a\\kappa) &=k\\tan(ak+\\delta).\\ \\end{align } $$ By the same manipulation, $$\\tan(\\delta)=\\frac{\\kappa\\tanh(a\\kappa)-k\\tan(ak)}{k+\\kappa\\tanh(a\\kappa)\\tan(ak)}.$$ Problem 4 Examine phase shift with $V(x)=g\\delta(x-a),$ $g$ of units $J\\cdot m$. On the left $\\sin(kx)$ on the right $\\sin(kx+\\delta),$ as usual. Integrating the Hamiltonian yields $$g\\Psi(a)=\\frac{\\hbar^2}{2m}\\Delta\\left.\\frac d{dx}\\Psi\\right|_a.$$ Matching via $\\sin(ak+\\delta)\\sin(kx)$ and $\\sin(ak)\\sin(kx+\\delta),$ we get $$ \\begin{align } g\\sin(ak)\\sin(ak+\\delta) &=\\frac{\\hbar^2}{2m}\\left(k\\cos(ak+\\delta)\\sin(ak)-k\\cos(ak)\\sin(ak+\\delta)\\right)\\ &=\\frac{k\\hbar^2}{2m}\\sin(ak-(ak+\\delta))\\ &=-\\frac{k\\hbar^2}{2m}\\sin\\delta. \\end{align } $$ Then $$g\\sin^2(ak)\\cos(\\delta)+g\\cos(ak)\\sin(ak)\\sin(\\delta)+\\frac{\\hbar^2}{2m}k\\sin(\\delta)=0.$$ Consequently $$\\tan(\\delta)=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\frac{\\hbar^2}{2amg}ak}=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\lambda ak},$$ using denote $\\lambda=\\frac{\\hbar^2}{2amg}.$ As $ak\\to 0$ we get $\\sin(ak)\\to ak$ so $\\tan(\\delta)\\to -\\frac{ak}{1+\\lambda},$ and as $\\lambda\\to\\infty$ we get $\\tan(\\delta)=0$ for any $ak$. Problem 5 Show that $x,y,p_x,p_y$ have zero expectation on a $L_z$ eigenstate. First we compute a bazillion commutators. $L_z=\\frac\\hbar i\\left(x\\frac d{dy}-y\\frac d{dz}\\right).$ Then $$ \\begin{align } [L_z,x]&=\\frac\\hbar i(0)-\\frac\\hbar i(y)=-\\frac\\hbar i y,\\ [L_z,y]&=\\frac\\hbar i(x)-\\frac\\hbar i(0)=\\frac\\hbar i x,\\ [L_z,z]&=0,\\ [L_z,p_x]&=\\frac\\hbar i\\left(-\\frac\\hbar i \\frac d{dy}\\right) - \\frac\\hbar i(0)=-\\frac\\hbar i p_y,\\ [L_z,p_y]&=\\frac\\hbar i(0) - \\frac\\hbar i\\left(-\\frac\\hbar i\\frac d{dx}\\right)=\\frac\\hbar i p_x,\\ [L_z,p_z]&=0. \\end{align } $$ Finally, if $[L_z,\\hat A]=\\hat B,$ then for an eigenstate $L_z\\psi=\\lambda\\psi$ we get $$\\begin{align } \\langle\\psi,\\hat A\\psi\\rangle &=\\langle\\psi,\\hat A\\lambda\\psi\\rangle\\frac1{\\lambda}\\ &=\\langle\\psi,\\hat AL_z\\psi\\rangle\\frac1{\\lambda}\\ &=\\left(\\langle\\psi,L_z\\hat A\\psi\\rangle+\\langle\\psi,[\\hat A,L_z]\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\left(\\langle L_z\\psi,\\hat A\\psi\\rangle+\\langle\\psi,-\\hat B\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\frac1{\\lambda}\\langle L_z\\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle\\ &=\\langle \\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle.\\ \\end{align }$$ Thus $\\langle\\psi,\\hat B\\psi\\rangle=0.$ Thus by the previous commutator computations, the claim is shown. Problem 6 Compute $\\bf L^2$ in spherical coordinates. Skipped","title":"8.04.17 Pset 9"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-1","text":"Explore $f(\\lambda)=\\int_{-\\infty}^{\\infty}dx\\,e^{-100(x-2)^2}e^{i\\phi(\\lambda,x)}$ where $\\phi(\\lambda,x)=50(x-\\lambda\\frac1{32}x^4).$ Half-max width is at $e^{-100(x-2)^2}=\\frac12,$ i.e. $100(x-2)^2=\\ln 2,$ so $\\Delta=\\frac{\\sqrt {\\ln 2}}{5}.$ Integrating from 1 to 3 is thus completely fine, at 1 or 3 we've reached $e^{-100},$ wow. Stationary phase specifies $\\left.\\frac d{dx}\\phi\\right|_2=0,$ i.e. $1-\\frac18\\lambda 2^3=0,$ so $\\lambda=1.$ Then $50(x-\\frac1{32} x^4)$ expanded degree 2 round $x=2$ is $50(1.5-\\frac3{16}(x-2)^2).$ Using that approximation, within the half-max width range, the excursion of $\\phi$ is just $50\\cdot\\frac 3{16}\\frac{ln 2}{100}=\\left(\\frac{3\\ln 2}{32\\pi}\\right)\\pi$ radians. Not all that much! Integrating with constant $\\phi$ yields $\\frac{\\sqrt\\pi}{10}\\cdot e^{75i}.$ Integrating with quadratic approx. $\\phi$ yields $\\frac{\\sqrt\\pi}{\\sqrt{100+\\frac3{16}i}}\\cdot e^{75i}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-2","text":"Analyze limits for $\\tan(\\delta)=\\frac{1-\\frac{k'}{k}\\cot k'a\\tan ka}{\\tan ka+\\frac{k'}k\\cot k'a},$ with $k^2=\\frac{2mE}{\\hbar^2}, k'^2=\\frac{2m(V+E)}{\\hbar^2},$ and $z_0=\\frac{2mV_0a^2}{\\hbar^2}.$ As $E\\to 0,$ $k\\to 0$ and $k'\\to \\frac{\\sqrt{2mV}}{\\hbar}.$ Then we can rewrite as $$\\tan(\\delta)=\\frac{k-k'\\cot k'a\\tan ka}{k\\tan ka+k'\\cot k'a}.$$ From this it is clear top $\\to 0$ and bottom $\\to k'\\cot k'a\\neq 0,$ so $\\tan(\\delta)\\to 0.$ As $E\\to\\infty,$ $k'-k\\to 0$$ so $\\cot k'a\\tan ka\\to 1.$ Top $\\to 0$ while bottom $\\to 2k(\\tan ka+\\cot k'a)$ which has norm at least $\\frac14$. Thus $\\tan(\\delta)\\to 0.$ Finally, letting $u=ka$ we have $k'a=\\sqrt{u^2+z_0^2},$ so we get $$\\delta=\\arctan\\left(\\frac{u-\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}\\tan u}{u\\tan u+\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}}\\right).$$","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-3","text":"Solve for phase shift in the potential $V$ equalling $V_0>0$ for $x\\in (0,a),$ $0$ for $x>a,$ and $\\infty$ otherwise. If $E(k)>V_0$ then we get $k^2=\\frac{2mE}{\\hbar^2}, k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ On the right we have $\\sin(kx+\\delta)$ and on the left $\\sin(k_2x)$ so the old $\\psi/\\psi'$ strat yields $$ \\begin{align } k_2\\tan(ak_2) &=k\\tan(ak+\\delta)\\ &=\\frac{k\\tan(ak)+k\\tan(\\delta)}{1-\\tan(ak)\\tan(\\delta)}. \\end{align } $$ Then it's plain to see (from sight expansion) that $$\\tan(\\delta)=\\frac{k_2\\tan(ak_2)-k\\tan(ak)}{k+k_2\\tan(ak_2)\\tan(ak)}.$$ If $E(k)<V_0$ then with $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2},$ left is $\\sinh(\\kappa x)$ and right is $\\sin(kx+\\delta)$ so same thing $$ \\begin{align } \\kappa\\tanh(a\\kappa) &=k\\tan(ak+\\delta).\\ \\end{align } $$ By the same manipulation, $$\\tan(\\delta)=\\frac{\\kappa\\tanh(a\\kappa)-k\\tan(ak)}{k+\\kappa\\tanh(a\\kappa)\\tan(ak)}.$$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-4","text":"Examine phase shift with $V(x)=g\\delta(x-a),$ $g$ of units $J\\cdot m$. On the left $\\sin(kx)$ on the right $\\sin(kx+\\delta),$ as usual. Integrating the Hamiltonian yields $$g\\Psi(a)=\\frac{\\hbar^2}{2m}\\Delta\\left.\\frac d{dx}\\Psi\\right|_a.$$ Matching via $\\sin(ak+\\delta)\\sin(kx)$ and $\\sin(ak)\\sin(kx+\\delta),$ we get $$ \\begin{align } g\\sin(ak)\\sin(ak+\\delta) &=\\frac{\\hbar^2}{2m}\\left(k\\cos(ak+\\delta)\\sin(ak)-k\\cos(ak)\\sin(ak+\\delta)\\right)\\ &=\\frac{k\\hbar^2}{2m}\\sin(ak-(ak+\\delta))\\ &=-\\frac{k\\hbar^2}{2m}\\sin\\delta. \\end{align } $$ Then $$g\\sin^2(ak)\\cos(\\delta)+g\\cos(ak)\\sin(ak)\\sin(\\delta)+\\frac{\\hbar^2}{2m}k\\sin(\\delta)=0.$$ Consequently $$\\tan(\\delta)=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\frac{\\hbar^2}{2amg}ak}=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\lambda ak},$$ using denote $\\lambda=\\frac{\\hbar^2}{2amg}.$ As $ak\\to 0$ we get $\\sin(ak)\\to ak$ so $\\tan(\\delta)\\to -\\frac{ak}{1+\\lambda},$ and as $\\lambda\\to\\infty$ we get $\\tan(\\delta)=0$ for any $ak$.","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-5","text":"Show that $x,y,p_x,p_y$ have zero expectation on a $L_z$ eigenstate. First we compute a bazillion commutators. $L_z=\\frac\\hbar i\\left(x\\frac d{dy}-y\\frac d{dz}\\right).$ Then $$ \\begin{align } [L_z,x]&=\\frac\\hbar i(0)-\\frac\\hbar i(y)=-\\frac\\hbar i y,\\ [L_z,y]&=\\frac\\hbar i(x)-\\frac\\hbar i(0)=\\frac\\hbar i x,\\ [L_z,z]&=0,\\ [L_z,p_x]&=\\frac\\hbar i\\left(-\\frac\\hbar i \\frac d{dy}\\right) - \\frac\\hbar i(0)=-\\frac\\hbar i p_y,\\ [L_z,p_y]&=\\frac\\hbar i(0) - \\frac\\hbar i\\left(-\\frac\\hbar i\\frac d{dx}\\right)=\\frac\\hbar i p_x,\\ [L_z,p_z]&=0. \\end{align } $$ Finally, if $[L_z,\\hat A]=\\hat B,$ then for an eigenstate $L_z\\psi=\\lambda\\psi$ we get $$\\begin{align } \\langle\\psi,\\hat A\\psi\\rangle &=\\langle\\psi,\\hat A\\lambda\\psi\\rangle\\frac1{\\lambda}\\ &=\\langle\\psi,\\hat AL_z\\psi\\rangle\\frac1{\\lambda}\\ &=\\left(\\langle\\psi,L_z\\hat A\\psi\\rangle+\\langle\\psi,[\\hat A,L_z]\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\left(\\langle L_z\\psi,\\hat A\\psi\\rangle+\\langle\\psi,-\\hat B\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\frac1{\\lambda}\\langle L_z\\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle\\ &=\\langle \\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle.\\ \\end{align }$$ Thus $\\langle\\psi,\\hat B\\psi\\rangle=0.$ Thus by the previous commutator computations, the claim is shown.","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-6","text":"Compute $\\bf L^2$ in spherical coordinates. Skipped","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/","text":"Problem 1 Show that $\\psi(x)=e^{i\\delta(k)}\\sin(kx+\\delta(k))$ having a bound-state solution means $A_s=e^{i\\delta}\\sin\\delta$ has a pole for some imaginary value $k=i\\kappa.$ Expand to get $e^{-ikx}+e^{i(kx+2\\delta(k))},$ signflip for convenience to get $e^{ikx}+e^{-i(kx+2\\delta(k))}.$ Then for bound state asymptotically becoming $e^{-\\kappa x}$ we get $k=i\\kappa$ so $e^{-\\kappa x}+e^{\\kappa x-2i\\delta(k)}.$ Then evidently $|e^{2i\\delta(k)}|>|e^{\\kappa x}$ as $x\\to\\infty$ so $e^{i\\delta(k)}$ has a pole there, done. Problem 2 Suppose $\\hat A,\\hat B$ are commuting operators, and spectrum of $\\hat A$ is nondegenerate. Then all eigenstates of $\\hat A$ are the same for $\\hat B.$ Simply, $\\lambda B\\psi=BA\\psi=AB\\psi,$ and $A\\varphi=\\lambda\\varphi$ if and only if $\\varphi=k\\psi,$ so $B\\psi=k\\psi$ and we're done. Problem 3 Determine expectation, uncertainty of $L_z, \\bf{L^2}$ for $$\\psi(r,\\theta,\\phi)=\\frac14\\sqrt{\\frac 5\\pi}\\sin^2\\theta(1+\\sqrt{14}\\cos\\theta)\\cos2\\phi f(r).$$ Well, using Wikipedia's handy spherical harmonic lookup table we get $$\\psi=\\frac1{\\sqrt 6}(Y_2^{-2}+Y_2^2)+\\frac1{\\sqrt 3}(Y_3^{-2}+Y_3^2).$$ Eigenvalue of $L_z$ is $m,$ $\\bf L^2$ is $l(l+1).$ Then $L_z$ is even chance $-2$ or $2$, and $\\bf L^2$ is $\\frac13$ chance $6,$ $\\frac23$ chance $12.$ Expectations are $0,10;$ uncertainties are $2,2\\sqrt 2.$ Problem 4 Consider $l=0$ states of an infinite/finite spherical well $V(r)$, $0$ for $r<a$ and $\\infty$ otherwise. Resulting equation for $u=r\\psi(r)$ is $$(-\\frac{\\hbar^2}{2m}\\nabla^2+V)u=Eu.$$ This is indistinguishable from time-independent Schrodinger's. Usual solving yields $k^2=\\frac{2mE}{\\hbar^2},$ and $u$ vanishes at $r=0$ so $u=\\sin(k), u(a)=0$ so $ak=n\\pi,$ etc.etc. Yields same quantized energies as infinite well potential. For a finite spherical well $V$ equalling $-V_0$ for small $r$ and $0$ for large $r$, stipulation that $u(a)=0$ yields $k^2=\\frac{2m(V-E)}{\\hbar^2},$ $\\kappa^2=\\frac{2mE}{\\hbar^2},$ and by usual derivative/value matching $ak\\cot(ak)=-a\\kappa.$ Note also that $(ak)^2+(a\\kappa)^2=\\frac{2V_0ma^2}{\\hbar^2}.$ By Desmos inspection/some weird casework it's clear $(ak)^2+(a\\kappa)^2\\geq\\frac{\\pi^2}4$ at $\\kappa=0,ak=\\frac\\pi 2,$ so $\\frac{2V_0ma^2}{\\hbar^2}\\geq\\frac{\\pi^2}4,$ the result follows. Problem 5 SKIPPED Problem 6 Examine the Virial Theorem $\\langle T\\rangle=-\\frac12\\langle V\\rangle.$ IDK bro just do shit lol. Problem 7 .","title":"8.04.18 Pset 10"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-1","text":"Show that $\\psi(x)=e^{i\\delta(k)}\\sin(kx+\\delta(k))$ having a bound-state solution means $A_s=e^{i\\delta}\\sin\\delta$ has a pole for some imaginary value $k=i\\kappa.$ Expand to get $e^{-ikx}+e^{i(kx+2\\delta(k))},$ signflip for convenience to get $e^{ikx}+e^{-i(kx+2\\delta(k))}.$ Then for bound state asymptotically becoming $e^{-\\kappa x}$ we get $k=i\\kappa$ so $e^{-\\kappa x}+e^{\\kappa x-2i\\delta(k)}.$ Then evidently $|e^{2i\\delta(k)}|>|e^{\\kappa x}$ as $x\\to\\infty$ so $e^{i\\delta(k)}$ has a pole there, done.","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-2","text":"Suppose $\\hat A,\\hat B$ are commuting operators, and spectrum of $\\hat A$ is nondegenerate. Then all eigenstates of $\\hat A$ are the same for $\\hat B.$ Simply, $\\lambda B\\psi=BA\\psi=AB\\psi,$ and $A\\varphi=\\lambda\\varphi$ if and only if $\\varphi=k\\psi,$ so $B\\psi=k\\psi$ and we're done.","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-3","text":"Determine expectation, uncertainty of $L_z, \\bf{L^2}$ for $$\\psi(r,\\theta,\\phi)=\\frac14\\sqrt{\\frac 5\\pi}\\sin^2\\theta(1+\\sqrt{14}\\cos\\theta)\\cos2\\phi f(r).$$ Well, using Wikipedia's handy spherical harmonic lookup table we get $$\\psi=\\frac1{\\sqrt 6}(Y_2^{-2}+Y_2^2)+\\frac1{\\sqrt 3}(Y_3^{-2}+Y_3^2).$$ Eigenvalue of $L_z$ is $m,$ $\\bf L^2$ is $l(l+1).$ Then $L_z$ is even chance $-2$ or $2$, and $\\bf L^2$ is $\\frac13$ chance $6,$ $\\frac23$ chance $12.$ Expectations are $0,10;$ uncertainties are $2,2\\sqrt 2.$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-4","text":"Consider $l=0$ states of an infinite/finite spherical well $V(r)$, $0$ for $r<a$ and $\\infty$ otherwise. Resulting equation for $u=r\\psi(r)$ is $$(-\\frac{\\hbar^2}{2m}\\nabla^2+V)u=Eu.$$ This is indistinguishable from time-independent Schrodinger's. Usual solving yields $k^2=\\frac{2mE}{\\hbar^2},$ and $u$ vanishes at $r=0$ so $u=\\sin(k), u(a)=0$ so $ak=n\\pi,$ etc.etc. Yields same quantized energies as infinite well potential. For a finite spherical well $V$ equalling $-V_0$ for small $r$ and $0$ for large $r$, stipulation that $u(a)=0$ yields $k^2=\\frac{2m(V-E)}{\\hbar^2},$ $\\kappa^2=\\frac{2mE}{\\hbar^2},$ and by usual derivative/value matching $ak\\cot(ak)=-a\\kappa.$ Note also that $(ak)^2+(a\\kappa)^2=\\frac{2V_0ma^2}{\\hbar^2}.$ By Desmos inspection/some weird casework it's clear $(ak)^2+(a\\kappa)^2\\geq\\frac{\\pi^2}4$ at $\\kappa=0,ak=\\frac\\pi 2,$ so $\\frac{2V_0ma^2}{\\hbar^2}\\geq\\frac{\\pi^2}4,$ the result follows.","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-5","text":"SKIPPED","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-6","text":"Examine the Virial Theorem $\\langle T\\rangle=-\\frac12\\langle V\\rangle.$ IDK bro just do shit lol.","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-7","text":".","title":"Problem 7"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/","text":"We solve Hydrogen! LFG Setup Wavefunction $\\Psi(x_p,x_e),$ probability of finding in neighborhood. - Usual canonical operators $\\hat x_p,\\hat p_p, \\hat x_e,\\hat p_e;$ pairwise commutator $i\\hbar,$ $p,e$ are independent from each other - Squared norm integral in $\\mathbb R^3\\otimes\\mathbb R^3$ is 1 The Hamiltonian is $\\frac{p_p^2}{2m_p}+\\frac{p_e^2}{2m_e}+V(|x_e-x_p|),$ which is rather unwieldy because of the terrible $V$ making it not separable. Let's separate degrees of freedom, having COM (upper) and relative (lower). Using $M=m_e+m_p,$ canonical commutator restrictions yield $$\\begin{align }X&=\\frac{m_ex_e+m_px_p}{M},\\x&=x_e-x_p,\\P&=p_e+p_p,\\p&=\\frac{m_e}Mp_e-\\frac{m_p}Mp_p.\\end{align }$$ Rewrite in $\\Psi(X,x).$ Now $p_p=\\frac{m_p}MP-p, p_e=\\frac{m_e}MP+p,$ Hamiltonian becomes $\\frac{P^2}{2M}+\\frac{Mp^2}{2m_pm_e}.$ Using $\\mu=\\frac{m_pm_e}{M},$ we get $$\\hat H=\\frac{P^2}{2M}+\\frac{p^2}{2\\mu}+V(x).$$ Letting $\\Psi(X,x)=\\Psi_{CM}(X)\\Psi_{rel}(x),$ we get $$\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}+\\frac1{\\Psi_{rel}}\\left(\\frac{p^2}{2\\mu}+V(x)\\right)\\Psi_{rel}=E.$$ Now we can separate into $E_{CM}$ and $E_{rel},$ which must sum to $E$. Analysis We use potential $V(x)=\\frac{Zke^2}{r},$ where $Z$ is # protons, $ke^2=K$ where $e$ electron charge, $k$ is Coulomb's const. To be safe, nondim. with $a_0=\\frac{\\hbar^2}{mke^2}\\approx 53$pm. Here $m=m_e\\approx\\mu,$ since $m_e\\ll m_p$. First equation $\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}=E_{CM}$ with $P=\\frac\\hbar i\\nabla_X$ resolves to $\\Psi_{CM}=e^{i(X\\cdot v)}$ for some vector $v\\cdot v=\\frac{2ME_{CM}}{\\hbar^2},$ as usual. Second equation is harder. Nondim. with $r=\\frac{a_0}{2Z}x.$ Then recall our radial ansatz $u=r\\Psi,$ $$\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+\\frac{\\hbar^2l(l+1)}{2mr^2}+\\frac{Zke^2}r\\right)u=Eu.$$ Nondim. and cancelling yields $$\\left(-\\frac{2Z^2\\hbar^2}{ma_0^2}\\nabla^2_x+\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac{l(l+1)}{x^2}-\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac1x\\right)u=Eu.$$ Then let $\\kappa^2=-\\frac{E}{2Z^2\\hbar^2/ma_0^2}$ so $$\\left(-\\nabla^2+\\frac{l(l+1)}{x^2}-\\frac1x\\right)u=-\\kappa^2 u.$$ In the limit $x\\to\\infty$ we get $u\\sim e^{\\pm \\kappa x}$. For $x\\to 0$ we must have $\\nabla^2=\\frac{l(l+1)}{x^2}$ so $u\\sim x^{l+1}$ there. Then $\\rho=\\kappa x$ and $u=e^{-\\rho}\\rho^{l+1}W$ yields $$\\rho W''+2(l+1-\\rho)W'+\\left(\\frac1\\kappa-2(l+1)\\right)W=0.$$ Series expansion $W=\\sum_i a_ix^i$ yields at degree $k$ $$(k+1)ka_{k+1}+2(l+1)(k+1)a_{k+1}-2ka_{k}+\\left(\\frac1\\kappa-2(l+1)\\right)a_k=0,$$ $$\\frac{a_{k+1}}{a_k}=\\frac{2(l+k+1)-\\frac1\\kappa}{2l+2+k}.$$ If this never terminates then $u$ is not tight enough, so we need $\\kappa=\\frac1{2(l+k+1)}.$ Set $n=l+k+1$ for some $k$. Then $E=-\\frac{Z^2\\hbar^2}{2ma_0^2n^2}.$ Finally our ansatz is $$ \\begin{align } \\Psi_{rel}=R(r)Y_{lm}&=\\mathcal N\\rho^le^{-\\rho}W_{nl}(\\rho)Y_{lm}(\\theta,\\phi)\\ &=\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi), \\end{align } $$ $$\\Psi=\\Psi_{CM}\\Psi_{rel}=e^{i(X\\cdot v)}\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi).$$","title":"8.04.18 Solving Hydrogen"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/#setup","text":"Wavefunction $\\Psi(x_p,x_e),$ probability of finding in neighborhood. - Usual canonical operators $\\hat x_p,\\hat p_p, \\hat x_e,\\hat p_e;$ pairwise commutator $i\\hbar,$ $p,e$ are independent from each other - Squared norm integral in $\\mathbb R^3\\otimes\\mathbb R^3$ is 1 The Hamiltonian is $\\frac{p_p^2}{2m_p}+\\frac{p_e^2}{2m_e}+V(|x_e-x_p|),$ which is rather unwieldy because of the terrible $V$ making it not separable. Let's separate degrees of freedom, having COM (upper) and relative (lower). Using $M=m_e+m_p,$ canonical commutator restrictions yield $$\\begin{align }X&=\\frac{m_ex_e+m_px_p}{M},\\x&=x_e-x_p,\\P&=p_e+p_p,\\p&=\\frac{m_e}Mp_e-\\frac{m_p}Mp_p.\\end{align }$$ Rewrite in $\\Psi(X,x).$ Now $p_p=\\frac{m_p}MP-p, p_e=\\frac{m_e}MP+p,$ Hamiltonian becomes $\\frac{P^2}{2M}+\\frac{Mp^2}{2m_pm_e}.$ Using $\\mu=\\frac{m_pm_e}{M},$ we get $$\\hat H=\\frac{P^2}{2M}+\\frac{p^2}{2\\mu}+V(x).$$ Letting $\\Psi(X,x)=\\Psi_{CM}(X)\\Psi_{rel}(x),$ we get $$\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}+\\frac1{\\Psi_{rel}}\\left(\\frac{p^2}{2\\mu}+V(x)\\right)\\Psi_{rel}=E.$$ Now we can separate into $E_{CM}$ and $E_{rel},$ which must sum to $E$.","title":"Setup"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/#analysis","text":"We use potential $V(x)=\\frac{Zke^2}{r},$ where $Z$ is # protons, $ke^2=K$ where $e$ electron charge, $k$ is Coulomb's const. To be safe, nondim. with $a_0=\\frac{\\hbar^2}{mke^2}\\approx 53$pm. Here $m=m_e\\approx\\mu,$ since $m_e\\ll m_p$. First equation $\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}=E_{CM}$ with $P=\\frac\\hbar i\\nabla_X$ resolves to $\\Psi_{CM}=e^{i(X\\cdot v)}$ for some vector $v\\cdot v=\\frac{2ME_{CM}}{\\hbar^2},$ as usual. Second equation is harder. Nondim. with $r=\\frac{a_0}{2Z}x.$ Then recall our radial ansatz $u=r\\Psi,$ $$\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+\\frac{\\hbar^2l(l+1)}{2mr^2}+\\frac{Zke^2}r\\right)u=Eu.$$ Nondim. and cancelling yields $$\\left(-\\frac{2Z^2\\hbar^2}{ma_0^2}\\nabla^2_x+\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac{l(l+1)}{x^2}-\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac1x\\right)u=Eu.$$ Then let $\\kappa^2=-\\frac{E}{2Z^2\\hbar^2/ma_0^2}$ so $$\\left(-\\nabla^2+\\frac{l(l+1)}{x^2}-\\frac1x\\right)u=-\\kappa^2 u.$$ In the limit $x\\to\\infty$ we get $u\\sim e^{\\pm \\kappa x}$. For $x\\to 0$ we must have $\\nabla^2=\\frac{l(l+1)}{x^2}$ so $u\\sim x^{l+1}$ there. Then $\\rho=\\kappa x$ and $u=e^{-\\rho}\\rho^{l+1}W$ yields $$\\rho W''+2(l+1-\\rho)W'+\\left(\\frac1\\kappa-2(l+1)\\right)W=0.$$ Series expansion $W=\\sum_i a_ix^i$ yields at degree $k$ $$(k+1)ka_{k+1}+2(l+1)(k+1)a_{k+1}-2ka_{k}+\\left(\\frac1\\kappa-2(l+1)\\right)a_k=0,$$ $$\\frac{a_{k+1}}{a_k}=\\frac{2(l+k+1)-\\frac1\\kappa}{2l+2+k}.$$ If this never terminates then $u$ is not tight enough, so we need $\\kappa=\\frac1{2(l+k+1)}.$ Set $n=l+k+1$ for some $k$. Then $E=-\\frac{Z^2\\hbar^2}{2ma_0^2n^2}.$ Finally our ansatz is $$ \\begin{align } \\Psi_{rel}=R(r)Y_{lm}&=\\mathcal N\\rho^le^{-\\rho}W_{nl}(\\rho)Y_{lm}(\\theta,\\phi)\\ &=\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi), \\end{align } $$ $$\\Psi=\\Psi_{CM}\\Psi_{rel}=e^{i(X\\cdot v)}\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi).$$","title":"Analysis"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.2%20Dualities%20of%20Light%20and%20Matter/","text":"Effectively, everything is both a wave and a particle. Lots of fun. Formulas: - Photon energy $E=h\\nu$ - ==Compton wavelength is the wavelength for a photon whose energy equals the particle's rest energy ==: $mc^2=h\\nu=h\\frac c\\lambda,$ so $\\lambda_C=\\frac h{mc}$ - As Compton wavelength is wavelength of photon with the same rest energy, ==de Broglie wavelength is that of photon with the same momentum .== - A matter particle has an approximate de Broglie wavelength $\\lambda=\\frac hp.$ - Relativity momentum: $E^2-p^2c^2=m^2c^4.$ Thus $E=\\sqrt{m^2c^4+p^2c^2,}$ and $\\Delta E=E-mc^2\\approx \\frac12{p^2}m$ as expected. - As a result, momentum and energy both have a $\\frac1{\\sqrt{1-\\frac{v^2}{c^2}}}$ factor on top of the classical $mc^2$ and $mv$. Photoelectron scattering is what happens when a photon hits an atom and ionizes it. By classical EM intuition it's just an electron that begins oscillating in a field, but that don't really work for photons at energies higher than potential holding electrons. Using relativistic momentum and energy conservation, Compton scattering yields new photon wavelength $\\lambda_f=\\lambda_i+\\lambda_C(1-\\cos\\theta),$ where $\\theta$ is directional angle change and $\\lambda_C$ is Compton wavelength for the electron. $\\require{mhchem}\\newcommand{\\CC}{\\mathbb C}$-latex","title":"8.04.2 Dualities of Light and Matter"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/","text":"de Broglie wavelengths We can take a normal Galilean transformation $$x'=x-vt, t'=t.$$ Notice that since $p=mv,$ and $\\lambda=\\frac hp,$ so the de Broglie wavelength actually changes with an inertial frame of reference change. Understandably, the observable, time-independently measurable phase $\\phi=kx-\\omega t$ is a Galilean invariant. [!TIP] Harmonic equations and relations The primitives are $v$ velocity and $\\lambda$ complete wavelength. $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda$$ ==> Here $\\nu$ is complete cycle frequency while $\\omega$ is the angular frequency, and $k$ is the wavenumber (inverse distance units), meaning one would write $\\sin(kx).$== Recall also the Planck constant $h$ and the reduced $\\hbar$. [!info] de Broglie relations $$p=\\hbar k,\\quad\\lambda=\\frac{2\\pi}k=\\frac hp.$$ Group Velocities We have a packet of waves traveling together. For each wavenumber $k$ there is an associated frequency $\\omega(k)$ and a magnitude $\\Phi(k).$ At each point in time $t,$ for each location $x$ we stack all the waves and their amplitudes at that point by computing $$\\psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ We can try to figure out where this is nonnegligible, determining the location of the \"body\" of the wave packet. Assume that $\\Phi(k)$ has a large peak at $k_0.$ We need $i(kx-\\omega(k)t)$ to be stationary at $k_0$, which yields $\\frac xt=\\frac{\\partial \\omega}{\\partial k}.$ Thus there is a group velocity of $\\frac{\\partial \\omega}{\\partial k},$ the speed at which the apparent bulk of the wave moves. Wavefunctions We can have candidate wavefunctions. Given $E,p$ we can determine $k,\\omega,$ sure. We could try $\\cos(kx\\pm\\omega t,)$ but we have the constraint that [!warning] Matter wavefunction constraint Under any superposition, $\\nexists t\\colon \\forall x, \\psi(x,t)=0.$ Turns out that $\\cos(kx-\\omega t)+\\cos(kx+\\omega t)=2\\cos(kx)\\cos(\\omega t),$ clearly fails. Therefore it has to be $e^{i(\\pm kx\\pm\\omega t)};$ sticking to the above convention, we have [!important] Matter wavefunction formula $$\\Psi(x,t)=e^{i(kx-\\omega t)}.$$ This allows the wave to propogate forward ; $kx'-\\omega t'=kx-\\omega t\\implies k\\Delta x=\\omega\\Delta t.$","title":"8.04.3 Matter Waves"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#de-broglie-wavelengths","text":"We can take a normal Galilean transformation $$x'=x-vt, t'=t.$$ Notice that since $p=mv,$ and $\\lambda=\\frac hp,$ so the de Broglie wavelength actually changes with an inertial frame of reference change. Understandably, the observable, time-independently measurable phase $\\phi=kx-\\omega t$ is a Galilean invariant. [!TIP] Harmonic equations and relations The primitives are $v$ velocity and $\\lambda$ complete wavelength. $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda$$ ==> Here $\\nu$ is complete cycle frequency while $\\omega$ is the angular frequency, and $k$ is the wavenumber (inverse distance units), meaning one would write $\\sin(kx).$== Recall also the Planck constant $h$ and the reduced $\\hbar$. [!info] de Broglie relations $$p=\\hbar k,\\quad\\lambda=\\frac{2\\pi}k=\\frac hp.$$","title":"de Broglie wavelengths"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#group-velocities","text":"We have a packet of waves traveling together. For each wavenumber $k$ there is an associated frequency $\\omega(k)$ and a magnitude $\\Phi(k).$ At each point in time $t,$ for each location $x$ we stack all the waves and their amplitudes at that point by computing $$\\psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ We can try to figure out where this is nonnegligible, determining the location of the \"body\" of the wave packet. Assume that $\\Phi(k)$ has a large peak at $k_0.$ We need $i(kx-\\omega(k)t)$ to be stationary at $k_0$, which yields $\\frac xt=\\frac{\\partial \\omega}{\\partial k}.$ Thus there is a group velocity of $\\frac{\\partial \\omega}{\\partial k},$ the speed at which the apparent bulk of the wave moves.","title":"Group Velocities"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#wavefunctions","text":"We can have candidate wavefunctions. Given $E,p$ we can determine $k,\\omega,$ sure. We could try $\\cos(kx\\pm\\omega t,)$ but we have the constraint that [!warning] Matter wavefunction constraint Under any superposition, $\\nexists t\\colon \\forall x, \\psi(x,t)=0.$ Turns out that $\\cos(kx-\\omega t)+\\cos(kx+\\omega t)=2\\cos(kx)\\cos(\\omega t),$ clearly fails. Therefore it has to be $e^{i(\\pm kx\\pm\\omega t)};$ sticking to the above convention, we have [!important] Matter wavefunction formula $$\\Psi(x,t)=e^{i(kx-\\omega t)}.$$ This allows the wave to propogate forward ; $kx'-\\omega t'=kx-\\omega t\\implies k\\Delta x=\\omega\\Delta t.$","title":"Wavefunctions"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.4%20Schrodinger%27s%20and%20Wavefunctions/","text":"[!FAQ] What is the wavefunction? It's a function returning the likelihood of finding a particle in the neighborhood . Specifically, $P(\\bf x)=|\\Psi(x)|^2.$ The Schrodinger equation will, in fact, preserve $\\int |\\Psi(\\bf x)|^2\\,d\\bf x^3=1.$ In one dimension, at non-relativistic speeds, since the particle is of the form $$e^{i(kx-\\omega t)}$$ and the momentum, energy are $p=\\hbar k$ and $E=\\hbar\\omega=\\frac{p^2}{2m},$ we get the operator $$\\hat p=\\frac{\\hbar}i\\frac{\\partial}{\\partial x}.$$ Then $$\\hat E\\Psi=\\hbar\\omega=i\\hbar\\frac{\\partial}{\\partial t}\\Psi,$$ but also $$\\hat E\\Psi=\\frac{p^2}{2m}\\Psi=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi.$$ Then we get the [!important] Free Particle Schrodinger's Equation $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi.$$ This holds for all free particles of mass m regardless of speed or frequency. Now if we add a potential energy, we now have $E=\\frac{p^2}{2m}+V$ (the first term is kinetic energy). Furthermore we can extend to three dimensions using the Laplacian and del , yielding [!important] Schrodinger's Equation Where both $V$ and $\\Psi$ are functions of $\\textbf{x}$ and $t,$ $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+V\\right)\\Psi.$$ Note that this is linear in $\\Psi$ and first order in $t$. We also have the $\\hat x=x$ operator (basically just an abstract formalization). Then we can compute commutators $[\\hat A,\\hat B]=\\hat A\\hat B-\\hat B\\hat A.$ It turns out that $[\\hat x,\\hat p]=i\\hbar.$ Quantum Linear Algebra Operators Matrices Wavefunctions/States Vectors Eigenstates Eigenvectors","title":"8.04.4 Schrodinger's and Wavefunctions"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/","text":"$\\require{mhchem}\\newcommand{\\CC}{\\mathbb C} \\newcommand{\\SN}{\\mathcal N} \\newcommand{\\Ham}{\\hat H}\\DeclareMathOperator{\\IM}{Im}$ As $\\Psi$ is a probability we can say at any time $t,$ the whole space $\\int |\\Psi|^2\\,dx=1.$ - This is a nice convention to have, but not necessary; at any time we can just scale by $\\mathcal N=\\int |\\Psi|^2\\,dx,\\Psi'=\\frac1{\\sqrt{\\mathcal N}}\\Psi.$ - In fact our previous wavefunction for the free particle doesn't work for this reason - We can superimpose non-square-integrable functions and arrive at a square-integrable function - In general we would like $\\int|\\Psi|^2\\,dx$ to exist. Total Amplitude Conservation We need two conditions: [!FAQ] Two necessary conditions It is sufficient for $$\\lim_{x\\to\\infty}\\frac{\\partial\\Psi}{\\partial x}<\\infty,\\qquad \\lim_{x\\to\\infty}\\Psi=0.$$ Let $\\rho(x,t)=\\Psi(x,t)\\Psi(x,t)^ ,$ and let $\\SN(t)\\equiv\\int\\rho(x,t)\\,dx.$ We want to show $\\frac{\\partial}{\\partial t}\\SN=0.$ Equivalently, $$\\int \\frac{\\partial\\Psi}{\\partial t}\\Psi^ +\\frac{\\partial\\Psi^ }{\\partial t}\\Psi\\,dx=0,$$ but we can use the Schrodinger equation and get $$\\int \\Psi^ \\cdot-\\frac i{\\hbar}\\Ham\\Psi+\\Psi\\cdot\\frac i{\\hbar}\\Ham\\Psi^ \\,dx=0,$$ equivalently $$\\int \\Psi^ \\Ham\\Psi\\,dx=\\int \\Psi\\Ham\\Psi^*\\,dx.$$ [!important] Hermitian operator In the case that $$\\int (\\Ham\\Psi_1)^ \\Psi_2\\,dx=\\int\\Psi_1^ (\\Ham\\Psi_2)\\,dx,$$ we call $\\Ham$ a Hermitian operator. In general a linear operator $T$ will have a conjugate $T^\\dagger$ that satisfies the above property, and $T$ is Hermitian if $T=T^\\dagger.$ Hamiltonian is Hermitian Expanding the expression $$\\frac i\\hbar\\left(\\Psi\\Ham\\Psi^ -\\Psi^ \\Ham\\Psi\\right)=-\\frac {i\\hbar}{2m}\\left(\\Psi\\nabla^2\\Psi^ -\\Psi^ \\nabla^2\\Psi\\right)=-\\frac{\\hbar}m\\nabla\\cdot\\left[\\frac1{2i}\\left(\\Psi^ (\\nabla\\Psi)-\\Psi(\\nabla \\Psi^ )\\right)\\right].$$ The conclusion is then $$\\frac{\\partial\\rho}{\\partial t}+\\frac\\hbar m\\nabla\\cdot\\text{Im}\\left(\\Psi^ (\\nabla\\Psi)\\right)=0.$$ Then letting $J=\\frac\\hbar m \\IM(\\Psi^ (\\nabla\\Psi))$ we get $\\frac{\\partial\\rho}{\\partial t}+\\frac{\\partial J}{\\partial x}=0,$ and so $$\\frac{\\partial \\SN}{\\partial t}=\\int\\frac{\\partial\\rho}{\\partial t}\\,dx=\\int\\frac{\\partial J}{\\partial x}\\,dx=\\left. \\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))\\right|_{-\\infty}^{\\infty}=0,$$ on the earlier assumption. [!important] Probability current $J=\\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))$ is the \"probability current,\" meaning that (1) in 1 dimension, $J$ is the ROC of the cumulative probability distribution (2) in 3 dimensions, flux integral of $J$ represents ROC in that region Item EM Quantum $\\rho$ charge density probability density $Q_V$ charge in volume probability in volume $\\bf J$ current flow density probability flow density","title":"8.04.5 Wavefunction as Probability"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/#total-amplitude-conservation","text":"We need two conditions: [!FAQ] Two necessary conditions It is sufficient for $$\\lim_{x\\to\\infty}\\frac{\\partial\\Psi}{\\partial x}<\\infty,\\qquad \\lim_{x\\to\\infty}\\Psi=0.$$ Let $\\rho(x,t)=\\Psi(x,t)\\Psi(x,t)^ ,$ and let $\\SN(t)\\equiv\\int\\rho(x,t)\\,dx.$ We want to show $\\frac{\\partial}{\\partial t}\\SN=0.$ Equivalently, $$\\int \\frac{\\partial\\Psi}{\\partial t}\\Psi^ +\\frac{\\partial\\Psi^ }{\\partial t}\\Psi\\,dx=0,$$ but we can use the Schrodinger equation and get $$\\int \\Psi^ \\cdot-\\frac i{\\hbar}\\Ham\\Psi+\\Psi\\cdot\\frac i{\\hbar}\\Ham\\Psi^ \\,dx=0,$$ equivalently $$\\int \\Psi^ \\Ham\\Psi\\,dx=\\int \\Psi\\Ham\\Psi^*\\,dx.$$ [!important] Hermitian operator In the case that $$\\int (\\Ham\\Psi_1)^ \\Psi_2\\,dx=\\int\\Psi_1^ (\\Ham\\Psi_2)\\,dx,$$ we call $\\Ham$ a Hermitian operator. In general a linear operator $T$ will have a conjugate $T^\\dagger$ that satisfies the above property, and $T$ is Hermitian if $T=T^\\dagger.$","title":"Total Amplitude Conservation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/#hamiltonian-is-hermitian","text":"Expanding the expression $$\\frac i\\hbar\\left(\\Psi\\Ham\\Psi^ -\\Psi^ \\Ham\\Psi\\right)=-\\frac {i\\hbar}{2m}\\left(\\Psi\\nabla^2\\Psi^ -\\Psi^ \\nabla^2\\Psi\\right)=-\\frac{\\hbar}m\\nabla\\cdot\\left[\\frac1{2i}\\left(\\Psi^ (\\nabla\\Psi)-\\Psi(\\nabla \\Psi^ )\\right)\\right].$$ The conclusion is then $$\\frac{\\partial\\rho}{\\partial t}+\\frac\\hbar m\\nabla\\cdot\\text{Im}\\left(\\Psi^ (\\nabla\\Psi)\\right)=0.$$ Then letting $J=\\frac\\hbar m \\IM(\\Psi^ (\\nabla\\Psi))$ we get $\\frac{\\partial\\rho}{\\partial t}+\\frac{\\partial J}{\\partial x}=0,$ and so $$\\frac{\\partial \\SN}{\\partial t}=\\int\\frac{\\partial\\rho}{\\partial t}\\,dx=\\int\\frac{\\partial J}{\\partial x}\\,dx=\\left. \\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))\\right|_{-\\infty}^{\\infty}=0,$$ on the earlier assumption. [!important] Probability current $J=\\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))$ is the \"probability current,\" meaning that (1) in 1 dimension, $J$ is the ROC of the cumulative probability distribution (2) in 3 dimensions, flux integral of $J$ represents ROC in that region Item EM Quantum $\\rho$ charge density probability density $Q_V$ charge in volume probability in volume $\\bf J$ current flow density probability flow density","title":"Hamiltonian is Hermitian"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.6%20Uncertainty%20and%20Fourier%20Inverse/","text":"When we have a wavepacket at fixed time $t=0$ $$\\Psi(x,0)=\\int\\Phi(k)e^{ikx}\\,dk$$ we get the relation [!important] Heisenberg Uncertainty $$\\Delta x\\Delta k\\geq\\frac12.$$ More specifically, $p=\\hbar k$ so $\\Delta x\\Delta p\\geq\\frac\\hbar 2.$ Recall the earlier exercise where we expanded $\\omega(k)$ to first order; now expanding to second order (suspicious) $$\\frac{d\\omega}{dk}=\\frac{dE}{dp}=\\frac pm=\\frac{\\hbar k}m.$$ This seems to interchange between de Broglie and Compton, but we won't worry about that. Then $\\frac{d^2\\omega}{d k^2}=\\frac\\hbar m$. We want to see how far we can go without destroying the linear $\\omega$ estimate; i.e. how far for $\\omega(k)t$ to be perturbed $O(1),$ i.e. $(\\nabla k)^2\\frac\\hbar m t\\ll1,$ or $t\\ll\\frac{m\\hbar}{(\\nabla p)^2}.$ We can also use $\\nabla x\\nabla p\\approx \\hbar$ to get $\\frac{\\nabla p}mt\\ll\\nabla x.$ This makes sense. Fourier inverse Given $\\Psi(x,0)$ we can get [!faq] Fourier Inverse $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ Then we can sorta hack with $E=\\hbar\\omega=\\frac{p^2}{2m}=\\frac{\\hbar^2k^2}{2m}$ to get $\\omega=\\frac{\\hbar k^2}{2m}.$ Then we can reconstruct all $\\Psi(x,t).$","title":"8.04.6 Uncertainty and Fourier Inverse"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.6%20Uncertainty%20and%20Fourier%20Inverse/#fourier-inverse","text":"Given $\\Psi(x,0)$ we can get [!faq] Fourier Inverse $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ Then we can sorta hack with $E=\\hbar\\omega=\\frac{p^2}{2m}=\\frac{\\hbar^2k^2}{2m}$ to get $\\omega=\\frac{\\hbar k^2}{2m}.$ Then we can reconstruct all $\\Psi(x,t).$","title":"Fourier inverse"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/","text":"Typically, Fourier Inversion tells us $$\\hat f(\\xi)=\\int f(x)e^{-2\\pi i\\xi x}\\,dx\\implies f(x)=\\int \\hat f(\\xi)e^{2\\pi i\\xi x}\\,d\\xi.$$ If we take $\\xi=\\frac1{2\\pi}k$ then $\\hat f(k)=\\int f(x)e^{-ikx},$ $f(x)=\\frac1{2\\pi}\\int f(x)e^{ikx}\\,dk.$ Splitting this factor and changing things around, we can use [!important] QM coordinate space Fourier transform $$\\Psi(x)=\\frac1{\\sqrt{2\\pi}}\\int \\Phi(x)e^{ikx}\\,dk,$$ $$\\Phi(k)=\\frac1{\\sqrt{2\\pi}}\\int \\Psi(x)e^{-ikx}\\,dx.$$ Through some interesting manipulation, one can arrive at $\\delta(k)=\\frac1{2\\pi}\\int e^{i kx}\\,dx.$ Just a broken version of the usual $\\delta(\\xi)=\\int e^{2\\pi i x\\xi}\\,dx.$ Now if we want to rewrite in terms of momentum we do something similar; now $k=\\frac1\\hbar p,$ so distributing that factor yields [!important] QM momentum space Fourier transform $$\\Psi(x)=\\frac1{\\sqrt{2\\pi\\hbar}}\\int \\Phi(p)e^{ipx/\\hbar}\\,dp,$$ $$\\Phi(p)=\\frac1{\\sqrt{2\\pi\\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar}\\,dx.$$ Then by Parseval/Plancherel's theorem $\\int |f|^2\\,dx=\\int|\\hat f|^2\\,d\\xi,$ we get $$\\int|\\Psi|^2\\,dx=\\int|\\Phi|^2\\,dp.$$ Then $\\Phi$ denotes probability distribution of getting that particular momentum. If we take the wavenumber interpretation, $\\Phi$ becomes distribution of getting that particular wavenumber. Expectation Pretty simple, really. $$\\mathbb E(Q)=\\int Q(x)P(x)\\,dx=\\int Q(x)|\\Psi(x)|^2\\,dx=\\int \\Psi^ Q\\Psi\\,dx.$$ Then we have \"joint probabilities\" $\\langle f|g\\rangle$ which is basically $\\int f^ g\\,dx$. We denote expectation using $\\langle Q\\rangle.$ Time derivative plugged into Schrodinger's yields [!faq] ROC of Expectation $$\\frac{\\partial}{\\partial t}\\langle Q\\rangle=\\frac1{i\\hbar}\\left\\langle[\\hat Q,\\hat H]\\right\\rangle.$$ Uncertainty Computation We can now even plug-and-chug uncertainty, as follows. Proof 1 (smarter) First note that for arbitrary $u,v$ and constant $t,$ $\\int (u+tv)^2=\\int u^2+2t\\int uv+t^2\\int v^2\\geq 0,$ so the discriminant in $t$ must be nonpositive i.e. $(2\\int uv)^2\\leq 4(\\int u^2)(\\int v^2),$ yielding [!c] Cauchy-Schwarz Inequality $$\\int u^2\\int v^2\\geq \\int uv.$$ Recall that $A$ is Hermitian if $\\int \\phi^ A\\psi=\\int\\psi^ A\\phi.$ Now let $A_0$ be the expected for $A$, and $B_0$ the same for $B$. Assume that $A_0=B_0=0.$ We will not use this condition, and are justified because $\\mathbb E[(X-c)^2]$ in general is minimized when $c=\\mathbb E[X],$ so we don't lose information by moving the distribution/assuming the mean is $0$ and not using that condition later. Then $\\sigma_A\\sigma_b=\\langle |A|^2\\rangle\\langle |B|^2\\rangle.$ Note that $\\langle |A|^2\\rangle=\\int \\Psi^ A^ A\\Psi=\\int|A\\Psi|^2.$ Then using Cauchy's from above, $$ \\begin{aligned} \\int|A\\Psi|^2\\int|B\\Psi^2| &\\geq\\int|A\\Psi||B\\Psi|\\ &\\geq\\int\\left|\\text{Im}((A\\Psi)^ B\\Psi)\\right|\\ &\\geq\\left|\\text{Im}\\left(\\int(A\\Psi)^ B\\Psi\\right)\\right|\\ &=\\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right|. \\end{aligned} $$ Now because $A,B$ are Hermitian, $$ \\begin{aligned} \\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right| &=\\left|\\frac{\\int \\Psi^ BA\\Psi-\\Psi^ AB\\Psi}{2i}\\right|\\ &=\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. \\end{aligned} $$ Thus we've arrived at [!important] Robertson Uncertainty If $A,B$ are Hermitian operators, $$\\sigma_A\\sigma_B\\geq\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. $$ Finally, $$[\\hat x,\\hat p]=x\\frac\\hbar i\\frac{\\partial}{\\partial x}-\\frac\\hbar i\\frac{\\partial}{\\partial x} x=-\\frac\\hbar i=\\hbar i.$$ Therefore, [!important] Heisenberg Uncertainty $$\\sigma_x\\sigma_p\\geq\\frac\\hbar 2.$$ Proof 2 (dumber) As before, note that $\\sigma_A=\\int\\Psi^ (A-A_0)^2\\Psi=\\int\\Psi^ (A-A_0)^*(A-A_0)\\Psi=\\int|(A-A_0)\\Psi)|^2.$ Assume that $\\Psi(x),\\Phi(p)$ are 0-meaned. Then $\\sigma_x^2=\\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle,$ and $\\sigma_p^2=\\int p|\\Phi(p)|^2\\,dp.$ Note that by Parseval's, $\\int p|\\Phi(p)|^2\\,dp=\\langle\\mathcal F^{-1}(p\\Phi(p))\\mid\\mathcal F^{-1}(p\\Phi(p))\\rangle,$ so now we compute this function using integration by parts. First $$ \\begin{aligned} p\\Phi(p)\\sqrt{2\\pi\\hbar} &=\\int p\\Psi(x)e^{-ipx/\\hbar}\\,dx\\ &=\\left.i\\hbar\\Psi(x)e^{-ipx/\\hbar}\\right|_{-\\infty}^{\\infty} - i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx\\ &=-i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx. \\end{aligned} $$ Then $$ \\begin{aligned} \\mathcal F^{-1}(p\\Phi(p))(x) &=\\frac1{\\sqrt{2\\pi\\hbar}}\\int p\\Phi(p)e^{ipx/\\hbar}\\,dp\\ &=\\frac1{2\\pi\\hbar}\\int e^{ipx/\\hbar}\\,dp(-i\\hbar)\\int\\frac{\\partial\\Psi}{\\partial x'}e^{-ipx'/\\hbar}\\,dx'\\ &=\\frac1{2\\pi i}\\int \\frac{\\partial\\Psi}{\\partial x'}\\,dx'\\int e^{\\frac{ip(x-x')}\\hbar}\\,dp\\ &=\\frac{\\hbar}i\\frac{\\partial\\Psi}{\\partial x}\\ &=\\hat p\\Psi(x). \\end{aligned} $$ Finally, $$ \\begin{aligned} \\sigma_x^2\\sigma_p^2 &= \\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle\\langle \\hat p\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle\\ &\\geq |\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle|^2\\ &\\geq \\left(\\frac{\\langle x\\Psi(x)\\mid \\hat p\\Psi(x)\\rangle-\\langle \\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle}{2i}\\right)^2. \\end{aligned} $$ Then (because of magic and how self-adjoint operators like $\\hat p$ work) $$ \\begin{aligned} \\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle &=\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\Psi(x)\\mid \\hat p(x\\Psi(x))\\rangle\\ &=\\int x\\Psi^ \\frac\\hbar i\\Psi'-\\Psi^ \\frac\\hbar{i}\\frac{\\partial}{\\partial x}(x\\Psi)\\,dx\\ &=\\frac\\hbar i\\int -\\Psi\\Psi^*\\,dx\\ &=\\hbar i. \\end{aligned} $$ Thus $\\sigma_x^2\\sigma_p^2\\geq \\left(\\frac{\\hbar}2\\right)^2$ and $\\sigma_x\\sigma_p\\geq\\frac\\hbar2.$","title":"8.04.7 Momentum space, Expectation, Uncertainty 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#expectation","text":"Pretty simple, really. $$\\mathbb E(Q)=\\int Q(x)P(x)\\,dx=\\int Q(x)|\\Psi(x)|^2\\,dx=\\int \\Psi^ Q\\Psi\\,dx.$$ Then we have \"joint probabilities\" $\\langle f|g\\rangle$ which is basically $\\int f^ g\\,dx$. We denote expectation using $\\langle Q\\rangle.$ Time derivative plugged into Schrodinger's yields [!faq] ROC of Expectation $$\\frac{\\partial}{\\partial t}\\langle Q\\rangle=\\frac1{i\\hbar}\\left\\langle[\\hat Q,\\hat H]\\right\\rangle.$$","title":"Expectation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#uncertainty-computation","text":"We can now even plug-and-chug uncertainty, as follows.","title":"Uncertainty Computation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#proof-1-smarter","text":"First note that for arbitrary $u,v$ and constant $t,$ $\\int (u+tv)^2=\\int u^2+2t\\int uv+t^2\\int v^2\\geq 0,$ so the discriminant in $t$ must be nonpositive i.e. $(2\\int uv)^2\\leq 4(\\int u^2)(\\int v^2),$ yielding [!c] Cauchy-Schwarz Inequality $$\\int u^2\\int v^2\\geq \\int uv.$$ Recall that $A$ is Hermitian if $\\int \\phi^ A\\psi=\\int\\psi^ A\\phi.$ Now let $A_0$ be the expected for $A$, and $B_0$ the same for $B$. Assume that $A_0=B_0=0.$ We will not use this condition, and are justified because $\\mathbb E[(X-c)^2]$ in general is minimized when $c=\\mathbb E[X],$ so we don't lose information by moving the distribution/assuming the mean is $0$ and not using that condition later. Then $\\sigma_A\\sigma_b=\\langle |A|^2\\rangle\\langle |B|^2\\rangle.$ Note that $\\langle |A|^2\\rangle=\\int \\Psi^ A^ A\\Psi=\\int|A\\Psi|^2.$ Then using Cauchy's from above, $$ \\begin{aligned} \\int|A\\Psi|^2\\int|B\\Psi^2| &\\geq\\int|A\\Psi||B\\Psi|\\ &\\geq\\int\\left|\\text{Im}((A\\Psi)^ B\\Psi)\\right|\\ &\\geq\\left|\\text{Im}\\left(\\int(A\\Psi)^ B\\Psi\\right)\\right|\\ &=\\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right|. \\end{aligned} $$ Now because $A,B$ are Hermitian, $$ \\begin{aligned} \\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right| &=\\left|\\frac{\\int \\Psi^ BA\\Psi-\\Psi^ AB\\Psi}{2i}\\right|\\ &=\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. \\end{aligned} $$ Thus we've arrived at [!important] Robertson Uncertainty If $A,B$ are Hermitian operators, $$\\sigma_A\\sigma_B\\geq\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. $$ Finally, $$[\\hat x,\\hat p]=x\\frac\\hbar i\\frac{\\partial}{\\partial x}-\\frac\\hbar i\\frac{\\partial}{\\partial x} x=-\\frac\\hbar i=\\hbar i.$$ Therefore, [!important] Heisenberg Uncertainty $$\\sigma_x\\sigma_p\\geq\\frac\\hbar 2.$$","title":"Proof 1 (smarter)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#proof-2-dumber","text":"As before, note that $\\sigma_A=\\int\\Psi^ (A-A_0)^2\\Psi=\\int\\Psi^ (A-A_0)^*(A-A_0)\\Psi=\\int|(A-A_0)\\Psi)|^2.$ Assume that $\\Psi(x),\\Phi(p)$ are 0-meaned. Then $\\sigma_x^2=\\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle,$ and $\\sigma_p^2=\\int p|\\Phi(p)|^2\\,dp.$ Note that by Parseval's, $\\int p|\\Phi(p)|^2\\,dp=\\langle\\mathcal F^{-1}(p\\Phi(p))\\mid\\mathcal F^{-1}(p\\Phi(p))\\rangle,$ so now we compute this function using integration by parts. First $$ \\begin{aligned} p\\Phi(p)\\sqrt{2\\pi\\hbar} &=\\int p\\Psi(x)e^{-ipx/\\hbar}\\,dx\\ &=\\left.i\\hbar\\Psi(x)e^{-ipx/\\hbar}\\right|_{-\\infty}^{\\infty} - i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx\\ &=-i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx. \\end{aligned} $$ Then $$ \\begin{aligned} \\mathcal F^{-1}(p\\Phi(p))(x) &=\\frac1{\\sqrt{2\\pi\\hbar}}\\int p\\Phi(p)e^{ipx/\\hbar}\\,dp\\ &=\\frac1{2\\pi\\hbar}\\int e^{ipx/\\hbar}\\,dp(-i\\hbar)\\int\\frac{\\partial\\Psi}{\\partial x'}e^{-ipx'/\\hbar}\\,dx'\\ &=\\frac1{2\\pi i}\\int \\frac{\\partial\\Psi}{\\partial x'}\\,dx'\\int e^{\\frac{ip(x-x')}\\hbar}\\,dp\\ &=\\frac{\\hbar}i\\frac{\\partial\\Psi}{\\partial x}\\ &=\\hat p\\Psi(x). \\end{aligned} $$ Finally, $$ \\begin{aligned} \\sigma_x^2\\sigma_p^2 &= \\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle\\langle \\hat p\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle\\ &\\geq |\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle|^2\\ &\\geq \\left(\\frac{\\langle x\\Psi(x)\\mid \\hat p\\Psi(x)\\rangle-\\langle \\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle}{2i}\\right)^2. \\end{aligned} $$ Then (because of magic and how self-adjoint operators like $\\hat p$ work) $$ \\begin{aligned} \\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle &=\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\Psi(x)\\mid \\hat p(x\\Psi(x))\\rangle\\ &=\\int x\\Psi^ \\frac\\hbar i\\Psi'-\\Psi^ \\frac\\hbar{i}\\frac{\\partial}{\\partial x}(x\\Psi)\\,dx\\ &=\\frac\\hbar i\\int -\\Psi\\Psi^*\\,dx\\ &=\\hbar i. \\end{aligned} $$ Thus $\\sigma_x^2\\sigma_p^2\\geq \\left(\\frac{\\hbar}2\\right)^2$ and $\\sigma_x\\sigma_p\\geq\\frac\\hbar2.$","title":"Proof 2 (dumber)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.8%20Eigenstates%2C%20Hermitian%2C%20Observables%2C%20Measurement/","text":"Define $\\langle \\psi,\\phi\\rangle=\\int\\psi^*\\phi.$ Operator $Q$ is Hermitian if $\\langle\\psi,\\hat Q\\phi\\rangle=\\langle\\hat Q\\psi,\\phi\\rangle.$ We can analyze all eigenstates $\\psi$ sat. $\\hat Q\\psi=q\\psi.$ Turns out that such $\\psi_i$ can be organized to be orthonormal (usual linalg) By making assumptions on space, assume eigenstates generate whole space Then $\\phi=\\sum_i a_i\\psi_i,$ where $a_i=\\langle \\psi_i,\\phi\\rangle.$ The \"state\" now becomes ${a_i}.$ For instance, take momentum on a circle; eigenstates are countable $e^{ix\\theta}$ waves [!important] Measurement Postulate (Copenhagen interp.) Assuming $\\langle\\phi,\\phi\\rangle=1,$ given $\\phi=\\sum_i a_i\\psi_i,$ interpretation of \"operator\" is when we observe that value, we get $p_i=a_i^2$ probability of obtaining the eigenvalue $q_i.$ Notice that this actually matches interp. for $\\hat x,$ eigenstates are normalized delta functions. Uncertainty is as established/studied previously. Note that $\\Phi$ is an eigenstate iff $\\Delta \\hat Q_\\Phi=0.$ ROC of Uncertainty $\\Delta x=\\langle x^2\\rangle-\\langle x\\rangle^2.$ We know $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\langle x^2\\rangle &=\\frac1{i\\hbar}\\langle [x^2,\\hat H]\\rangle\\ &=\\frac1{i\\hbar}\\cdot-\\frac{\\hbar^2}{2m}\\langle -2+4x\\nabla\\rangle\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ \\end{aligned} $$ $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\ \\end{aligned} $$","title":"8.04.8 Eigenstates, Hermitian, Observables, Measurement"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.8%20Eigenstates%2C%20Hermitian%2C%20Observables%2C%20Measurement/#roc-of-uncertainty","text":"$\\Delta x=\\langle x^2\\rangle-\\langle x\\rangle^2.$ We know $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\langle x^2\\rangle &=\\frac1{i\\hbar}\\langle [x^2,\\hat H]\\rangle\\ &=\\frac1{i\\hbar}\\cdot-\\frac{\\hbar^2}{2m}\\langle -2+4x\\nabla\\rangle\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ \\end{aligned} $$ $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\ \\end{aligned} $$","title":"ROC of Uncertainty"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/","text":"Recall Schrodinger equation; if we want a \"time independent\" or \"stationary\" state we need wavefunction ROC to be proportional to the wavefunction, i.e. an eigenstate of the Hamiltonian. More formally, [!important] Stationary States $\\Psi(x,t)=e^{-iEt/\\hbar}\\psi(x),$ where $E\\in\\mathbb R$ and $\\hat H\\psi=E\\psi.$ The exponential is actually a \"rotation\", just a scalar complex multiplication of norm 1. This preserves the normalization. The expectation value of a stationary state for a time-independent operator is itself time-independent; notice how the rotation cancels out in the integral $\\int \\Psi Q \\Psi^*.$ If we compose two stationary states we don't get another stationary state. Eigenstates for Hamiltonian are also called energy eigenstates . Characterization If we make assumptions on the potential, we can get info about the wavefunction. We will allow - Piecewise continuous - Delta functions - Infinities at hard walls As a result $\\psi'$ exists and has finitely many discontinuities. Free Particle on Circle We have $x\\in[0,L]$ a looped domain. Then we want to compute all of the stationary eigenstates. We get $e^{ikx}$ as an eigenstate for some $k,$ at which point $kL=2\\pi n$ from periodicity, so we've got our finite collection of eigenstates, and can characterize all solutions by Fourier expansion. (Discrete spread of eigenvalues, i.e. observables) We see that energy and momentum is quantized on this looped domain. Both $\\psi_n$ and $\\psi_{-n}$ have energy $E_n=\\frac{2\\pi^2\\hbar n^2}{mL^2},$ the eigenvalue from the Hamiltonian. The momentum eigenvalues of each of the eigenstates are different, so they must all be orthonormal. The resulting \"infinite vector\" of Fourier coefficients has norm 1.","title":"8.04.9 Time Independence, Free Particle on Circle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/#characterization","text":"If we make assumptions on the potential, we can get info about the wavefunction. We will allow - Piecewise continuous - Delta functions - Infinities at hard walls As a result $\\psi'$ exists and has finitely many discontinuities.","title":"Characterization"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/#free-particle-on-circle","text":"We have $x\\in[0,L]$ a looped domain. Then we want to compute all of the stationary eigenstates. We get $e^{ikx}$ as an eigenstate for some $k,$ at which point $kL=2\\pi n$ from periodicity, so we've got our finite collection of eigenstates, and can characterize all solutions by Fourier expansion. (Discrete spread of eigenvalues, i.e. observables) We see that energy and momentum is quantized on this looped domain. Both $\\psi_n$ and $\\psi_{-n}$ have energy $E_n=\\frac{2\\pi^2\\hbar n^2}{mL^2},$ the eigenvalue from the Hamiltonian. The momentum eigenvalues of each of the eigenstates are different, so they must all be orthonormal. The resulting \"infinite vector\" of Fourier coefficients has norm 1.","title":"Free Particle on Circle"},{"location":"Public%20Pages/homepage/","text":"All raw files available on Github . These files are a hand-picked subset of my personal Obsidian vault. About Me love food Sections Food \u2013 selected food and cooking notes","title":"Welcome"},{"location":"Public%20Pages/homepage/#about-me","text":"love food","title":"About Me"},{"location":"Public%20Pages/homepage/#sections","text":"Food \u2013 selected food and cooking notes","title":"Sections"}]}