{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"All raw files available on Github . These files are a hand-picked subset of my personal Obsidian vault. About Me love food Sections Food \u2013 selected food and cooking notes","title":"Home"},{"location":"#about-me","text":"love food","title":"About Me"},{"location":"#sections","text":"Food \u2013 selected food and cooking notes","title":"Sections"},{"location":"Material%20Knowledge/Food%2BCooking/24%2B7%20ways%20to%20do%20eggs/","text":"[x] scrambled [x] Tamagoyaki style scramble [x] sunny-up [x] poached [ ] eggs in cocotte [ ] soft-boiled (ramen eggs) [ ] tea eggs [x] hard-boiled [x] french omelette [x] country omelette [ ] baked eggs american country style [ ] deviled [ ] quiche (no pastry) [ ] frittata [ ] egg casserole [ ] breakfast egg wrap [x] eggs benedict [ ] breakfast egg sandwich (bacon, egg, cheese on muffin) [ ] egg salad [ ] scotch egg [x] french toast [ ] \u70b8\u86cb [ ] crepe [ ] omurice Desserts [ ] egg custard tart [ ] egg pancakes [ ] creme brulee (?) [ ] flan [ ] egg souffle [ ] egg cakes [ ] rum eggnog","title":"24+7 ways to do eggs"},{"location":"Material%20Knowledge/Food%2BCooking/24%2B7%20ways%20to%20do%20eggs/#desserts","text":"[ ] egg custard tart [ ] egg pancakes [ ] creme brulee (?) [ ] flan [ ] egg souffle [ ] egg cakes [ ] rum eggnog","title":"Desserts"},{"location":"Material%20Knowledge/Food%2BCooking/Things%20To%20Cook/","text":"[x] eggs benedict [x] https://www.thespruceeats.com/rum-eggnog-recipes-760561 [ ] easy over, correctly [ ] french souffle omelette https://www.youtube.com/watch?v=4XiWUis2eKc&ab_channel=ItaliaSquisita [ ] \u5364\u8089\u996d [ ] ahi poke [ ] gnocchi o pizza (with premade gnocchi ?) [ ] cotoletta alla milanese [ ] spezzatino with potatos [ ] melanzane alla parmigiana [ ] the korean egg rice thing i saw on insta","title":"Things To Cook"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Asian%20Barbecue%20Breaded%20Chicken/","text":"Marinate in brown sugar, barbecue mix, drumstick asian barbecue mix Cover with flour Melt butter in plate, arrange drumsticks tightly on the butter Turn halfway thru 425 F for 45 min","title":"Asian Barbecue Breaded Chicken"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/","text":"Batter 2 large eggs, 1 cup whole milk, dash salt, 2tbsp butter, 1/2 cup water using blender , add in 1cup allpurpose flour carefully dash of vanilla optional chill for 1hr Cooking Medium heat 10in skillet, oil with high-tolerance vegetable oil (butter if daring) 1/4 cup batter in, swirl, wait till brown underneath + flip Garnishes/Flourish Oil on medium heat, then a rested crepe, then low heat Galette Completes Egg on, don't break yolk, move egg white around [flavored, marinated] meat around yolk to fix in place, cheese on top Fold edges, careful not to burn on high heat, lid on until brown DW about uncooked egg Black pepper to finish Folded Egg on, use spoon to break and spread Cheese of choice Vegetables/thin meats of choice Fold into quarter circle, shift so inside can be seen Rolled Bottom 3/4 add meat, potatoes, cheese, vegetables etc. Roll, and place edge down to seal Roll into rectangle or circle shape","title":"Crepes"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#batter","text":"2 large eggs, 1 cup whole milk, dash salt, 2tbsp butter, 1/2 cup water using blender , add in 1cup allpurpose flour carefully dash of vanilla optional chill for 1hr","title":"Batter"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#cooking","text":"Medium heat 10in skillet, oil with high-tolerance vegetable oil (butter if daring) 1/4 cup batter in, swirl, wait till brown underneath + flip","title":"Cooking"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#garnishesflourish","text":"Oil on medium heat, then a rested crepe, then low heat","title":"Garnishes/Flourish"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#galette-completes","text":"Egg on, don't break yolk, move egg white around [flavored, marinated] meat around yolk to fix in place, cheese on top Fold edges, careful not to burn on high heat, lid on until brown DW about uncooked egg Black pepper to finish","title":"Galette Completes"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#folded","text":"Egg on, use spoon to break and spread Cheese of choice Vegetables/thin meats of choice Fold into quarter circle, shift so inside can be seen","title":"Folded"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Crepes/#rolled","text":"Bottom 3/4 add meat, potatoes, cheese, vegetables etc. Roll, and place edge down to seal Roll into rectangle or circle shape","title":"Rolled"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Eggs%20Benedict/","text":"Hollandaise - three egg yolks, teaspoon dijon mustard, tablespoon lemon juice, good bit of salt, some cayenne, blend with electric whisk - hot melt a LOT (half cup !!) of butter, and pour into mixture while whisking Poached Eggs - Can safely make multiple at once - Larger pan, deep enough water to cover the egg no more - Bring to a boil, then simmer, add (couple) splashes white vinegar - Very very very gently lower desired number of eggs into water, shape eggs in the water - Four minutes, take em out Microwaved version: - 1/4 cup water in a flat-bottom mug, microwave 50secs or till steaming - egg into water, microwave on 100 for 30secs or to desired consistency - alternatively, lower power, longer time? - take out and DRY Plus a toasted English Muffin, + fried Canadian Bacon 1min each side Plating - Bacon, Salmon if desired, not too much hollandaise - With parsley/rosemary garnish if desired","title":"Eggs Benedict"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/French%20Omelette/","text":"Butter + olive oil to barely a foam at low-medium heat Three eggs, whisk with salt white pepper and a bit of cold water Pour in, whisk and stir and shake, AVOID COLORING at all costs Three stages: scramble, spread, fold Hold at 45 deg angle and strike handle to get to slip and fold in that way","title":"French Omelette"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/French%20Toast/","text":"one egg, 1/6cup milk, 1tbsp flour, dash of vanilla/cinnamon, salt to taste, for two slices of Costco croissant bread in a lightly buttered hot skillet, add the two slices, one quarter of the egg on each face of each slice serve","title":"French Toast"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/My%20fave%20alcs%20%28so%20far%29/","text":"choya, horchata, elderflower liqueur, jaegermeister, smirnoff ice gin+tonic pinot grigio","title":"My fave alcs (so far)"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/Sous%20Vide%20Steak/","text":"Any cut, add seasonings/rubs/butter/herbs to both sides Add the machine, fill the pot to about half full with warm water Set up preheating, cook with \"rare\" (127) cook, when done dry thoroughly with paper towels take to skillet outside and sear with olive oil (350degs F) till just brown flipping 30 secs","title":"Sous Vide Steak"},{"location":"Material%20Knowledge/Food%2BCooking/Miscell/%E7%83%A4%E5%A4%A7%E6%8E%92%E9%AA%A8/","text":"Typically roast at 375 for 20-30 mins Kinder's barbecue is king Take large bone-in pork rib, season richly with barbecue rub, prime rib rub, brown sugar Marinate overnight Roast at 275 preheated for two-three hours Either stack crosswise in a dutch oven Or on an oven tray wrapped in aluminum foil Keeps moisture intact; won't dry out Serve","title":"\u70e4\u5927\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/Tasty%20things/Snacks/","text":"giant ono shrimp chips \u7d2b\u7c73\u7ca5 \u6761\u5934\u7cd5 \u5c0f\u9f99\u867e\u5473 \u9505\u5df4","title":"Snacks"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/","text":"\u7092\uff0c\u52a0\u8c03\u6599 \u4e94\u82b1\u8089\u5207\u6210\u201d\u5c0f\u5757\u201c\u4e0d\u8981\u5207\u5f97\u592a\u5c0f\uff0c\u8089\u4f1a\u7f29 \u7528\u5c11\u8bb8\u6cb9\u5148\u7ffb\u7092\uff0c\u7b49\u5230\u8868\u9762\u53d8\u8272\uff0c\u8fd9\u662f\u5c0f\u5fc3\u8868\u9762\u6e29\u5ea6 \u52a0\u51b0\u7cd6\u548c\u91d1\u5170\u6cb9\u818f\uff0c\u62cc\u5300 \u52a0\u5927\u91cf\u6599\u9152\uff0c\u5c11\u91cf\u7c73\u9152 \u52a0\u9999\u6599 2-3\u7247\u516b\u89d2 1-2\u7247\u9999\u53f6 2-3\u68f5\u4e01\u9999 \uff5e8\u514b\u4e94\u9999\u7c89 \u62cc\u5300\u540e\uff0c\u52a0\u6c34\u4e00\u76f4\u5230\u628a\u8089\u5168\u90e8\u76d6\u4f4f \u52a04-6\u716e\u597d\u4e86\u7684\u9e21\u86cb \u6700\u540e\u52a0\u9002\u91cf\u7092\u6d0b\u8471\u9165 \u987f\u3001\u6536\u6c41 \u7528\u5e95\u706b\u6162\u7096\uff0c\u7b49\u5230\u5feb\u8981\u6536\u6c41 \u9700\u8981\u7b49\u7684\u8bdd\u53ef\u4ee5\u653e\u5230\u4e00\u8fb9 \u6700\u540e\u6536\u6c41\u65f6 \u8089\u63a8\u5230\u4e00\u8fb9 \u9505\u62ac\u8d77\u6765\uff0c\u7528\u52fa\u5b50\u628a\u7559\u4e0b\u6765\u7684\u6c41\u5012\u5230\u8089\u4e0a \u8fd9\u6837\u8089\u4e0d\u4f1a\u5e72 \u6446\u76d8 blanch \u4e00\u4e9b\u5c0f\u767d\u83dc\u5fc3 \u628a\u9e21\u86cb\u5207\u6210\u534a\u6839\u7c73\u996d\u6446\u5728\u4e00\u8d77","title":"\u5364\u8089\u996d"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_1","text":"\u4e94\u82b1\u8089\u5207\u6210\u201d\u5c0f\u5757\u201c\u4e0d\u8981\u5207\u5f97\u592a\u5c0f\uff0c\u8089\u4f1a\u7f29 \u7528\u5c11\u8bb8\u6cb9\u5148\u7ffb\u7092\uff0c\u7b49\u5230\u8868\u9762\u53d8\u8272\uff0c\u8fd9\u662f\u5c0f\u5fc3\u8868\u9762\u6e29\u5ea6 \u52a0\u51b0\u7cd6\u548c\u91d1\u5170\u6cb9\u818f\uff0c\u62cc\u5300 \u52a0\u5927\u91cf\u6599\u9152\uff0c\u5c11\u91cf\u7c73\u9152 \u52a0\u9999\u6599 2-3\u7247\u516b\u89d2 1-2\u7247\u9999\u53f6 2-3\u68f5\u4e01\u9999 \uff5e8\u514b\u4e94\u9999\u7c89 \u62cc\u5300\u540e\uff0c\u52a0\u6c34\u4e00\u76f4\u5230\u628a\u8089\u5168\u90e8\u76d6\u4f4f \u52a04-6\u716e\u597d\u4e86\u7684\u9e21\u86cb \u6700\u540e\u52a0\u9002\u91cf\u7092\u6d0b\u8471\u9165","title":"\u7092\uff0c\u52a0\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_2","text":"\u7528\u5e95\u706b\u6162\u7096\uff0c\u7b49\u5230\u5feb\u8981\u6536\u6c41 \u9700\u8981\u7b49\u7684\u8bdd\u53ef\u4ee5\u653e\u5230\u4e00\u8fb9 \u6700\u540e\u6536\u6c41\u65f6 \u8089\u63a8\u5230\u4e00\u8fb9 \u9505\u62ac\u8d77\u6765\uff0c\u7528\u52fa\u5b50\u628a\u7559\u4e0b\u6765\u7684\u6c41\u5012\u5230\u8089\u4e0a \u8fd9\u6837\u8089\u4e0d\u4f1a\u5e72","title":"\u987f\u3001\u6536\u6c41"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%8D%A4%E8%82%89%E9%A5%AD/#_3","text":"blanch \u4e00\u4e9b\u5c0f\u767d\u83dc\u5fc3 \u628a\u9e21\u86cb\u5207\u6210\u534a\u6839\u7c73\u996d\u6446\u5728\u4e00\u8d77","title":"\u6446\u76d8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/","text":"\u5907\u6599 \u4e24\u7aef\u5207\u4e0b\u6765\uff0c\u76ae\u8584\u4e86 \u5207\u6210\u4e1d\uff1a\u6cbf\u7740equator\u5207 \u5207\u6210\u7247\uff1a\u6cbf\u7740radial\u5207 \u7f8a\u8089\u4e1d\u6ed1\u597d \u52a0\uff1a\u9178\u7c89\uff0cyoshida\uff0c\u6d0b\u8471\u5206\uff0c\u6599\u9152\uff0c\u7cd6\uff0c\u9171\u6cb9\u4e5f\u884c\uff0c\u8c46\u74e3\u9171\u4e5f\u884c \u8c03\u6599 \u5b5c\u7136 \u9e21\u6c64 \u70f9\u996a\u6cd5 \u52a0\u6cb9\uff0c\u5148\u628a\u8089\u7092\u4e86\uff0c\u52a0\u5b5c\u7136\u7c89 \u52a0\u6d0b\u8471\uff0c\u52a0\u9e21\u6c64\uff0c\u7092\u5230\u7f29\uff0c\u5728\u52a0\u5b5c\u7136\u7c89 \u8089\u52a0\u56de\u53bb\uff0c\u6709\u52a0\u5b5c\u7136\u7c89\uff0c\u53ef\u4ee5\u5728\u52a0\u9e21\u6c64\uff0c\u6700\u540e\u5c1d\u4e00\u53e3\u8bd5\u8bd5\u54b8\u86cb","title":"\u5b5c\u7136\u7f8a\u8089\u6d0b\u8471"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_1","text":"\u4e24\u7aef\u5207\u4e0b\u6765\uff0c\u76ae\u8584\u4e86 \u5207\u6210\u4e1d\uff1a\u6cbf\u7740equator\u5207 \u5207\u6210\u7247\uff1a\u6cbf\u7740radial\u5207 \u7f8a\u8089\u4e1d\u6ed1\u597d \u52a0\uff1a\u9178\u7c89\uff0cyoshida\uff0c\u6d0b\u8471\u5206\uff0c\u6599\u9152\uff0c\u7cd6\uff0c\u9171\u6cb9\u4e5f\u884c\uff0c\u8c46\u74e3\u9171\u4e5f\u884c","title":"\u5907\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_2","text":"\u5b5c\u7136 \u9e21\u6c64","title":"\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E5%AD%9C%E7%84%B6%E7%BE%8A%E8%82%89%E6%B4%8B%E8%91%B1/#_3","text":"\u52a0\u6cb9\uff0c\u5148\u628a\u8089\u7092\u4e86\uff0c\u52a0\u5b5c\u7136\u7c89 \u52a0\u6d0b\u8471\uff0c\u52a0\u9e21\u6c64\uff0c\u7092\u5230\u7f29\uff0c\u5728\u52a0\u5b5c\u7136\u7c89 \u8089\u52a0\u56de\u53bb\uff0c\u6709\u52a0\u5b5c\u7136\u7c89\uff0c\u53ef\u4ee5\u5728\u52a0\u9e21\u6c64\uff0c\u6700\u540e\u5c1d\u4e00\u53e3\u8bd5\u8bd5\u54b8\u86cb","title":"\u70f9\u996a\u6cd5"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E6%B8%85%E7%82%92%E5%B0%8F%E7%99%BD%E8%8F%9C/","text":"\u5c0f\u767d\u83dc\u5207\u6210\u5757\uff0c\u6839\u5207\u7684\u5c0f\uff0c\u53f6\u5b50\u5207\u7684\u5927 \u52a0\u4e00\u70b9\u6cb9\uff0c\u70e7\u70ed \u52a0\u83dc\u65f6\u8be5\u6709\u201c\u5472\u5566\u201d\u58f0 \u7b49\u53f6\u5b50\u7f29\uff0c\u52a0\u7cd6\uff0c\u76d0\uff0c\u9e21\u6c64 \u6491\u51fa\u6765 \u5403\uff01","title":"\u6e05\u7092\u5c0f\u767d\u83dc"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/","text":"dice ribs to desired size, bring water to a boil in large pot blanch ribs until white (20min) skim top stuff for each 500g of meat, add: lots of \u6599\u9152 2tbsp \u9171\u6cb9 mixed 3tbsp \u918b/\u7c73\u918b 4tbsp \u7ea2\u7cd6 \u59dc\u7247 if desired turn down heat, sit for 40 mins till color changes skim top stuff, turn up heat when sticky, tilt pot and baste in its own sauce, \u6536\u6c41 serve with green onion finely diced and white sesame, with modernist plating","title":"\u7cd6\u918b\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E7%99%BD%E8%8F%9C%EF%BC%88%E8%A6%81%E7%BB%83%EF%BC%89/","text":"\u7528\u6cb9\u70e7\u8471\u82b1 \u653e\u85d5\u7247/\u767d\u83dc \u7ffb\u7092 \u52a0\u76d0\uff0c\u7cd6 \u5feb\u719f\u4e86\uff0c\u52a0\u918b \u51fa\u9505\u524d\uff0c\u5173\u706b\uff0c\u52a0\u82b1\u6912\u6cb9 \u79f0\u51fa\u6765\uff0c\u52a0\u751f\u8471\u82b1","title":"\u7cd6\u918b\u767d\u83dc\uff08\u8981\u7ec3\uff09"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%BA%A2%E7%83%A7%E8%82%89/","text":"\u7528\u8db3\u91cf\u7684\u6c34\u70eb\u719f\uff0c\u628a\u4e0a\u9762\u7684\u6742\u522e\u5e72\u51c0 \u6c34\u5012\u6389\uff0c\u52a0\u8db3\u91cf\u7684\u6599\u9152 \u6bcf\u78c5\u8089\u52a0 2 tbsp \u751f\u62bd 1 tbsp \u8001\u62bd 2-3 \u52fa\u7cd6 \u8fd8\u53ef\u4ee5\u52a0\u59dc\uff0c\u9999\u53f6\uff0c\u4e01\u9999\uff0c\u516b\u89d2 \u5148\u716e\u5f00\u7136\u540e\u7528\u5e95\u706b\u987f \u6700\u540e\u6536\u6c41","title":"\u7ea2\u70e7\u8089"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%82%A5%E8%82%A0%E7%82%92%E8%92%9C%E8%8B%97/","text":"\u80a5\u80a0\u7a0d\u5fae\u7092\uff0c\u5012\u51fa\u6765 \u52a0\u849c\u82d7\uff0c\u7a0d\u5fae\u52a0\u76d0 \u6700\u540e\u975e\u5e38\u52a0\u56de\u53bb \u814a\u8089\u7092\u82b9\u83dc \u5f88\u50cf","title":"\u80a5\u80a0\u7092\u849c\u82d7"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/","text":"\u5907\u6599 \u7528\u5200\u628a\u83dc\u82b1\u6839\u5207\u4e0b\u6765\uff0c\u7528\u5200\u5728\u6839\u90a3\u91cc\u5207\u6210\u5757 \u7528\u5200\uff0c\u4f7f\u52b2\u538b\uff0c\u628a\u814a\u8089\u5207\u6210\u7247 \u8c03\u6599 \u8fa3\u8c46\u74e3\u9171 \u70f9\u996a\u6cd5 \u814a\u8089\u7092\u4e00\u4e0b\uff0c\u7b49\u53d8\u900f\u660e\uff0c\u5012\u51fa\u6765 \u83dc\u82b1\u52a0\u8fdb\u53bb\uff0c\u52a0\u9e21\u6c64\uff0c\u52a0\u8fa3\u8c46\u74e3\u9171\uff0c\u8d85\u51fa\u989c\u8272 \u52a0\u8089","title":"\u83dc\u82b1\u7092\u814a\u8089"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_1","text":"\u7528\u5200\u628a\u83dc\u82b1\u6839\u5207\u4e0b\u6765\uff0c\u7528\u5200\u5728\u6839\u90a3\u91cc\u5207\u6210\u5757 \u7528\u5200\uff0c\u4f7f\u52b2\u538b\uff0c\u628a\u814a\u8089\u5207\u6210\u7247","title":"\u5907\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_2","text":"\u8fa3\u8c46\u74e3\u9171","title":"\u8c03\u6599"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%8F%9C%E8%8A%B1%E7%82%92%E8%85%8A%E8%82%89/#_3","text":"\u814a\u8089\u7092\u4e00\u4e0b\uff0c\u7b49\u53d8\u900f\u660e\uff0c\u5012\u51fa\u6765 \u83dc\u82b1\u52a0\u8fdb\u53bb\uff0c\u52a0\u9e21\u6c64\uff0c\u52a0\u8fa3\u8c46\u74e3\u9171\uff0c\u8d85\u51fa\u989c\u8272 \u52a0\u8089","title":"\u70f9\u996a\u6cd5"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%9C%82%E9%92%88%E6%8E%92%E9%AA%A8/","text":"\u7528\u9171\u6cb9\u548c\u6599\u9152\uff0c\u732a\u6392\u814c\u4e24\u5c0f\u65f6 \u7528\u7c89\u9488\u6392\u9aa8\u7c89\uff0c\u628a\u6392\u9aa8\u7ed9\u76d6\u5300 \u653e\u9ad8\u538b\u9505\uff0c\u7528\u6c34\u84b855\u5206\u949f\uff08\u6309\u7167\u8bf4\u660e\uff09","title":"\u8702\u9488\u6392\u9aa8"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%A5%BF%E7%BA%A2%E6%9F%BF%E7%82%92%E9%B8%A1%E8%9B%8B/","text":"\u897f\u7ea2\u67ff\u5207\u6210\u4e01 \u52a0\u6cb9\uff0c\u9e21\u86cb\u7092\u4e0a\uff0c\u534a\u719f\u4fbf\u5012\u51fa\u6765 \u52a0\u897f\u7ea2\u67ff\uff0c\u52a0\u7cd6\u548c\u897f\u7ea2\u67ff\u9171\uff0c\u52a0\u9e21\u6c64\uff0c\u628a\u6c41\u7092\u51fa\u6765 \u677e\u4e86\u4ee5\u540e\uff0c\u53ef\u4ee5\u7528\u7b77\u5b50\u6311\u897f\u7ea2\u67ff\u76ae \u9e21\u86cb\u52a0\u56de\u53bb\uff0c\u626b\u4e00\u628a\u76d0\uff0c\u7ee7\u7eed\u7092 \u628a\u9e21\u86cb\u7092\u7684\u51dd\u56fa\u4e00\u70b9 \u52a0\u918b\uff0c\u82b1\u6912\u6cb9\uff0c\u8d85\u5300\uff0c\u6700\u540e\u52a0\u8471","title":"\u897f\u7ea2\u67ff\u7092\u9e21\u86cb"},{"location":"Material%20Knowledge/Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E8%B1%86%E8%A7%92%E7%84%96%E9%9D%A2/","text":"\u5728\u84b8\u9505\u91cc\u55b7\u4e00\u5c42\u6cb9\uff0c\u628a\u9762\u6563\u5f00\uff0c\u7528\u70ed\u6c34\u84b8 \u516b\u5206\u949f \u732a\u8089\u672b\u5907\u597d\uff0c\u52a0\u9171\u6cb9\uff0c\u869d\u6cb9\uff0c\u9ed1\u548c\u767d\u80e1\u6912\uff0cyoshida\uff0c\u849c\u7c89\uff0c\u6d0b\u8471\u7c89\uff0c\u62cc\u5300 \u849c\u9897\u5907\u597d\uff0c\u76ae\u8584\u4e86 \u8c46\u89d2\u5907\u597d\uff0c\u8fb9\u63b0\u4e0b\u6765\uff0c\u8c46\u89d2\u63b0\u6210\u5757 \u8471\u82b1\u5907\u597d \u9505\u91cc\u52a0\u6cb9\uff0c\u70e7\u70ed\uff0c\u7b49\u8089\u672b\u91cc\u7684\u6cb9\u7092\u51fa\u6765\uff0c\u52a0\u8c46\u74e3\u9171 \u5728\u52a0\u8c46\u89d2\uff0c\u6301\u7eed\u7ffb\u7092 \u51fa\u8272\u65f6\uff0c\u628a\u9762\u52a0\u5728\u4e0a\u9762\uff0c\u7116\u4e09\u5206\u949f \u52a0\u4e00\u534a\u8471\uff0c\u518d\u7ffb\u7092 \u628a\u989c\u8272\u5747\u5300\u5435\u5230\u9762\u4e0a","title":"\u8c46\u89d2\u7116\u9762"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/","text":"General idea: given samples from a distribution, we want to answer questions about the underlying distribution/predict the future. Process goes: - data, assumptions -> model -> predictions, declarations of uncertainty Notes on moments: - Variance is $\\mathbb{E}[X^{2}]-\\mathbb{E}[X]^{2},$ adds linearly over independent variables - Covariance is $\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y],$ is 0 for indep. variables - Covariance matrix $\\Sigma$ satisfies $\\Sigma_{ij}=\\text{Covar}[X_{i},X_{j}],$ note that it is symmetric - Can also think of $\\Sigma$ as the expected value of the outer product of random vector $X$ with itself - Hence $\\Sigma^{-1/2}X$ is unit normal, since its outer product is $\\Sigma^{-1/2}X X^{T}\\Sigma^{-1/2}$ whose expected value is $I$ Distributions Multivariate Gaussian \u2014invert the covariance: $\\frac{1}{\\sqrt{ (2\\pi)^{k}\\det(\\Sigma) }} \\exp((x-\\mu)^{\\dagger}\\Sigma ^{-1}(x-\\mu)).$ Beta \u2014conjugate prior for Bayesian updates on Bernoulli parameter: $\\text{Norm}\\left[ x^{a}(1-x)^{b} \\right]:x \\in [0,1]$ Poisson \u2014models discrete distribution of event count in fixed time, limit of Binomial: $\\frac{{\\lambda^{k}e^{\\lambda}}}{k!}$ Exponential \u2014time for an event to happen if each $dt$ is independent: $\\frac{e^{-x/\\lambda}}{\\lambda}$ has eV $\\lambda$ Gamma \u2014models time until $k$ rare events happen, i.e. sum of $k$ independent Exponentials: Student-t \u2014fat version of Gaussian used when fitting model with unknown true variance, using sample variance Chi-squared \u2014with $n$ degrees of freedom is sum of $n$ independent unit normal squared CLT If $X_{i}$ drawn from distribution with average $\\mu$ and std $\\sigma$ then the $n$th average $\\bar{X}$ has mean $\\mu$ and std $\\frac{\\sigma}{\\sqrt{ n }}.$ We call the constant $\\sigma$ the \"asymptotic std\". If the sample averages for $X$ converge to mean $\\mu$ and asymptotic std $\\sigma$ then the sample averages for $g(X)$ converge to mean $g(\\mu)$ and asymptotic std $g'(\\mu)\\sigma.$ This makes sense since as we grab more samples the density clusters around $\\mu$ so we only care about the \"stretching\" behavior of $g$ local to $\\mu.$ If $g$ spreads things out by a factor of $2$ then our linear unit, std., also stretches out by 2. Parametric Estimation MLE, Fisher Given data, we have a model $\\mathbb{P}_{\\theta}$ parametrized by a (usually finite) list of constants $\\theta$. Given data we map it to an estimator $\\hat{\\theta}$ that can be judged from being - biased (if its EV is different from true) - has se (the std from true value) - consistent (if it converges to true value in probability as we take many samples) - asymptotically normal if as $n$ goes to infty it converges to being normal around true value with variance $\\propto \\,n^{-1/2}$. We get an MLE estimator for all parameters by plugging in likelihood of drawing all of our data from the pdf formula, and maximizing log likelihood (take a derivative). It is asymptotically normal, consistent, and asymptotically efficient. Note that true value maximizes population log likelihood i.e. log likelihood over the population, because log likelihood is really just KL divergence from true distribution to our proposed distribution. The sample MLE maximizes sample log likelihood . Fisher info Given a model $f_{\\theta}$ and a particular value for $\\theta,$ the Fisher information is $$ I(\\theta):=\\mathbb{E} {\\theta}[-\\nabla^{2} {\\theta}\\log f_{\\theta}(X)]=\\mathbb{V} {\\theta}[\\nabla \\log f {\\theta}(X)], $$ when drawing a random $X$ from the distribution $f_{\\theta}$. To check the second equality, note that $$ -\\nabla^{2}\\log f_{\\theta}=-\\nabla\\frac{{\\nabla f_{\\theta}}}{f_{\\theta}}=-\\frac{{\\nabla^{2}f_{\\theta}f_{\\theta}-(\\nabla f_{\\theta}^{2})}}{f_{\\theta}^{2}}=\\left( \\frac{{\\nabla f_{\\theta}}}{f_{\\theta}} \\right) ^{2} - \\frac{{\\nabla^{2}f_{\\theta}}}{f_{\\theta}}. $$ But then $\\mathbb{E}\\left( \\frac{{\\nabla g}}{f_{\\theta}} \\right)=\\int \\nabla g=0,$ so $\\mathbb{E}[-\\nabla^{2}\\log f_{\\theta}]=\\mathbb{V}[\\nabla \\log f_{\\theta}].$ Key claim: asymptotic std of the MLE estimator is $I(\\theta^{*})^{-1}.$ Proof: Taylor expand $$ 0=l_{n}'(\\theta)\\approx l_{n}'(\\theta^{ })+l''_{n}(\\theta^{ })(\\hat{\\theta}-\\theta^{ }), $$ so that $$ \\sqrt{ n }(\\hat{\\theta}-\\theta^ )=-\\frac{{\\sqrt{ n }\\overline{\\partial_{\\theta} \\log f_{\\theta}{(X_{i})|_{\\theta ^ }} } }}{\\overline{\\partial^{2} {\\theta}\\log f {\\theta}(X_{i})|_{\\theta^ }}}. $$ The numerator has expected value $0$ since $l'(\\theta^ )=0,$ and variance equal to $I(\\theta^ ).$ The denominator converges to its mean value, also equal to $I(\\theta^ ).$ Hence the quotient has mean $0$ and variance $I(\\theta^ )^{-1}.$ Method of Moments Given $k$ real parameters $\\theta$ that we want to solve for, we plug in for the first $k$ moments of our model $f_{\\theta}(X).$ Then we can express the $i$th moment in terms of our $k$ parameters, and we solve the resulting $k$ equations. Remark: we could take higher order moments, but they would yield higher variance answers, since higher moments \"swing around\" more. EM Algo Used for mixture models. If we already knew which Gaussian each data sample is from, we can normal MLE in order, first by estimating $\\mu,\\sigma$ for each Gaussian and then balancing $p_{i}.$ However if we don't know, we need to assign a probability distribution of which Gaussian each element is in. Then the log likelihood for a given datapoint is $\\sum_{i}p_{i}l_{i}(X),$ where $p_{i}$ is the likelihood of belonging to the $i$th Gaussian. Bring in the EM-algorithm. On the Expectation step we weight the odds of $p_{i}$ by comparing odds according to the current $\\mu_{i},\\sigma_{i}.$ On the Maximization step we calculate the $\\mu_{i},\\sigma_{i}$ exactly using MLE and weighted log probs according to $p_{i}$. Bootstrap We want to get variance on our estimator, but the underlying pdf is intractable. Instead, we \"repeat the experiment\" by taking bootstrap samples from our existing samples with replacement . 1. For each sample we compute our estimator, then we create a histogram of drawn estimators from which we can compute variance, confidence intervals, etc. 2. We can do normal confidence interval by taking std and assuming distribution is normal, or we can take percentile confidence interval which just takes quartiles using our histogram. 3. Finally we can take pivotal confidence interval $\\left( 2\\hat{\\theta} {0}-q {\\frac{\\alpha}{2}}, 2\\hat{\\theta} {0}-q {1-\\frac{\\alpha}{2}} \\right)$ because we want true to lie within a certain range of our estimate, not vice versa (??) so we flip the interval around $\\hat{\\theta_{0}}.$ Optimally we do all $n^{n}$ random samplings with replacement but this is not actually feasible. Linear Regression Given vectors $X_{i}$ we want to map to scalars $Y_{i}.$ We assume our distribution is $f(X)=\\mathcal N(X\\beta ^{T}, \\mathbf{1}).$ Then MLE just wants us to minimize least-squares distance between $X\\beta$ and $Y,$ where we now take the matrix $X$ and vector $Y.$ Here the rows of $X$ are the vectors $X_{i}.$ We can take gradients to solve the minimization problem $$ L=\\left( X\\beta-Y \\right) \\cdot \\left( X\\beta-Y \\right) = \\beta^{T}X^{T}X\\beta-2Y^{T}X\\beta. $$ Then $$ \\nabla_{\\beta} L=2X^{T}X\\beta - 2Y^{T}X, $$ so $$ \\beta=(X^{T}X)^{-1}X^{T}Y. $$ Now note that our \"closest solution\" $X\\beta=X(X^{T}X)^{-1}X^{T}Y$ is in fact the result of projecting $Y$ down to $\\text{colspace}(X).$ Clearly the output of $$P=X(X^{T}X)^{-1}X^{T}$$ is in $\\text{colspace}(X),$ and since $P^{2}=P^{T}=P$ we get $P(I-P)^{T}=0$ and so projections onto $\\text{colspace}(X)$ are in fact normal. Define $\\mathbb{X}$ as the matrix with $X_{i}^{\\dagger}$ as rows. Variance on Linear Regression Suppose our (independent) $Y$ have (diagonalized) variance $\\sigma^{2}.$ Then we can model $Y=X\\beta^*+\\epsilon$ where $\\epsilon\\sim\\mathcal N(0,\\sigma).$ Plugging back into our expression for $\\beta$ we get that $$Var(\\beta)=\\sigma^{2}((X^{T}X)^{-1}X^{T})((X^{T}X)^{-1}X^{T})^{T}=\\sigma^{2}(X^{T}X)^{-1}.$$ Note that $\\sigma^{2}$ commutes because it's diagonalized. As an operational note we often use $\\sigma^{2}=\\frac{{\\lVert Y-X\\beta \\rVert^{2}}}{n-k}$ as our error estimate. Logistic Regression We want to predict a binary, or more generally a multiclass, i.e. what is the likelihood $Y$ will happen given $X$? This is very machine-learning style! We get $\\sigma(x^{T}\\beta)$ as our model, where $x^{T}\\beta$ outputs a bunch of logits. We can then do softmax to get our probabilities, and perform MLE on the matrix. Statisticians will insist we conserve degrees of freedom, so our softmax just sets $l_{0}=0$ and only predicts the latter $m-1$ logits. Note that this is consistent with using sigmoid for binary classification. R squared The $R^{2}$ score is $$ R^{2}(S)=1-\\frac{{\\lVert Y-X\\beta(S) \\rVert ^{2}}}{\\lVert Y-\\bar{Y}\\mathbb 1 \\rVert^{2} }. $$ The denom is squared distance from $Y$ to the line spanned by $\\mathbb{1},$ (also equals $Var(Y)$), i.e. performance if we make our model just the mean. It becomes negative when the $X$ features predict worse than the mean. Feature selection Generally adding more features to input data $X$ will allow model to fit better (e.g. project closer, eventually memorize data) but this is bad. We can pick features that increase $R^{2}$ the fastest, but don't include all to avoid overfit. Can do beam search and look for some plateau. We can also measure overfitting using information. - Akaike info: $l_{n}(\\hat{\\beta}(S))-|S|$ - Bayesian info: $l_{n}(\\hat{\\beta}(S))-\\frac{{\\log n}}{2}|S|$ Nonparametric Estimation Goal: estimating curves/distributions without setting an ansatz and solving. Generally we have some true distribution $g$ generating noisy samples $X$ which we fit to an estimator $X_{i}\\to \\hat{g}.$ Then (taking expectations over samples, for conf. interval purposes) - bias is how far estimator mean is from truth, for some input $x$ - variance is variance of estimator from its own mean - mean squared error is actual optimization point, MSE of estimator from ground truth Note that $MSE(\\hat{g},x)=b_{\\hat{g}}^{2}(x)+v_{\\hat{g}}(x).$ We define $R(\\hat{g},g)=\\int MSE(\\hat{g},x)\\,dx$ as error over the entire input. Also called mean integral squared error. For optimization purposes we want to check the MISE, reduces to $\\int g(x)\\hat{g}(x)\\,dx=\\mathbb{E}_{x\\sim g} \\int \\hat{g}(x).$ Since $\\hat{g}$ depends on chosen $x$ we just split train/test, textbook suggests to run all of the $n-1 / 1$ train-test splits. Can estimate using - histograms (box samples and make uniform in each box) called Regressogram - kernel density estimators (put a blob on each sample, fix variance to $h^{2}$ with reparameterization, mean over samples) called Nadaraya-Watson If we want to do regression we have a distribution over pairs $X,Y.$ Can apply either technique\u2014would be taking kernels in $X\\oplus Y$ and taking probabilities over fixed $X$. Testing Have null hypothesis, which we take to be status quo. Assume null hypothesis, what is the likelihood of seeing given data? Call that $p$-value, determines the level of our test. Type 1 error is false positive, Type 2 error is false negative. The \"power\" of a test is likelihood of rejecting null, defined by $\\beta(\\theta).$ If $H_{0}: \\theta=\\theta_{0},$ then likelihood of type 1 (rejecting when shouldn't reject) is $\\beta(\\theta_{0}).$ Wald test Asymptotically normal estimator $\\hat{\\theta},$ we return positive iff outside percentile confidence interval. Specifically, take sample mean and we know the variance. If null is interval then $p$-value is likelihood of falling in that interval. If null is equals some value then $p$-value is likelihood of being at least as far as that value. Chi-squared goodness-of-fit Define $\\chi_{k}^{2}=\\sum_{i.i.d.}^{k}Z_{i}^{2},$ where $Z_{i}\\sim \\mathcal N(0,1).$ Then given drawn samples $X_{i}$ and null hypothesis pmf, we can count real and expected occurrences of each outcome. Then the value $$ T=\\sum_{i}\\frac{{(O_{i}-E_{i})^{2}}}{E_{i}} $$ follows a $\\chi_{k-1}^{2}$ distribution. Proof: Let our samples $X_{i}$ be represented as one-hot encodings in our pmf. Then null hypothesis is that $$ \\Sigma=\\begin{bmatrix} p_{1}(1-p_{1})&-p_{1}p_{2}&-p_{1}p_{3} \\ -p_{2}p_{1}&p_{2}(1-p_{2})&-p_{2}p_{3} \\ -p_{3}p_{1}&-p_{3}p_{2}&-p_{3}(1-p_{3}) \\end{bmatrix} = -\\vec{p}\\otimes \\vec{p}+diag(\\vec{p}). $$ Let's remove the last row/column to get $\\hat{\\Sigma}$. Then letting $\\hat{p}$ denote the truncated probability vector, notably $$ \\hat{\\Sigma} ^{-1}=diag(\\hat{p}^{-1})+\\frac{1}{p_{k}}\\begin{bmatrix} 1 & \\dots&1 \\ \\vdots &\\ddots &\\vdots\\ 1 & \\dots&1 \\ \\end{bmatrix} $$ To check this, off diagonal dot products $M_{ij}$ of $\\hat{\\Sigma}\\hat{\\Sigma}^{-1}$ become $-p_{i}+\\frac{1}{}$ $\\textcolor{red}{TODO}$ KL and KS Tests Given iid samples from a fixed pdf, we can use Kolmogorov-Smirnov. We can generate an empirical CDF generated from uniform over our samples. The sup of the difference between our empirical CDF and the real CDF is distributed according to the KS distribution, dependent on $n$ but not dependent on the distribution! In the case that it's a Gaussian whose $\\mu$ and $\\sigma$ we derive from the sample, we need to use the Kolmogorov Lilliefors test, which requires a smaller distance to reject the null (because the Gaussian is already fitted to the data). Permutation, Multiple Hypothesis Are two samples from the same distribution? Bootstrap-style, mix them up and take a histogram of the difference in sample means. If the difference in means of our two samples are too far OOD then reject. For multiple hypothesis, we can take a naive union bound over all our hypothesis, dividing the $p$-value for each test by the number of tests (Bonferroni). Alternatively if they're guaranteed to be independent, the Benjamini-Hochberg method sorts the $p$-values in order, and taking the biggest one that lies below $\\frac{\\alpha}{m}i$ (where $i$ is the index of $p$-value) keeps (#false positives)/(#positives) below $\\alpha$ in expectation. Student-t Smarter version of Wald's. It compensates for not knowing the std $\\alpha$. Specifically $\\sqrt{ n }(\\bar{X} {n}-\\mu)\\hat{\\sigma}\\sim t {n-1}$ for $n-1$ DOF. Things to Rem. Within 1 std: 68% Within 2 std: 95% Within 3 std: 99.7% Distributions Beta : $p(x)=\\frac{1}{K}x^{a-1}(1-x)^{b-1}$ for $Beta(a,b).$ Expected value $\\frac{a}{a+b}.$ Bayesian We have a prior distribution on parameters $\\theta.$ After seeing data we adjust the distribution based on a conditional. The net effect is that the posterior is the likelihood of seeing this data, weighted by the prior: $$ f(\\theta|X_{i})\\propto f_{\\theta}(X_{i})\\cdot f(\\theta). $$ If we start with uniform over $\\mathbb{R}$ or some interval $[a,b]$ then this is equivalent to MLE. Estimators Bayes estimator is just the mean of our posterior, i.e. mean a posteriori . MAP, or maximum a posteriori, is the mode of our posterior. Aside: Robustness An estimator is robust if it stays bounded when an adversary changes some amount of it. For instance median is robust with breakdown point $\\frac{1}{2}.$ We can arbitrarily make an estimator more robust by dropping the top and bottom percentiles. More robust estimators do MLE assuming that $\\epsilon$ of our samples have been contaminated by an adversary $q$. We can also try omitting samples, i.e. calculate MLE by maximizing $l_{n}$ over all possible omission sets, and then maximize $\\theta$ to get a double max: $$ \\hat{\\theta}={\\arg\\max} {\\theta}\\max {|C|=m}l_{n}(\\theta,X_{i}\\backslash C). $$ Confounding Setups Survival/Censoring Sometimes we want to collect samples $T_{i}$ but they get \"cut off\" arbitrarily at $C_{i}$, independent of the statistic. How to estimate the cdf? Derive a \"harm\" estimator $h(t):=\\Pr(T_{i}=t\\mid T_{i}\\geq t)$. Then we can directly count these from taking all $C_{i}>t$ and computing $$ \\frac{{# T_{i}=t}}{# C_{i}, T_{i} \\geq t}. $$ Then $\\Pr(T_{i}> t)=\\prod_{i\\leq t} (1-h(t)).$ Randomized Controlled Trials We have people $i$ with treatment applied or unapplied $X_{i}$ and reward distribution $C_{X}$ for each person. Aim: general effect of application $$\\theta=\\mathbb{E}[C_{1}] - \\mathbb{E}[C_{0}].$$ However for each sample we only get to see $C_{X},$ so if we just make $X$ independent of the \"person\" (since $C_{i}$ is dependent on the person) then we get $$ \\theta=\\alpha:=\\mathbb{E}[C_{1}\\mid X=1] - \\mathbb{E}[C_{0}\\mid X=0]. $$ Surveys We want to know $T:=\\sum_{i}Y_{i}$ over the entire body, but can only grab $\\sum_{i\\in S}Y_{i}$ for some random $S$. Then the Horvitz-Thompson estimator is $$ T^{HT}:=\\mathbb{E}\\left[ \\sum_{i\\in S} \\frac{Y_{i}}{\\pi(i)} \\right]=\\sum_{i} \\frac{Y_{i}}{\\pi(i)}\\Pr(i\\in S)=\\sum_{i}Y_{i}. $$ Can evaluate the variance in theory using $\\pi_{ij},$ then apply Horvitz-Thompson on this by dividing by $\\pi_{ij}$ again. Classification Given inputs $X$ want to classify into labels $Y\\in \\left{ 0,1 \\right}.$ We find a \"Bayes estimator\" $$ h:=\\arg\\max_{h} \\Pr[Y \\neq h(X)]. $$ Equivalently, $h=1$ if $r(x):=\\mathbb{E}(Y|X=x)\\geq \\frac{1}{2}$. Can also do if $\\frac{\\Pr(X=x|Y=1)}{\\Pr(X=x|Y=0)} \\geq \\frac{{\\Pr(Y=0)}}{\\Pr(Y=1)},$ hence called Bayes. In high dimensions naive KDE isn't great, so can either - naively partition input dimensions into independent subsets - assume $X|Y=i$ are Gaussian, obtain a quadratic classifier Can also classify by mean of $k$ nearest neighbors, can be represented using KDE (look at \"effective range\" of each node).","title":"18.650"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#distributions","text":"Multivariate Gaussian \u2014invert the covariance: $\\frac{1}{\\sqrt{ (2\\pi)^{k}\\det(\\Sigma) }} \\exp((x-\\mu)^{\\dagger}\\Sigma ^{-1}(x-\\mu)).$ Beta \u2014conjugate prior for Bayesian updates on Bernoulli parameter: $\\text{Norm}\\left[ x^{a}(1-x)^{b} \\right]:x \\in [0,1]$ Poisson \u2014models discrete distribution of event count in fixed time, limit of Binomial: $\\frac{{\\lambda^{k}e^{\\lambda}}}{k!}$ Exponential \u2014time for an event to happen if each $dt$ is independent: $\\frac{e^{-x/\\lambda}}{\\lambda}$ has eV $\\lambda$ Gamma \u2014models time until $k$ rare events happen, i.e. sum of $k$ independent Exponentials: Student-t \u2014fat version of Gaussian used when fitting model with unknown true variance, using sample variance Chi-squared \u2014with $n$ degrees of freedom is sum of $n$ independent unit normal squared","title":"Distributions"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#clt","text":"If $X_{i}$ drawn from distribution with average $\\mu$ and std $\\sigma$ then the $n$th average $\\bar{X}$ has mean $\\mu$ and std $\\frac{\\sigma}{\\sqrt{ n }}.$ We call the constant $\\sigma$ the \"asymptotic std\". If the sample averages for $X$ converge to mean $\\mu$ and asymptotic std $\\sigma$ then the sample averages for $g(X)$ converge to mean $g(\\mu)$ and asymptotic std $g'(\\mu)\\sigma.$ This makes sense since as we grab more samples the density clusters around $\\mu$ so we only care about the \"stretching\" behavior of $g$ local to $\\mu.$ If $g$ spreads things out by a factor of $2$ then our linear unit, std., also stretches out by 2.","title":"CLT"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#parametric-estimation","text":"","title":"Parametric Estimation"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#mle-fisher","text":"Given data, we have a model $\\mathbb{P}_{\\theta}$ parametrized by a (usually finite) list of constants $\\theta$. Given data we map it to an estimator $\\hat{\\theta}$ that can be judged from being - biased (if its EV is different from true) - has se (the std from true value) - consistent (if it converges to true value in probability as we take many samples) - asymptotically normal if as $n$ goes to infty it converges to being normal around true value with variance $\\propto \\,n^{-1/2}$. We get an MLE estimator for all parameters by plugging in likelihood of drawing all of our data from the pdf formula, and maximizing log likelihood (take a derivative). It is asymptotically normal, consistent, and asymptotically efficient. Note that true value maximizes population log likelihood i.e. log likelihood over the population, because log likelihood is really just KL divergence from true distribution to our proposed distribution. The sample MLE maximizes sample log likelihood .","title":"MLE, Fisher"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#fisher-info","text":"Given a model $f_{\\theta}$ and a particular value for $\\theta,$ the Fisher information is $$ I(\\theta):=\\mathbb{E} {\\theta}[-\\nabla^{2} {\\theta}\\log f_{\\theta}(X)]=\\mathbb{V} {\\theta}[\\nabla \\log f {\\theta}(X)], $$ when drawing a random $X$ from the distribution $f_{\\theta}$. To check the second equality, note that $$ -\\nabla^{2}\\log f_{\\theta}=-\\nabla\\frac{{\\nabla f_{\\theta}}}{f_{\\theta}}=-\\frac{{\\nabla^{2}f_{\\theta}f_{\\theta}-(\\nabla f_{\\theta}^{2})}}{f_{\\theta}^{2}}=\\left( \\frac{{\\nabla f_{\\theta}}}{f_{\\theta}} \\right) ^{2} - \\frac{{\\nabla^{2}f_{\\theta}}}{f_{\\theta}}. $$ But then $\\mathbb{E}\\left( \\frac{{\\nabla g}}{f_{\\theta}} \\right)=\\int \\nabla g=0,$ so $\\mathbb{E}[-\\nabla^{2}\\log f_{\\theta}]=\\mathbb{V}[\\nabla \\log f_{\\theta}].$ Key claim: asymptotic std of the MLE estimator is $I(\\theta^{*})^{-1}.$ Proof: Taylor expand $$ 0=l_{n}'(\\theta)\\approx l_{n}'(\\theta^{ })+l''_{n}(\\theta^{ })(\\hat{\\theta}-\\theta^{ }), $$ so that $$ \\sqrt{ n }(\\hat{\\theta}-\\theta^ )=-\\frac{{\\sqrt{ n }\\overline{\\partial_{\\theta} \\log f_{\\theta}{(X_{i})|_{\\theta ^ }} } }}{\\overline{\\partial^{2} {\\theta}\\log f {\\theta}(X_{i})|_{\\theta^ }}}. $$ The numerator has expected value $0$ since $l'(\\theta^ )=0,$ and variance equal to $I(\\theta^ ).$ The denominator converges to its mean value, also equal to $I(\\theta^ ).$ Hence the quotient has mean $0$ and variance $I(\\theta^ )^{-1}.$","title":"Fisher info"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#method-of-moments","text":"Given $k$ real parameters $\\theta$ that we want to solve for, we plug in for the first $k$ moments of our model $f_{\\theta}(X).$ Then we can express the $i$th moment in terms of our $k$ parameters, and we solve the resulting $k$ equations. Remark: we could take higher order moments, but they would yield higher variance answers, since higher moments \"swing around\" more.","title":"Method of Moments"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#em-algo","text":"Used for mixture models. If we already knew which Gaussian each data sample is from, we can normal MLE in order, first by estimating $\\mu,\\sigma$ for each Gaussian and then balancing $p_{i}.$ However if we don't know, we need to assign a probability distribution of which Gaussian each element is in. Then the log likelihood for a given datapoint is $\\sum_{i}p_{i}l_{i}(X),$ where $p_{i}$ is the likelihood of belonging to the $i$th Gaussian. Bring in the EM-algorithm. On the Expectation step we weight the odds of $p_{i}$ by comparing odds according to the current $\\mu_{i},\\sigma_{i}.$ On the Maximization step we calculate the $\\mu_{i},\\sigma_{i}$ exactly using MLE and weighted log probs according to $p_{i}$.","title":"EM Algo"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#bootstrap","text":"We want to get variance on our estimator, but the underlying pdf is intractable. Instead, we \"repeat the experiment\" by taking bootstrap samples from our existing samples with replacement . 1. For each sample we compute our estimator, then we create a histogram of drawn estimators from which we can compute variance, confidence intervals, etc. 2. We can do normal confidence interval by taking std and assuming distribution is normal, or we can take percentile confidence interval which just takes quartiles using our histogram. 3. Finally we can take pivotal confidence interval $\\left( 2\\hat{\\theta} {0}-q {\\frac{\\alpha}{2}}, 2\\hat{\\theta} {0}-q {1-\\frac{\\alpha}{2}} \\right)$ because we want true to lie within a certain range of our estimate, not vice versa (??) so we flip the interval around $\\hat{\\theta_{0}}.$ Optimally we do all $n^{n}$ random samplings with replacement but this is not actually feasible.","title":"Bootstrap"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#linear-regression","text":"Given vectors $X_{i}$ we want to map to scalars $Y_{i}.$ We assume our distribution is $f(X)=\\mathcal N(X\\beta ^{T}, \\mathbf{1}).$ Then MLE just wants us to minimize least-squares distance between $X\\beta$ and $Y,$ where we now take the matrix $X$ and vector $Y.$ Here the rows of $X$ are the vectors $X_{i}.$ We can take gradients to solve the minimization problem $$ L=\\left( X\\beta-Y \\right) \\cdot \\left( X\\beta-Y \\right) = \\beta^{T}X^{T}X\\beta-2Y^{T}X\\beta. $$ Then $$ \\nabla_{\\beta} L=2X^{T}X\\beta - 2Y^{T}X, $$ so $$ \\beta=(X^{T}X)^{-1}X^{T}Y. $$ Now note that our \"closest solution\" $X\\beta=X(X^{T}X)^{-1}X^{T}Y$ is in fact the result of projecting $Y$ down to $\\text{colspace}(X).$ Clearly the output of $$P=X(X^{T}X)^{-1}X^{T}$$ is in $\\text{colspace}(X),$ and since $P^{2}=P^{T}=P$ we get $P(I-P)^{T}=0$ and so projections onto $\\text{colspace}(X)$ are in fact normal. Define $\\mathbb{X}$ as the matrix with $X_{i}^{\\dagger}$ as rows.","title":"Linear Regression"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#variance-on-linear-regression","text":"Suppose our (independent) $Y$ have (diagonalized) variance $\\sigma^{2}.$ Then we can model $Y=X\\beta^*+\\epsilon$ where $\\epsilon\\sim\\mathcal N(0,\\sigma).$ Plugging back into our expression for $\\beta$ we get that $$Var(\\beta)=\\sigma^{2}((X^{T}X)^{-1}X^{T})((X^{T}X)^{-1}X^{T})^{T}=\\sigma^{2}(X^{T}X)^{-1}.$$ Note that $\\sigma^{2}$ commutes because it's diagonalized. As an operational note we often use $\\sigma^{2}=\\frac{{\\lVert Y-X\\beta \\rVert^{2}}}{n-k}$ as our error estimate.","title":"Variance on Linear Regression"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#logistic-regression","text":"We want to predict a binary, or more generally a multiclass, i.e. what is the likelihood $Y$ will happen given $X$? This is very machine-learning style! We get $\\sigma(x^{T}\\beta)$ as our model, where $x^{T}\\beta$ outputs a bunch of logits. We can then do softmax to get our probabilities, and perform MLE on the matrix. Statisticians will insist we conserve degrees of freedom, so our softmax just sets $l_{0}=0$ and only predicts the latter $m-1$ logits. Note that this is consistent with using sigmoid for binary classification.","title":"Logistic Regression"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#r-squared","text":"The $R^{2}$ score is $$ R^{2}(S)=1-\\frac{{\\lVert Y-X\\beta(S) \\rVert ^{2}}}{\\lVert Y-\\bar{Y}\\mathbb 1 \\rVert^{2} }. $$ The denom is squared distance from $Y$ to the line spanned by $\\mathbb{1},$ (also equals $Var(Y)$), i.e. performance if we make our model just the mean. It becomes negative when the $X$ features predict worse than the mean.","title":"R squared"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#feature-selection","text":"Generally adding more features to input data $X$ will allow model to fit better (e.g. project closer, eventually memorize data) but this is bad. We can pick features that increase $R^{2}$ the fastest, but don't include all to avoid overfit. Can do beam search and look for some plateau. We can also measure overfitting using information. - Akaike info: $l_{n}(\\hat{\\beta}(S))-|S|$ - Bayesian info: $l_{n}(\\hat{\\beta}(S))-\\frac{{\\log n}}{2}|S|$","title":"Feature selection"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#nonparametric-estimation","text":"Goal: estimating curves/distributions without setting an ansatz and solving. Generally we have some true distribution $g$ generating noisy samples $X$ which we fit to an estimator $X_{i}\\to \\hat{g}.$ Then (taking expectations over samples, for conf. interval purposes) - bias is how far estimator mean is from truth, for some input $x$ - variance is variance of estimator from its own mean - mean squared error is actual optimization point, MSE of estimator from ground truth Note that $MSE(\\hat{g},x)=b_{\\hat{g}}^{2}(x)+v_{\\hat{g}}(x).$ We define $R(\\hat{g},g)=\\int MSE(\\hat{g},x)\\,dx$ as error over the entire input. Also called mean integral squared error. For optimization purposes we want to check the MISE, reduces to $\\int g(x)\\hat{g}(x)\\,dx=\\mathbb{E}_{x\\sim g} \\int \\hat{g}(x).$ Since $\\hat{g}$ depends on chosen $x$ we just split train/test, textbook suggests to run all of the $n-1 / 1$ train-test splits. Can estimate using - histograms (box samples and make uniform in each box) called Regressogram - kernel density estimators (put a blob on each sample, fix variance to $h^{2}$ with reparameterization, mean over samples) called Nadaraya-Watson If we want to do regression we have a distribution over pairs $X,Y.$ Can apply either technique\u2014would be taking kernels in $X\\oplus Y$ and taking probabilities over fixed $X$.","title":"Nonparametric Estimation"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#testing","text":"Have null hypothesis, which we take to be status quo. Assume null hypothesis, what is the likelihood of seeing given data? Call that $p$-value, determines the level of our test. Type 1 error is false positive, Type 2 error is false negative. The \"power\" of a test is likelihood of rejecting null, defined by $\\beta(\\theta).$ If $H_{0}: \\theta=\\theta_{0},$ then likelihood of type 1 (rejecting when shouldn't reject) is $\\beta(\\theta_{0}).$","title":"Testing"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#wald-test","text":"Asymptotically normal estimator $\\hat{\\theta},$ we return positive iff outside percentile confidence interval. Specifically, take sample mean and we know the variance. If null is interval then $p$-value is likelihood of falling in that interval. If null is equals some value then $p$-value is likelihood of being at least as far as that value.","title":"Wald test"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#chi-squared-goodness-of-fit","text":"Define $\\chi_{k}^{2}=\\sum_{i.i.d.}^{k}Z_{i}^{2},$ where $Z_{i}\\sim \\mathcal N(0,1).$ Then given drawn samples $X_{i}$ and null hypothesis pmf, we can count real and expected occurrences of each outcome. Then the value $$ T=\\sum_{i}\\frac{{(O_{i}-E_{i})^{2}}}{E_{i}} $$ follows a $\\chi_{k-1}^{2}$ distribution. Proof: Let our samples $X_{i}$ be represented as one-hot encodings in our pmf. Then null hypothesis is that $$ \\Sigma=\\begin{bmatrix} p_{1}(1-p_{1})&-p_{1}p_{2}&-p_{1}p_{3} \\ -p_{2}p_{1}&p_{2}(1-p_{2})&-p_{2}p_{3} \\ -p_{3}p_{1}&-p_{3}p_{2}&-p_{3}(1-p_{3}) \\end{bmatrix} = -\\vec{p}\\otimes \\vec{p}+diag(\\vec{p}). $$ Let's remove the last row/column to get $\\hat{\\Sigma}$. Then letting $\\hat{p}$ denote the truncated probability vector, notably $$ \\hat{\\Sigma} ^{-1}=diag(\\hat{p}^{-1})+\\frac{1}{p_{k}}\\begin{bmatrix} 1 & \\dots&1 \\ \\vdots &\\ddots &\\vdots\\ 1 & \\dots&1 \\ \\end{bmatrix} $$ To check this, off diagonal dot products $M_{ij}$ of $\\hat{\\Sigma}\\hat{\\Sigma}^{-1}$ become $-p_{i}+\\frac{1}{}$ $\\textcolor{red}{TODO}$","title":"Chi-squared goodness-of-fit"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#kl-and-ks-tests","text":"Given iid samples from a fixed pdf, we can use Kolmogorov-Smirnov. We can generate an empirical CDF generated from uniform over our samples. The sup of the difference between our empirical CDF and the real CDF is distributed according to the KS distribution, dependent on $n$ but not dependent on the distribution! In the case that it's a Gaussian whose $\\mu$ and $\\sigma$ we derive from the sample, we need to use the Kolmogorov Lilliefors test, which requires a smaller distance to reject the null (because the Gaussian is already fitted to the data).","title":"KL and KS Tests"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#permutation-multiple-hypothesis","text":"Are two samples from the same distribution? Bootstrap-style, mix them up and take a histogram of the difference in sample means. If the difference in means of our two samples are too far OOD then reject. For multiple hypothesis, we can take a naive union bound over all our hypothesis, dividing the $p$-value for each test by the number of tests (Bonferroni). Alternatively if they're guaranteed to be independent, the Benjamini-Hochberg method sorts the $p$-values in order, and taking the biggest one that lies below $\\frac{\\alpha}{m}i$ (where $i$ is the index of $p$-value) keeps (#false positives)/(#positives) below $\\alpha$ in expectation.","title":"Permutation, Multiple Hypothesis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#student-t","text":"Smarter version of Wald's. It compensates for not knowing the std $\\alpha$. Specifically $\\sqrt{ n }(\\bar{X} {n}-\\mu)\\hat{\\sigma}\\sim t {n-1}$ for $n-1$ DOF.","title":"Student-t"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#things-to-rem","text":"Within 1 std: 68% Within 2 std: 95% Within 3 std: 99.7%","title":"Things to Rem."},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#distributions_1","text":"Beta : $p(x)=\\frac{1}{K}x^{a-1}(1-x)^{b-1}$ for $Beta(a,b).$ Expected value $\\frac{a}{a+b}.$","title":"Distributions"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#bayesian","text":"We have a prior distribution on parameters $\\theta.$ After seeing data we adjust the distribution based on a conditional. The net effect is that the posterior is the likelihood of seeing this data, weighted by the prior: $$ f(\\theta|X_{i})\\propto f_{\\theta}(X_{i})\\cdot f(\\theta). $$ If we start with uniform over $\\mathbb{R}$ or some interval $[a,b]$ then this is equivalent to MLE.","title":"Bayesian"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#estimators","text":"Bayes estimator is just the mean of our posterior, i.e. mean a posteriori . MAP, or maximum a posteriori, is the mode of our posterior.","title":"Estimators"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#aside-robustness","text":"An estimator is robust if it stays bounded when an adversary changes some amount of it. For instance median is robust with breakdown point $\\frac{1}{2}.$ We can arbitrarily make an estimator more robust by dropping the top and bottom percentiles. More robust estimators do MLE assuming that $\\epsilon$ of our samples have been contaminated by an adversary $q$. We can also try omitting samples, i.e. calculate MLE by maximizing $l_{n}$ over all possible omission sets, and then maximize $\\theta$ to get a double max: $$ \\hat{\\theta}={\\arg\\max} {\\theta}\\max {|C|=m}l_{n}(\\theta,X_{i}\\backslash C). $$","title":"Aside: Robustness"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#confounding-setups","text":"","title":"Confounding Setups"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#survivalcensoring","text":"Sometimes we want to collect samples $T_{i}$ but they get \"cut off\" arbitrarily at $C_{i}$, independent of the statistic. How to estimate the cdf? Derive a \"harm\" estimator $h(t):=\\Pr(T_{i}=t\\mid T_{i}\\geq t)$. Then we can directly count these from taking all $C_{i}>t$ and computing $$ \\frac{{# T_{i}=t}}{# C_{i}, T_{i} \\geq t}. $$ Then $\\Pr(T_{i}> t)=\\prod_{i\\leq t} (1-h(t)).$","title":"Survival/Censoring"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#randomized-controlled-trials","text":"We have people $i$ with treatment applied or unapplied $X_{i}$ and reward distribution $C_{X}$ for each person. Aim: general effect of application $$\\theta=\\mathbb{E}[C_{1}] - \\mathbb{E}[C_{0}].$$ However for each sample we only get to see $C_{X},$ so if we just make $X$ independent of the \"person\" (since $C_{i}$ is dependent on the person) then we get $$ \\theta=\\alpha:=\\mathbb{E}[C_{1}\\mid X=1] - \\mathbb{E}[C_{0}\\mid X=0]. $$","title":"Randomized Controlled Trials"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#surveys","text":"We want to know $T:=\\sum_{i}Y_{i}$ over the entire body, but can only grab $\\sum_{i\\in S}Y_{i}$ for some random $S$. Then the Horvitz-Thompson estimator is $$ T^{HT}:=\\mathbb{E}\\left[ \\sum_{i\\in S} \\frac{Y_{i}}{\\pi(i)} \\right]=\\sum_{i} \\frac{Y_{i}}{\\pi(i)}\\Pr(i\\in S)=\\sum_{i}Y_{i}. $$ Can evaluate the variance in theory using $\\pi_{ij},$ then apply Horvitz-Thompson on this by dividing by $\\pi_{ij}$ again.","title":"Surveys"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/18.650/#classification","text":"Given inputs $X$ want to classify into labels $Y\\in \\left{ 0,1 \\right}.$ We find a \"Bayes estimator\" $$ h:=\\arg\\max_{h} \\Pr[Y \\neq h(X)]. $$ Equivalently, $h=1$ if $r(x):=\\mathbb{E}(Y|X=x)\\geq \\frac{1}{2}$. Can also do if $\\frac{\\Pr(X=x|Y=1)}{\\Pr(X=x|Y=0)} \\geq \\frac{{\\Pr(Y=0)}}{\\Pr(Y=1)},$ hence called Bayes. In high dimensions naive KDE isn't great, so can either - naively partition input dimensions into independent subsets - assume $X|Y=i$ are Gaussian, obtain a quadratic classifier Can also classify by mean of $k$ nearest neighbors, can be represented using KDE (look at \"effective range\" of each node).","title":"Classification"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/2.678/","text":"Resistors, voltage/current Kirchoff's laws Battery, ground, voltage vs. current Important Components Diode Silicon diode: .7 volts Germanium diode: .3 volts LED: 2-3 volts Current only goes forwards, acts as fixed voltage drop when passes through, open wire when none Capacitor Used in signal processing; acts as closed wire for \"high-frequency\" signals (cancel out), open wire for \"low-frequency\" signals If placed in series with the load, becomes high-pass filter (only high frequency passes, low cancelled out) If placed in parallel, becomes low-pass filter (only low frequency passes, high goes to ground) BJT - Bipolar Junction Transistor Base, Emitter, Collector Base acts as \"control\", collector current must be fixed $\\approx70$ multiple of base current, exits through emitter Enables \"op-amps\" (draws power source, amplifies difference between +/-) which uses BJT, voltage splitter resistors at base, collector to control voltage/current ratios voltage drop across collector resistor is fixed product of voltage drop across base resistor take output voltage after collector resistor, before the transistor\u2014is a clipped inverted multiple of input voltage MOSFET - Metal Oxide Semiconductor Field Effect Transistor when voltage passes through metal gate, turns on the semiconductor p-mosfet connects source to drain when gate is above source n-mosfet connects source to drain when gate is below source Motors when two magnets aren't aligned, there is some force pushing them together brush magnets have four electromagnet coils each at 90 degs current flows through brushes attached to the axle as the axle spins it activates the coil normal to the axle magnet ideal model: - calculates a constant from torque, voltage, and angular velocity","title":"2.678"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/2.678/#important-components","text":"Diode Silicon diode: .7 volts Germanium diode: .3 volts LED: 2-3 volts Current only goes forwards, acts as fixed voltage drop when passes through, open wire when none Capacitor Used in signal processing; acts as closed wire for \"high-frequency\" signals (cancel out), open wire for \"low-frequency\" signals If placed in series with the load, becomes high-pass filter (only high frequency passes, low cancelled out) If placed in parallel, becomes low-pass filter (only low frequency passes, high goes to ground) BJT - Bipolar Junction Transistor Base, Emitter, Collector Base acts as \"control\", collector current must be fixed $\\approx70$ multiple of base current, exits through emitter Enables \"op-amps\" (draws power source, amplifies difference between +/-) which uses BJT, voltage splitter resistors at base, collector to control voltage/current ratios voltage drop across collector resistor is fixed product of voltage drop across base resistor take output voltage after collector resistor, before the transistor\u2014is a clipped inverted multiple of input voltage MOSFET - Metal Oxide Semiconductor Field Effect Transistor when voltage passes through metal gate, turns on the semiconductor p-mosfet connects source to drain when gate is above source n-mosfet connects source to drain when gate is below source Motors when two magnets aren't aligned, there is some force pushing them together brush magnets have four electromagnet coils each at 90 degs current flows through brushes attached to the axle as the axle spins it activates the coil normal to the axle magnet ideal model:","title":"Important Components"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/2.678/#-calculates-a-constant-from-torque-voltage-and-angular-velocity","text":"","title":"- calculates a constant from torque, voltage, and angular velocity"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.1%20-%20Motivating%20Fib%20Heaps/","text":"Shortest Paths, Minimum Spanning Tree Kruskal's: sort edges, use union data structure to collect edges Prim's: grow tree by keeping PQ of could-be-added edges approx. Dijkstra Boruvka's: parallelize by taking minimum edge out of each established component at the same time (start with all nodes as components) Now we want a priority queue to track edges. Reminder: heap means f(node) < f(child) for all children For MST we know in advance the tradeoff between different operations, so we can adjust arity of the heap suitably. Call this a $d$-heap. Laziness paradigm Don't work until you need to. When you work make it maximally efficient. Work in a way that enables future lazy work to be easy. Alternatively, create an algorithm where an \"adversary\" must also work (contrive) a bad counterexample to make you work. We care about \"amortized\" randomized performance ( not worst-case anymore). For laziness we define a \"potential\" function that measures the amount of lazy work that needs to be done. Now let's do min deletion lazily. We go through the whole list from left to right, pick up our min and write down everything\u2014then we get this comparator tree. To balance this tree we run a divide-and-conquer. Now we call these trees \"heap-ordered trees\"\u2014somewhat balanced trees that satisfy heap property. On insert, just add HOT(n). On deletion, compare all the roots and consolidate, then cut the head and get a collection of HOT again. Finally, we choose to only compare roots with equal degree $\\to$ subtrees have size $2^{k}$. Then we have buckets by subtree size, and only compare/upgrade within the same bucket. We use the potential to \"amortize\" the cost by picking $\\phi$ such that $c_{a}=c + \\Delta \\phi$ so that $c_{a}$ is worst-case not that bad. Then we set $\\phi$ to size of our collection. Then","title":"6.5210.1   Motivating Fib Heaps"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.1%20-%20Motivating%20Fib%20Heaps/#shortest-paths-minimum-spanning-tree","text":"Kruskal's: sort edges, use union data structure to collect edges Prim's: grow tree by keeping PQ of could-be-added edges approx. Dijkstra Boruvka's: parallelize by taking minimum edge out of each established component at the same time (start with all nodes as components) Now we want a priority queue to track edges. Reminder: heap means f(node) < f(child) for all children For MST we know in advance the tradeoff between different operations, so we can adjust arity of the heap suitably. Call this a $d$-heap.","title":"Shortest Paths, Minimum Spanning Tree"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.1%20-%20Motivating%20Fib%20Heaps/#laziness-paradigm","text":"Don't work until you need to. When you work make it maximally efficient. Work in a way that enables future lazy work to be easy. Alternatively, create an algorithm where an \"adversary\" must also work (contrive) a bad counterexample to make you work. We care about \"amortized\" randomized performance ( not worst-case anymore). For laziness we define a \"potential\" function that measures the amount of lazy work that needs to be done. Now let's do min deletion lazily. We go through the whole list from left to right, pick up our min and write down everything\u2014then we get this comparator tree. To balance this tree we run a divide-and-conquer. Now we call these trees \"heap-ordered trees\"\u2014somewhat balanced trees that satisfy heap property. On insert, just add HOT(n). On deletion, compare all the roots and consolidate, then cut the head and get a collection of HOT again. Finally, we choose to only compare roots with equal degree $\\to$ subtrees have size $2^{k}$. Then we have buckets by subtree size, and only compare/upgrade within the same bucket. We use the potential to \"amortize\" the cost by picking $\\phi$ such that $c_{a}=c + \\Delta \\phi$ so that $c_{a}$ is worst-case not that bad. Then we set $\\phi$ to size of our collection. Then","title":"Laziness paradigm"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.10%20-%20Min%20Cost%20Flow/","text":"Min Cost New paradigm: each edge has a cost so that the cost of the total flow is $c(e)f(u)$. Then the goal is to maximize flow (first) while minimizing flow (second). We can translate MCF to Min Cost Circulation, i.e. no source/sink. The algorithm is just to take residual of maxflow, and then pick a min cost circulation to add on top. Min Cost Circulation Just repeat Bellman-Floyd (finding negative cycles) and augment circulation. Prices Let's add a market at each node. Each node offers a price to buy/sell at. It's stable when no traffickers can make a profit, i.e. $\\Delta_{v,w} p\\leq d(v,w)$. Then define \"reduced cost\" $c_{p}(vw)=c(vw)+p(v)-p(w)$, cost to buy at $v$, ship, and sell at $w$. All reduced costs should be $0$ along shortest paths, and positive along inoptimal paths. Price function is feasible if (residual) graph if all arcs have nonnegative cost, i.e. there is no profit game. Notably, reduced cost path prices are just constant function of normal path prices (for fixed $s,t$). Circulation is optimal cost (i.e. when added to max flow, is best circulation) iff. exists feasible price function. If unoptimal, there must be a negative cycle, i.e. shipping will always print money.","title":"6.5210.10   Min Cost Flow"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.10%20-%20Min%20Cost%20Flow/#min-cost","text":"New paradigm: each edge has a cost so that the cost of the total flow is $c(e)f(u)$. Then the goal is to maximize flow (first) while minimizing flow (second). We can translate MCF to Min Cost Circulation, i.e. no source/sink. The algorithm is just to take residual of maxflow, and then pick a min cost circulation to add on top.","title":"Min Cost"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.10%20-%20Min%20Cost%20Flow/#min-cost-circulation","text":"Just repeat Bellman-Floyd (finding negative cycles) and augment circulation.","title":"Min Cost Circulation"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.10%20-%20Min%20Cost%20Flow/#prices","text":"Let's add a market at each node. Each node offers a price to buy/sell at. It's stable when no traffickers can make a profit, i.e. $\\Delta_{v,w} p\\leq d(v,w)$. Then define \"reduced cost\" $c_{p}(vw)=c(vw)+p(v)-p(w)$, cost to buy at $v$, ship, and sell at $w$. All reduced costs should be $0$ along shortest paths, and positive along inoptimal paths. Price function is feasible if (residual) graph if all arcs have nonnegative cost, i.e. there is no profit game. Notably, reduced cost path prices are just constant function of normal path prices (for fixed $s,t$). Circulation is optimal cost (i.e. when added to max flow, is best circulation) iff. exists feasible price function. If unoptimal, there must be a negative cycle, i.e. shipping will always print money.","title":"Prices"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.11%20-%20Min%20Cost%202/","text":"Ok, so we now know optimality for min cost problem\u2014no negative cycles. We've got the min cost circulation formulation. How to do? Max flow, then solve MCC (min cost circulation). Alternatively: Shortest Augmenting Path First, clear all existing negative cycles. Then each time we add the AP with lowest cost, and send maximal flow down said path. This cannot create any additional negative cycles: - If it does, said cycle must go \"backwards\" along existing path, take the complement of intersection in cycle - Alternatively, use price function; because we're augmenting a shortest path, a previously feasible price function stays feasible Algorithms for finding SAP Naively, we run Bellman Ford $mU$ times, to get total cost $O(mnF)\\leq O(m^{2}nU).$ We can do better; run Bellman Ford once to get all shortest paths, then reduced costs $d_{uv}'=d_{suv}-d_{sv}$ are all nonneg so we can just use Dijkstra's. We can keep using this because as we augment prices don't change; only capacities change. This yields $O(mF)\\leq O(m^{2}U).$ Now let's do scaling to get true polynomial time. When scaling with adds to negative cost arcs, we begin by saturating all negative cost arcs and adding supply/demands, so that all edges become nonnegative cost and we can use Dijkstra's. Then $U$ is set to $1$ but we run this algorithm $\\log U$ times so we get $O(m^{2}\\log U)$ time.","title":"6.5210.11   Min Cost 2"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.11%20-%20Min%20Cost%202/#shortest-augmenting-path","text":"First, clear all existing negative cycles. Then each time we add the AP with lowest cost, and send maximal flow down said path. This cannot create any additional negative cycles: - If it does, said cycle must go \"backwards\" along existing path, take the complement of intersection in cycle - Alternatively, use price function; because we're augmenting a shortest path, a previously feasible price function stays feasible","title":"Shortest Augmenting Path"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.11%20-%20Min%20Cost%202/#algorithms-for-finding-sap","text":"Naively, we run Bellman Ford $mU$ times, to get total cost $O(mnF)\\leq O(m^{2}nU).$ We can do better; run Bellman Ford once to get all shortest paths, then reduced costs $d_{uv}'=d_{suv}-d_{sv}$ are all nonneg so we can just use Dijkstra's. We can keep using this because as we augment prices don't change; only capacities change. This yields $O(mF)\\leq O(m^{2}U).$ Now let's do scaling to get true polynomial time. When scaling with adds to negative cost arcs, we begin by saturating all negative cost arcs and adding supply/demands, so that all edges become nonnegative cost and we can use Dijkstra's. Then $U$ is set to $1$ but we run this algorithm $\\log U$ times so we get $O(m^{2}\\log U)$ time.","title":"Algorithms for finding SAP"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.12%20-%20Intro%20to%20LP/","text":"Start with two different framings. [!important] Canonical LP formulation Maximize the linear objective $c^{\\dagger}x$ under the component-wise linear constraint $Ax\\leq b$. [!important] Standard LP formulation Minimize the linear objective $c^{\\dagger}x$ in the affine space $Ax=b$ under the nonnegative constraint $x\\geq 0$. Convert between by writing $x=x^{+}-x^{-}$ for $x^{*}\\geq 0.$ Canonical is useful for theory (less moving parts), standard is better for algorithms. Do some bashing (Cramer's det. rule, Gaussian elim., size checks, etc.) show we can check $Ax=b$ in polynomial time. Vertex intuition Vertex : point that is not convex combo of two others Extreme point : unique solution for some objective $c$ Basic feasible point : tight intersection of $n$ half-planes and feasible These are equivalent. Then opt must be at a BFS, if it exists. Decision problem Checking if opt. can do by - provide BFS achieving opt. - take the half-spaces of $c^{\\dagger}x=k$ forced by each of the linear constraints - show has point solution","title":"6.5210.12   Intro to LP"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.12%20-%20Intro%20to%20LP/#vertex-intuition","text":"Vertex : point that is not convex combo of two others Extreme point : unique solution for some objective $c$ Basic feasible point : tight intersection of $n$ half-planes and feasible These are equivalent. Then opt must be at a BFS, if it exists.","title":"Vertex intuition"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.12%20-%20Intro%20to%20LP/#decision-problem","text":"Checking if opt. can do by - provide BFS achieving opt. - take the half-spaces of $c^{\\dagger}x=k$ forced by each of the linear constraints - show has point solution","title":"Decision problem"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/","text":"We introduced BFS (intersection of $n$), vertex (not lin. comb), extreme (unique opt. for some $c$) last time. Clear that - not BFS -> not vertex (find subspace) - not vertex -> not extreme (take lin. comb) BFS to extreme is more difficult. Given a BFS, transform to normal form so that the $n$ tight equalities/inequalities become $x_{i}\\geq 0.$ Then we take our objective to be $\\sum x_{i}.$ Clearly unique opt., since any other feasible point must have some $x_{i}>0.$ Examining the Dual Take $\\min cx$ such that $Ax=b, x\\geq 0.$ Then for any $y$ satisfying $yA\\leq c$ componentwise, for any feasible $x$ we get $$yb=y(Ax)=(yA)x\\leq cx,$$ i.e. $yb$ is always at most $cx$. Then we get weak duality . [!important] Dual of Standard LP The dual of $$\\min c^{\\dagger}x \\colon Ax=b, x\\geq 0$$ is $$\\max b^{\\dagger}y\\colon A^{\\dagger}y\\leq c.$$ The claim to strong duality is that $\\max yb=\\min cx.$ Physics Proof of Strong Duality Given a ball touching a bunch of walls, we have normal forces $A_{i}$ with magnitude $x_{i}$ all touching a ball, with resulting force $b$. Reorient so that the objective function becomes $y$-height. Ball must have net force $b$ and nonnegative force from the walls ($x_{i}\\geq_{}0$). Strong Duality Proof TBD Duality Intuition TBD Complementary Slackness Change our dual to $$\\max b^{\\dagger}y\\colon A^{\\dagger}y+s= c, s\\geq0.$$ Then note that when constraints are satisfied, $$c^{\\dagger}x-b^{\\dagger}y=(A^{\\dagger}y+s)x-b^{\\dagger}y=(y^{\\dagger}A)x+sx-y^{\\dagger}(Ax)=sx,$$ so at $opt$ we must have $sx=0.$ Hence $s_{i}x_{i}=0 \\,\\forall i$ so either $x_{i}=0$ or $A_{i}^{\\dagger}y=c_{i}.$ In the generalized case, if $x_{i}$ is UIS then TBD","title":"6.5210.13   LP Duality"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/#examining-the-dual","text":"Take $\\min cx$ such that $Ax=b, x\\geq 0.$ Then for any $y$ satisfying $yA\\leq c$ componentwise, for any feasible $x$ we get $$yb=y(Ax)=(yA)x\\leq cx,$$ i.e. $yb$ is always at most $cx$. Then we get weak duality . [!important] Dual of Standard LP The dual of $$\\min c^{\\dagger}x \\colon Ax=b, x\\geq 0$$ is $$\\max b^{\\dagger}y\\colon A^{\\dagger}y\\leq c.$$ The claim to strong duality is that $\\max yb=\\min cx.$","title":"Examining the Dual"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/#physics-proof-of-strong-duality","text":"Given a ball touching a bunch of walls, we have normal forces $A_{i}$ with magnitude $x_{i}$ all touching a ball, with resulting force $b$. Reorient so that the objective function becomes $y$-height. Ball must have net force $b$ and nonnegative force from the walls ($x_{i}\\geq_{}0$).","title":"Physics Proof of Strong Duality"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/#strong-duality-proof","text":"TBD","title":"Strong Duality Proof"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/#duality-intuition","text":"TBD","title":"Duality Intuition"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.13%20-%20LP%20Duality/#complementary-slackness","text":"Change our dual to $$\\max b^{\\dagger}y\\colon A^{\\dagger}y+s= c, s\\geq0.$$ Then note that when constraints are satisfied, $$c^{\\dagger}x-b^{\\dagger}y=(A^{\\dagger}y+s)x-b^{\\dagger}y=(y^{\\dagger}A)x+sx-y^{\\dagger}(Ax)=sx,$$ so at $opt$ we must have $sx=0.$ Hence $s_{i}x_{i}=0 \\,\\forall i$ so either $x_{i}=0$ or $A_{i}^{\\dagger}y=c_{i}.$ In the generalized case, if $x_{i}$ is UIS then TBD","title":"Complementary Slackness"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.14%20-%20LP%20Algorithms/","text":"Simplex Method Start with standard form $Ax=b,$ make columns independent by removing redundant stuff (values in $b$ had better match up). Any set of $m$ columns (basis) yields one intersection, then check feasibility and optimality. (grab a vertex) If more than $m$ planes pass through vertex, we can discard until $m$ remain. Then all $m-1$ subsets determine an edge. Collect all hyperplanes that touch chosen vertex, then look over all edges; one such edge must increase objective (this can be shown since all differentials are convex combo. of edges), then walk across. Called pivoting because set of $m$ changes by one element. Specifically in standard formulation, involves walking around the edges of the hyperplane $Ax=b.$ 1992 bound: $m^{\\log n}$ or $2^{n}m.$ Ellipsoid method Reduce to finding feasible of $Ax\\leq b$ by taking primal with dual. Take a very large ellipsoid that covers entire space, then - want center of ellipsoid to lie in $P$ - Take two ellipsoid covers of large ellipsoid, check each cover - Cover necessarily has volume $V_{1}\\leq(1-k)V_{0}$, so is polynomial in length of inputs (because of log stuff) Interior point method Have a \"loss function\" defined by distance from the walls (log of $Ax-b$) plus actual objective $cx$ Then gradient descent/cont. optimization on this thing","title":"6.5210.14   LP Algorithms"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.14%20-%20LP%20Algorithms/#simplex-method","text":"Start with standard form $Ax=b,$ make columns independent by removing redundant stuff (values in $b$ had better match up). Any set of $m$ columns (basis) yields one intersection, then check feasibility and optimality. (grab a vertex) If more than $m$ planes pass through vertex, we can discard until $m$ remain. Then all $m-1$ subsets determine an edge. Collect all hyperplanes that touch chosen vertex, then look over all edges; one such edge must increase objective (this can be shown since all differentials are convex combo. of edges), then walk across. Called pivoting because set of $m$ changes by one element. Specifically in standard formulation, involves walking around the edges of the hyperplane $Ax=b.$ 1992 bound: $m^{\\log n}$ or $2^{n}m.$","title":"Simplex Method"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.14%20-%20LP%20Algorithms/#ellipsoid-method","text":"Reduce to finding feasible of $Ax\\leq b$ by taking primal with dual. Take a very large ellipsoid that covers entire space, then - want center of ellipsoid to lie in $P$ - Take two ellipsoid covers of large ellipsoid, check each cover - Cover necessarily has volume $V_{1}\\leq(1-k)V_{0}$, so is polynomial in length of inputs (because of log stuff)","title":"Ellipsoid method"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.14%20-%20LP%20Algorithms/#interior-point-method","text":"Have a \"loss function\" defined by distance from the walls (log of $Ax-b$) plus actual objective $cx$ Then gradient descent/cont. optimization on this thing","title":"Interior point method"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/","text":"For some optimization problem, we want to get within $\\epsilon$ of the true answer, or within ratio of $[(1-\\epsilon)O, (1+\\epsilon)O].$ Greedy Algorithms Often we can get some $C$-approximation (frequently 2 or 3/2) by just making some simplifying assumption, doing things greedily, and then double- or triple-counting to show that our greedy construction is not that much worse than opt. [!important] Matching Classic example. Just take heavy edges greedily; 2-approximation to opt. Scheduling Theory We have tasks to be completed by machines in parallel, under certain constraints, with certain objective. Notation machines | constraints | objective We have some number of machines $1$ or $m$. Constraints could be unit time tasks, $r_{j}$ \"release dates\", $d_{j}$ deadlines for discrete objective. Objective could be number of late tasks, combined total lateness, time to complete last task, average \"freed time\" (timestamp at which machine $i$ is no longer needed). FPAS, PAS We want to get within $\\epsilon$ of error. PAS means for any fixed $\\epsilon$ we can get within $\\epsilon$ relative error in $n-$polytime. FPAS (fully polynomial approx. scheme) means we can get within $\\epsilon$ relative error in $n, \\epsilon^{-1}$ polytime. One common paradigm (for knapsack, bin packing, etc.) is to round item sizes, task times, etc. to integers, and solve with DP. Then runtime is in item size, so we must scale items to a certain function of $n, \\epsilon^{-1}$ and round to integers, in order to get good relative error $\\epsilon$ and poly runtime. This general strategy of taking some optimization problem, rounding, solving is called \"relaxation\". We have several examples: Traveling Salesman LP General strategy: - write our problem as some integral linear problem - scale, solve in fractions, round to ints Vertex Cover Facility Location $O(m)$ many costs of opening facilities, $O(mn)$ many costs for matching customers to facilities (each facility can support inf. many customers). We can set up an integer LP for aour booleans denoting which facility each customer goes to. Take the average cost of connecting each customer, and cluster those that are at least $\\rho$ times the average. Max SAT Problem setup: we have several clauses, each of which is boolean OR of inputs (or their negations). Try to satisfy as many as possible with some input. Clearly $OR(y_{i})\\leq \\sum_{i}y_{i},$ so we have the LP $$ \\begin{align} \\max &\\sum z_{j},\\ z_{j}&\\leq \\sum {i \\text{ negated}}(1-y {i}) + \\sum_{i\\text{ not negated}}y_{i}. \\end{align} $$ Then given each fractional $y_{i}$ we round down to $0$ with likelihood $1-y$ and round to 1 with likelihood $y$. Then we can look at expected value of sum of all our $z_{i}.$ Take our $y_{i}$ summing to at least $z_{j}$, then probability $z_{j}$ is 1 is $$1 - \\prod(1-y_{i})\\geq 1-\\left( 1-\\frac{z}{k} \\right)^{k}\\geq \\left( 1-\\frac{1}{e} \\right)z.$$ In short: solve our LP, take our $y$, round probabilistically, and generate some $\\sum z$ expected value is good. If we combo LP and randomized expected, can do even better. We can \"cheat\" by breaking into two pieces, each of which has expected approximation error $1-\\left( 1-\\frac{2}{k} \\right)^{k/2}$ instead of $1-\\left( 1-\\frac{1}{k} \\right)^{k},$ just keep breaking things down.","title":"6.5210.15   Approximation Algorithms"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#greedy-algorithms","text":"Often we can get some $C$-approximation (frequently 2 or 3/2) by just making some simplifying assumption, doing things greedily, and then double- or triple-counting to show that our greedy construction is not that much worse than opt. [!important] Matching Classic example. Just take heavy edges greedily; 2-approximation to opt.","title":"Greedy Algorithms"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#scheduling-theory","text":"We have tasks to be completed by machines in parallel, under certain constraints, with certain objective.","title":"Scheduling Theory"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#notation","text":"machines | constraints | objective We have some number of machines $1$ or $m$. Constraints could be unit time tasks, $r_{j}$ \"release dates\", $d_{j}$ deadlines for discrete objective. Objective could be number of late tasks, combined total lateness, time to complete last task, average \"freed time\" (timestamp at which machine $i$ is no longer needed).","title":"Notation"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#fpas-pas","text":"We want to get within $\\epsilon$ of error. PAS means for any fixed $\\epsilon$ we can get within $\\epsilon$ relative error in $n-$polytime. FPAS (fully polynomial approx. scheme) means we can get within $\\epsilon$ relative error in $n, \\epsilon^{-1}$ polytime. One common paradigm (for knapsack, bin packing, etc.) is to round item sizes, task times, etc. to integers, and solve with DP. Then runtime is in item size, so we must scale items to a certain function of $n, \\epsilon^{-1}$ and round to integers, in order to get good relative error $\\epsilon$ and poly runtime. This general strategy of taking some optimization problem, rounding, solving is called \"relaxation\". We have several examples:","title":"FPAS, PAS"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#traveling-salesman","text":"","title":"Traveling Salesman"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#lp","text":"General strategy: - write our problem as some integral linear problem - scale, solve in fractions, round to ints","title":"LP"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#vertex-cover","text":"","title":"Vertex Cover"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#facility-location","text":"$O(m)$ many costs of opening facilities, $O(mn)$ many costs for matching customers to facilities (each facility can support inf. many customers). We can set up an integer LP for aour booleans denoting which facility each customer goes to. Take the average cost of connecting each customer, and cluster those that are at least $\\rho$ times the average.","title":"Facility Location"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.15%20-%20Approximation%20Algorithms/#max-sat","text":"Problem setup: we have several clauses, each of which is boolean OR of inputs (or their negations). Try to satisfy as many as possible with some input. Clearly $OR(y_{i})\\leq \\sum_{i}y_{i},$ so we have the LP $$ \\begin{align} \\max &\\sum z_{j},\\ z_{j}&\\leq \\sum {i \\text{ negated}}(1-y {i}) + \\sum_{i\\text{ not negated}}y_{i}. \\end{align} $$ Then given each fractional $y_{i}$ we round down to $0$ with likelihood $1-y$ and round to 1 with likelihood $y$. Then we can look at expected value of sum of all our $z_{i}.$ Take our $y_{i}$ summing to at least $z_{j}$, then probability $z_{j}$ is 1 is $$1 - \\prod(1-y_{i})\\geq 1-\\left( 1-\\frac{z}{k} \\right)^{k}\\geq \\left( 1-\\frac{1}{e} \\right)z.$$ In short: solve our LP, take our $y$, round probabilistically, and generate some $\\sum z$ expected value is good. If we combo LP and randomized expected, can do even better. We can \"cheat\" by breaking into two pieces, each of which has expected approximation error $1-\\left( 1-\\frac{2}{k} \\right)^{k/2}$ instead of $1-\\left( 1-\\frac{1}{k} \\right)^{k},$ just keep breaking things down.","title":"Max SAT"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.16%20-%20Fixed%20Parameter%20Tractability/","text":"Given a fixed parameter $k$ we're able to exress the problem in some fixed constant difficulty in $k$. Specifically we're able to make a reduction to something that's bounded size in $k$. That's basically it.","title":"6.5210.16   Fixed Parameter Tractability"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/","text":"Algos that react to inputs a little bit at a time. Examples Ski rentals : each day I choose to ski or not to ski, I can choose to rent at cost $1$ or buy at cost $T$. Competitive analysis is the best my algorithm can do vs opt against an adversary that is picking the sequence. If my algorithm is to rent $T$ times before buying, get a factor of 2. Finance : suppose I'm selling a car, and bids come in within some range $[m,M].$ I know when the last offer is. Let $\\phi=\\frac{M}{m}.$ Deterministically I can take the first offer above $\\sqrt{ mM },$ if I take then I'm at most $\\sqrt{ \\phi }$ below the max, and if I never take the max is at most $\\sqrt{ mM }$ so last offer is at most $\\sqrt{ \\phi }$ below the max. Randomly I can pick a power of 2 cutoff price from $[m,M]$ so I have $\\frac{1}{\\log \\phi}$ chance of being within $2$ of the best. Then my expected selling price is at least on the order of $\\frac{1}{\\log \\phi}$ of the best. Scheduling : for the problem $P||\\max C_{j}$ (no constrains, minimax load) assign each job to the least currently loaded machine. We call this Graham's Rule . Take* largest-load machine $M_{}$ with last job $p.$ Then all other loads are at least $M_{}-p$ so opt is at least $\\frac{{m(M-p)+p}}{m}=M-p+\\frac{p}{m},$ and also $opt\\geq p,$ so $opt\\left( 2-\\frac{1}{m} \\right)\\geq M.$ Various randomized algorithms have gotten $2-o(1)$ (none better than 1.9). Paging : we have memory and a cache, whenever we miss we have to evict and pay a price. Online algorithm is really about picking which item to evict. Deterministic Paging - Last in First out - First in First out - Flush when full - Least frequently used - Least recently used LIFO, LFU not competitive Lower bound: no online algorithm beats $k$-competitive. Have a $k+1$-page adversary that just calls whichever page is not in the cache\u2014faults every time. But offline algorithm can lookahead and evict whichever page is not included in the next $k$ queries. This offline algorithm called Longest Forward Distance is optimal. Randomized Paging Called Fiat marking: all pages start unmarked, when evicting pick random unmarked page and mark new page. Sort of like randomized FIFO. Can induct to get a harmonic series EV yielding $\\log k$-competitive. Types of Adversaries oblivious knows probability distribution but not the tosses, probabilistic opt adaptive knows tosses up to now fully adaptive knows all tosses, might as well play best determinstic K-server problem In some metric space requests appear in $d$-dimensional space and we have $k$ servers that have to move in space to serve them. This generalizes paging; the paging problem is when distance between any two points is $1,$ each server represents a cache item. On a Line (1D) Intuition: greedy doesn't work because if we have servers at $0,1$ and requests that alternate between $0, 0.1$ then this becomes ski rental. Hence we want to \"count\" or \"build up\" potential to move in servers from \"far away\". This motivates double coverage where both of the closest servers move closer towards the desired point. If new request is outside convex hull, move single closest server. Claim this is $k$-competitive. Define potential function using in-order matching cost $M$ and pairwise $\\binom n 2$ distances in the DC setup. Opt moving by $d$ increases $M$ term by at most $d$ (it's just matching). Now we analyze the effect of DC moving on this potential. 1. if moving to outside hull: - definitely moving \"correctly\" so $\\Delta M=-d$ - But $\\Delta\\Sigma=(k-1)d$ since moving farther from all other $k-1$ points 2. if moving to inside hull: - DC moving by $d$, at least one of the two servers is moving in the right direction so $M$ doesn't increase for suff. small $d$ - Since two moving servers are adjacent, pairwise sum distances goes down by $d$ SOTA for generalizations This generalizes to trees for randomized $O(\\log ^{3}n\\log ^{2}k)$ for $k$-server using tree embeddings (!?). Have also achieved $2k$ for general $k$-server using some magic function.","title":"6.5210.17   Online Algorithms, Paging"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/#examples","text":"Ski rentals : each day I choose to ski or not to ski, I can choose to rent at cost $1$ or buy at cost $T$. Competitive analysis is the best my algorithm can do vs opt against an adversary that is picking the sequence. If my algorithm is to rent $T$ times before buying, get a factor of 2. Finance : suppose I'm selling a car, and bids come in within some range $[m,M].$ I know when the last offer is. Let $\\phi=\\frac{M}{m}.$ Deterministically I can take the first offer above $\\sqrt{ mM },$ if I take then I'm at most $\\sqrt{ \\phi }$ below the max, and if I never take the max is at most $\\sqrt{ mM }$ so last offer is at most $\\sqrt{ \\phi }$ below the max. Randomly I can pick a power of 2 cutoff price from $[m,M]$ so I have $\\frac{1}{\\log \\phi}$ chance of being within $2$ of the best. Then my expected selling price is at least on the order of $\\frac{1}{\\log \\phi}$ of the best. Scheduling : for the problem $P||\\max C_{j}$ (no constrains, minimax load) assign each job to the least currently loaded machine. We call this Graham's Rule . Take* largest-load machine $M_{}$ with last job $p.$ Then all other loads are at least $M_{}-p$ so opt is at least $\\frac{{m(M-p)+p}}{m}=M-p+\\frac{p}{m},$ and also $opt\\geq p,$ so $opt\\left( 2-\\frac{1}{m} \\right)\\geq M.$ Various randomized algorithms have gotten $2-o(1)$ (none better than 1.9). Paging : we have memory and a cache, whenever we miss we have to evict and pay a price. Online algorithm is really about picking which item to evict. Deterministic Paging - Last in First out - First in First out - Flush when full - Least frequently used - Least recently used LIFO, LFU not competitive Lower bound: no online algorithm beats $k$-competitive. Have a $k+1$-page adversary that just calls whichever page is not in the cache\u2014faults every time. But offline algorithm can lookahead and evict whichever page is not included in the next $k$ queries. This offline algorithm called Longest Forward Distance is optimal. Randomized Paging Called Fiat marking: all pages start unmarked, when evicting pick random unmarked page and mark new page. Sort of like randomized FIFO. Can induct to get a harmonic series EV yielding $\\log k$-competitive.","title":"Examples"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/#types-of-adversaries","text":"oblivious knows probability distribution but not the tosses, probabilistic opt adaptive knows tosses up to now fully adaptive knows all tosses, might as well play best determinstic","title":"Types of Adversaries"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/#k-server-problem","text":"In some metric space requests appear in $d$-dimensional space and we have $k$ servers that have to move in space to serve them. This generalizes paging; the paging problem is when distance between any two points is $1,$ each server represents a cache item.","title":"K-server problem"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/#on-a-line-1d","text":"Intuition: greedy doesn't work because if we have servers at $0,1$ and requests that alternate between $0, 0.1$ then this becomes ski rental. Hence we want to \"count\" or \"build up\" potential to move in servers from \"far away\". This motivates double coverage where both of the closest servers move closer towards the desired point. If new request is outside convex hull, move single closest server. Claim this is $k$-competitive. Define potential function using in-order matching cost $M$ and pairwise $\\binom n 2$ distances in the DC setup. Opt moving by $d$ increases $M$ term by at most $d$ (it's just matching). Now we analyze the effect of DC moving on this potential. 1. if moving to outside hull: - definitely moving \"correctly\" so $\\Delta M=-d$ - But $\\Delta\\Sigma=(k-1)d$ since moving farther from all other $k-1$ points 2. if moving to inside hull: - DC moving by $d$, at least one of the two servers is moving in the right direction so $M$ doesn't increase for suff. small $d$ - Since two moving servers are adjacent, pairwise sum distances goes down by $d$","title":"On a Line (1D)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.17%20-%20Online%20Algorithms%2C%20Paging/#sota-for-generalizations","text":"This generalizes to trees for randomized $O(\\log ^{3}n\\log ^{2}k)$ for $k$-server using tree embeddings (!?). Have also achieved $2k$ for general $k$-server using some magic function.","title":"SOTA for generalizations"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/","text":"Orthogonal Range Queries Given points in $\\mathbb{R}^{n},$ how do we find all points in some axis-aligned box in $O(A+\\log n)$ time, where $A$ is the answer? Assume $2^{n}$ points Create data structure for each \"block\" of length power of 2 e.g. 2D case break $x$-interval into $\\log n$ preprocessed slabs Each preprocessed slab is BST of $y$-coordinates Total space taken is $O(n\\log n).$ Our construction cost is $O(n\\log ^{2}n).$ Query cost is $O(\\log ^{2}n).$ In general it's $\\log ^{d}n.$ If we want dynamic updates can use tree heap with faster insert/deletion times. Plane/Line Sweep, Convex Hull For cases of low dimension can just sweep. E.g. for 2D convex hull, use Graham's line sweep. Sort by $x$-value, grab the lowest x-value (guaranteed to be on CH). Take a line through $P$ and rotate, maintaining that all \"turns\" in our growing path go only rightwards. Maintain with a stack\u2014pop last, check angle with last 2 until consistent, then insert. Costs $O(n\\log n).$ Can do recursively with smart scheduling to get $O(n\\log h)$ where $h$ is the size of the answer. Can also do with randomized incremental construction . Given convex hull and new point, take arbitrary line through new point. If two intersections, inside hull; otherwise outside hull. Use sorted BBST to find vertices directly above/below the line and check for intersections. In case of new, can find new neighbors in $\\log n$ time with exponential momentum. For 3D, note linearly many edges/faces. Can naively check if inside the hull in $O(n)$ time, and delete faces in $O(n)$ time (check which side of half-plane new point is on). Say a vertex can \"see\" a face if it's on the wrong half-side of it. Clarkson-Shor gets $O(n\\log n)$ speedup. Voronoi diagrams Given fixed servers, we want to find the closest server to some query point in 2D Euclidean space. The bisectors partition into polytopes. Adding point at infinity creates a planar graph with exactly $n+1$ faces and each vertex has degree at most 3, so linearly many edges and vertices . We can do sweep line again\u2014except we are only sure about the status of points that are closer to a server than to the line, i.e. union of parabolas. Call this union the beach line . Note that by parabola definition, intersection of parabolas must lie on bisectors, i.e. \"lowest\" intersections are points that actually lie on edges. They are equidistant from foci and the line. As line sweeps, we track events online: - site event where we hit new point, an edge could now split into two - circle event where two edges meet, now combines into one and one parabola can be dropped We implement this using BBST on adjacent (intersecting) parabola pieces. There are linearly many inserts/deletions (linear in the number of edges/vertices) so $O(n\\log n)$ runtime. Remark BSP: binary space partitioning, used to partition objects/meshes into halves, can render the closer half without any concern for the farther half - gives a robust ordering for rendering Low-dimensional LP Goal: $O(d!n)$ time.","title":"6.5210.18   Computational Geometry"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/#orthogonal-range-queries","text":"Given points in $\\mathbb{R}^{n},$ how do we find all points in some axis-aligned box in $O(A+\\log n)$ time, where $A$ is the answer? Assume $2^{n}$ points Create data structure for each \"block\" of length power of 2 e.g. 2D case break $x$-interval into $\\log n$ preprocessed slabs Each preprocessed slab is BST of $y$-coordinates Total space taken is $O(n\\log n).$ Our construction cost is $O(n\\log ^{2}n).$ Query cost is $O(\\log ^{2}n).$ In general it's $\\log ^{d}n.$ If we want dynamic updates can use tree heap with faster insert/deletion times.","title":"Orthogonal Range Queries"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/#planeline-sweep-convex-hull","text":"For cases of low dimension can just sweep. E.g. for 2D convex hull, use Graham's line sweep. Sort by $x$-value, grab the lowest x-value (guaranteed to be on CH). Take a line through $P$ and rotate, maintaining that all \"turns\" in our growing path go only rightwards. Maintain with a stack\u2014pop last, check angle with last 2 until consistent, then insert. Costs $O(n\\log n).$ Can do recursively with smart scheduling to get $O(n\\log h)$ where $h$ is the size of the answer. Can also do with randomized incremental construction . Given convex hull and new point, take arbitrary line through new point. If two intersections, inside hull; otherwise outside hull. Use sorted BBST to find vertices directly above/below the line and check for intersections. In case of new, can find new neighbors in $\\log n$ time with exponential momentum. For 3D, note linearly many edges/faces. Can naively check if inside the hull in $O(n)$ time, and delete faces in $O(n)$ time (check which side of half-plane new point is on). Say a vertex can \"see\" a face if it's on the wrong half-side of it. Clarkson-Shor gets $O(n\\log n)$ speedup.","title":"Plane/Line Sweep, Convex Hull"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/#voronoi-diagrams","text":"Given fixed servers, we want to find the closest server to some query point in 2D Euclidean space. The bisectors partition into polytopes. Adding point at infinity creates a planar graph with exactly $n+1$ faces and each vertex has degree at most 3, so linearly many edges and vertices . We can do sweep line again\u2014except we are only sure about the status of points that are closer to a server than to the line, i.e. union of parabolas. Call this union the beach line . Note that by parabola definition, intersection of parabolas must lie on bisectors, i.e. \"lowest\" intersections are points that actually lie on edges. They are equidistant from foci and the line. As line sweeps, we track events online: - site event where we hit new point, an edge could now split into two - circle event where two edges meet, now combines into one and one parabola can be dropped We implement this using BBST on adjacent (intersecting) parabola pieces. There are linearly many inserts/deletions (linear in the number of edges/vertices) so $O(n\\log n)$ runtime.","title":"Voronoi diagrams"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/#remark","text":"BSP: binary space partitioning, used to partition objects/meshes into halves, can render the closer half without any concern for the farther half - gives a robust ordering for rendering","title":"Remark"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.18%20-%20Computational%20Geometry/#low-dimensional-lp","text":"Goal: $O(d!n)$ time.","title":"Low-dimensional LP"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.19%20-%20Memory%20Hierarchies/","text":"External Memory Have some large dataset with size $N$ in \"external memory\" (disk) Memory is chunked into blocks of length $B,$ each of which takes $O(1)$ to read into/write from internal memory with size $M$ Operations in internal memory are assumed to be free Examples: Matmul, by loading submatrices with side length $\\sqrt{ M }$ Sorted linked list, by keeping auxiliary data about segment in each block B-trees which grab all children on one read However, we want to be cache-size oblivious. Divide-and-Conquer is nice lol, will auto-size","title":"6.5210.19   Memory Hierarchies"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.19%20-%20Memory%20Hierarchies/#external-memory","text":"Have some large dataset with size $N$ in \"external memory\" (disk) Memory is chunked into blocks of length $B,$ each of which takes $O(1)$ to read into/write from internal memory with size $M$ Operations in internal memory are assumed to be free Examples: Matmul, by loading submatrices with side length $\\sqrt{ M }$ Sorted linked list, by keeping auxiliary data about segment in each block B-trees which grab all children on one read However, we want to be cache-size oblivious.","title":"External Memory"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.19%20-%20Memory%20Hierarchies/#divide-and-conquer","text":"is nice lol, will auto-size","title":"Divide-and-Conquer"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.2%20-%20Fib%20Heaps/","text":"insert pop min Current approach: Binary-order HOTs Compare only if same degree ( union by rank , referring to # children per root) Then we get a binomial tree \u2014with $2^{n}$ nodes, root has $n$ children each root for a $2^{i}$ binomial tree. This collection of binomial trees forms a binomial heap . Insertion, merging are all amortized $O(1)$ (creating new 1-node trees lazily) and min deletion is amortized $O(\\log n)$. Decrease Key We want to cut a node, promote to root, rewrite its value. The $2^{k}$ invariant is softened a little bit\u2014nodes can only ever lose one child. If a node loses two children then it is also cut from its parent, and we keep going. If the entire path up to root has all already been cut once, then we get lots of cutting. Now we set $\\phi=2\\cdot n_{marked}+n_{roots}$. Then when we delete key , in the case of no cut $\\phi \\mathrel{+}=3$, in the case of parent cut we get an extra operation and extra root but $n_{marked}$ went down by 2. In this way, amortized is $O(1)$. Fibonacci naming If size of tree of order $n$ is $S_{n}$ then the worst case is $$ S_{n}=\\sum ^{n-2}S_{i}. $$ Given $S_{1}=1$ and $S_{2}=2$ we get $S_{n}=F_{n}$, the Fibonacci numbers. The marking and cascading cut strategy is called Saving Private Ryan . Practical considerations Binary heap is just flat \u2014there's no auxiliary stuff, no pointers. Fibonacci heap is not a full binary tree so have to do tree pointer manipulation, which is not cache friendly. Back to MST Prim's says: grow a single tree. Kruskal's says: grab edges repeatedly. The $\\log n$ cost comes from having a large heap\u2014what if every time our current tree gets big, what if we just hop to another node and start again? Let's make trees of size $k$ and then contract them into a single super-node. We can just use $k\\approx 2^{m/t}$, since our cost is $m+t\\log k$. After every phase we divide $t$ by $k$, so $k'=2^{m/tk}=k2^{k}\\approx 2^{k}$, so we get power tower ($\\log ^*$) number of phases.","title":"6.5210.2   Fib Heaps"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.2%20-%20Fib%20Heaps/#decrease-key","text":"We want to cut a node, promote to root, rewrite its value. The $2^{k}$ invariant is softened a little bit\u2014nodes can only ever lose one child. If a node loses two children then it is also cut from its parent, and we keep going. If the entire path up to root has all already been cut once, then we get lots of cutting. Now we set $\\phi=2\\cdot n_{marked}+n_{roots}$. Then when we delete key , in the case of no cut $\\phi \\mathrel{+}=3$, in the case of parent cut we get an extra operation and extra root but $n_{marked}$ went down by 2. In this way, amortized is $O(1)$.","title":"Decrease Key"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.2%20-%20Fib%20Heaps/#fibonacci-naming","text":"If size of tree of order $n$ is $S_{n}$ then the worst case is $$ S_{n}=\\sum ^{n-2}S_{i}. $$ Given $S_{1}=1$ and $S_{2}=2$ we get $S_{n}=F_{n}$, the Fibonacci numbers. The marking and cascading cut strategy is called Saving Private Ryan .","title":"Fibonacci naming"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.2%20-%20Fib%20Heaps/#practical-considerations","text":"Binary heap is just flat \u2014there's no auxiliary stuff, no pointers. Fibonacci heap is not a full binary tree so have to do tree pointer manipulation, which is not cache friendly.","title":"Practical considerations"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.2%20-%20Fib%20Heaps/#back-to-mst","text":"Prim's says: grow a single tree. Kruskal's says: grab edges repeatedly. The $\\log n$ cost comes from having a large heap\u2014what if every time our current tree gets big, what if we just hop to another node and start again? Let's make trees of size $k$ and then contract them into a single super-node. We can just use $k\\approx 2^{m/t}$, since our cost is $m+t\\log k$. After every phase we divide $t$ by $k$, so $k'=2^{m/tk}=k2^{k}\\approx 2^{k}$, so we get power tower ($\\log ^*$) number of phases.","title":"Back to MST"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/","text":"DSs satisfy one of the three: - ephemeral (query, update the present) - partial persistence (update present, query past) - full persistence (git branch the past) Stupid 1 (fat BST) Our model consists of addresses each associated with some values and some pointers. To make reads/writes persistent, replace each field with a timestamped log. Then each update costs constant space, and reads are $O(\\log n)$. For a BST of such addresses, we need $\\log n$ pointer jumps, and each pointer read takes $\\log t$ (for $t$ updates), so lookup is $\\log n\\log t$. Stupid 2 (naive Path Copying) Instead, when we change node $n$, we log the entire path from root $r$ to $n$ at time $t$. The nodes on the path include child pointers to nodes on previous logs , i.e. each node appears in several logs and a single pointer in a single log can be pointed to from several other logs. Then querying at a timestamp is just finding the root path that we care about and traversing. Solution (lazy Path Copying) Let's relax path copying a bit, Fib-tree style. Each node has one field for potential update. When we overflow, copy that node (one step of the recursive path copy) and add field update to parent. If parent is also full, keep going, potentially to root path-copy style if all nodes along the way are full. Summary Top-level structure is an array of roots, each of which is full (has both fields full). Each field contains timestamp and a pointer to another node. There could be multiple copies of any given node \"floating around\", with their fields corresponding to increasing timestamps. Recall that each node will actually have several fields (children, parent, etc.) and the ones that didn't change would have to be copied. Time analysis On queries, find the root we care about. Then for each node go left/right, pick the version we want, and keep going down, yielding $\\log n$ lookup. After finding the node updates are amortized $O(\\log n)$. Use $\\phi$ equalling number of full nodes in the present, then after we find the node we either make it full (increasing $\\phi$) or path-copy it (real cost 1, decrease $\\phi$ by 1). Application Problem Planar Point Location Given a decomposition of the plane into polygons, determine which polygon a given point lies in. Cut into vertical slabs by $x=x_{i}$ for each node. Within each slab, all relevant segments are well-ordered. Then we slide from left to right, adjusting the segments incident to the left node (removing ones going left, adding ones going right). Then for any query we slide to the root corresponding to the correct slab, then conduct BST search. For cherry on top, use balanced BST (red-black tree, or AVL, or 2-3 tree).","title":"6.5210.3   Persistence"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#stupid-1-fat-bst","text":"Our model consists of addresses each associated with some values and some pointers. To make reads/writes persistent, replace each field with a timestamped log. Then each update costs constant space, and reads are $O(\\log n)$. For a BST of such addresses, we need $\\log n$ pointer jumps, and each pointer read takes $\\log t$ (for $t$ updates), so lookup is $\\log n\\log t$.","title":"Stupid 1 (fat BST)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#stupid-2-naive-path-copying","text":"Instead, when we change node $n$, we log the entire path from root $r$ to $n$ at time $t$. The nodes on the path include child pointers to nodes on previous logs , i.e. each node appears in several logs and a single pointer in a single log can be pointed to from several other logs. Then querying at a timestamp is just finding the root path that we care about and traversing.","title":"Stupid 2 (naive Path Copying)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#solution-lazy-path-copying","text":"Let's relax path copying a bit, Fib-tree style. Each node has one field for potential update. When we overflow, copy that node (one step of the recursive path copy) and add field update to parent. If parent is also full, keep going, potentially to root path-copy style if all nodes along the way are full.","title":"Solution (lazy Path Copying)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#summary","text":"Top-level structure is an array of roots, each of which is full (has both fields full). Each field contains timestamp and a pointer to another node. There could be multiple copies of any given node \"floating around\", with their fields corresponding to increasing timestamps. Recall that each node will actually have several fields (children, parent, etc.) and the ones that didn't change would have to be copied.","title":"Summary"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#time-analysis","text":"On queries, find the root we care about. Then for each node go left/right, pick the version we want, and keep going down, yielding $\\log n$ lookup. After finding the node updates are amortized $O(\\log n)$. Use $\\phi$ equalling number of full nodes in the present, then after we find the node we either make it full (increasing $\\phi$) or path-copy it (real cost 1, decrease $\\phi$ by 1).","title":"Time analysis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#application","text":"","title":"Application"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.3%20-%20Persistence/#problem","text":"Planar Point Location Given a decomposition of the plane into polygons, determine which polygon a given point lies in. Cut into vertical slabs by $x=x_{i}$ for each node. Within each slab, all relevant segments are well-ordered. Then we slide from left to right, adjusting the segments incident to the left node (removing ones going left, adding ones going right). Then for any query we slide to the root corresponding to the correct slab, then conduct BST search. For cherry on top, use balanced BST (red-black tree, or AVL, or 2-3 tree).","title":"Problem"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/","text":"Typical BBST - Auxiliary info track (or correlate to) heights - Rotate to re-balance when heights are to far off (large subtree deep in tree) Consider the \"worst\" example with a left-chain to smallest value $x$. Then to bring $x$ to the top we rotate node by node. This has some bad stuff\u2014the resulting tree is still a long chain. Fundamental Double Rotations We want to rotate $x$ up two levels. Going up take $y,z$. - If same direction ($x<y y$, $y<x$) then rot $x$ then rot $z$, so that $x$ is root and $y,z$ are children. Then we define Splay($x$) defined as - use double rot plus maybe a single rot to move $x$ to root Search is defined as usual - Left, right etc. - Splay to root Amortized Analysis Take weights $w_{x}$ for each node (in example set to $w_{x}=1$). Then let - $s(x)$ be sum of weights in subtree, - $r(x)=\\log_{2}(s(x))$. Finally, use $$ \\phi:=\\sum_{x}r(x). $$ Access Lemma Amortized time to splay $x$ to root $t$ is at most $$3(r(t)-r(x))+1.$$ We just examine each operation\u2014zigzig and zigzag\u2014to verify the lemma. We want to show $cost \\leq 3(r'(x)-r(x))$ and then can telescope. ![[IMG_6692.jpeg]] Zigzig The real cost of double rot is $2$, and $\\Delta \\phi=\\sum_{x,y,z}\\Delta r(k)$. Note that $r'(x)=r(z),$ so then we get $$ \\begin{align } 2+\\Delta \\phi &= 2+r'(y)+r'(z)-r(x)-r(y)\\ &\\leq 2+r'(x)+r'(z)-r(x)-r(x)\\ &=2+r'(z)-r(x) + (r'(x)-r(x)).\\ \\end{align } $$ Then $$ 2+r'(z)+r(x)=\\log_{2}(2s'(z))+\\log_{2}(2s(x))\\leq 2\\log_{2}(s'(z)+s(x))\\leq 2\\log_{2}(s'(x)). $$ Hence $$ 2+\\Delta \\phi\\leq 3(r'(x)-r(x)). $$ Zigzag Exercise to the reader. Nice Properties We can just get certain properties for free by picking the weights $w$. Info-theory mumbo jumbo Splay trees are good for high-query situations. If items are queried a lot they should be closer to the top. Idea is that you should do $\\log p$ work for items accessed $p$ of the time.Splay tree does this well, called \"static optimality\". \"Finger splay tree\" If we start with a finger at node $f$, set $w_{x}=\\frac{1}{1+(\\Delta r)^{2}}$ and we get the property we want. Working set Set $w_{x}=\\log(\\text{distinct items accessed})$ and more recent items cost less. For any given task we just set the weights $w_{x}$ to what we want. Technically the non-constant weights $w_{x}$ means we have to re-derive parts of the Access Lemma proof, but otherwise it's the same.","title":"6.5210.4   Splay Trees"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#fundamental-double-rotations","text":"We want to rotate $x$ up two levels. Going up take $y,z$. - If same direction ($x<y y$, $y<x$) then rot $x$ then rot $z$, so that $x$ is root and $y,z$ are children. Then we define Splay($x$) defined as - use double rot plus maybe a single rot to move $x$ to root Search is defined as usual - Left, right etc. - Splay to root","title":"Fundamental Double Rotations"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#amortized-analysis","text":"Take weights $w_{x}$ for each node (in example set to $w_{x}=1$). Then let - $s(x)$ be sum of weights in subtree, - $r(x)=\\log_{2}(s(x))$. Finally, use $$ \\phi:=\\sum_{x}r(x). $$ Access Lemma Amortized time to splay $x$ to root $t$ is at most $$3(r(t)-r(x))+1.$$ We just examine each operation\u2014zigzig and zigzag\u2014to verify the lemma. We want to show $cost \\leq 3(r'(x)-r(x))$ and then can telescope. ![[IMG_6692.jpeg]]","title":"Amortized Analysis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#zigzig","text":"The real cost of double rot is $2$, and $\\Delta \\phi=\\sum_{x,y,z}\\Delta r(k)$. Note that $r'(x)=r(z),$ so then we get $$ \\begin{align } 2+\\Delta \\phi &= 2+r'(y)+r'(z)-r(x)-r(y)\\ &\\leq 2+r'(x)+r'(z)-r(x)-r(x)\\ &=2+r'(z)-r(x) + (r'(x)-r(x)).\\ \\end{align } $$ Then $$ 2+r'(z)+r(x)=\\log_{2}(2s'(z))+\\log_{2}(2s(x))\\leq 2\\log_{2}(s'(z)+s(x))\\leq 2\\log_{2}(s'(x)). $$ Hence $$ 2+\\Delta \\phi\\leq 3(r'(x)-r(x)). $$","title":"Zigzig"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#zigzag","text":"Exercise to the reader.","title":"Zigzag"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#nice-properties","text":"We can just get certain properties for free by picking the weights $w$.","title":"Nice Properties"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#info-theory-mumbo-jumbo","text":"Splay trees are good for high-query situations. If items are queried a lot they should be closer to the top. Idea is that you should do $\\log p$ work for items accessed $p$ of the time.Splay tree does this well, called \"static optimality\".","title":"Info-theory mumbo jumbo"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#finger-splay-tree","text":"If we start with a finger at node $f$, set $w_{x}=\\frac{1}{1+(\\Delta r)^{2}}$ and we get the property we want.","title":"\"Finger splay tree\""},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.4%20-%20Splay%20Trees/#working-set","text":"Set $w_{x}=\\log(\\text{distinct items accessed})$ and more recent items cost less. For any given task we just set the weights $w_{x}$ to what we want. Technically the non-constant weights $w_{x}$ means we have to re-derive parts of the Access Lemma proof, but otherwise it's the same.","title":"Working set"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.5%20-%20Splay%20Cont.%2C%20Buckets/","text":"Splay Trees Insert: insert at leaf then splay Delete: delete as normal then splay parent Split: find $x$, splay, take subtree Merge of majorizing trees: Splay $\\max(T_{1}),$ attach $T_{2}$ as right subtree We can abstract insert and delete as combination of split and merge. [!warning] Splays are asymptotically \"paid for\" by potential, but inserts/deletes aren't accounted for. Recall to analyze $\\Delta \\phi$ on inserts. Potential issues amortized changes on reads We can try to \"only splay on really long searches\" or \"stop splaying\" according to \"statically optimal word assignment\". Buckets (finite-bit items [ints]) Let's do Dijkstra's, but for integer length edges. For a reminder, the algorithm is - priority queue of nodes - pop-min - decrease-key for neighbors - repeat Suppose max edge length is small. Dial's We want a big priority queue. However, we have assumption of high collisions, so we can have a priority queue of equivalence classes. Each class has same distance from source. Classes can be represented cheaply as flat arrays, linked list, etc. If we address by mod $C+1$ distance from source, we maintain a $C+1$-length array of LL buckets, which have a total of $m$ insertions/deletions. So storage is ok for now. This yields Dial's algorithm with runtime $O(m+nC)$. Now consider tower of empty bit indicators, also called summary structures . We could populate these immediately, which would also take a lot of space, or ... we could do things lazily . 1. Keep our tower of summary structures. 2. Only one path of buckets from \"root\" to \"address\" is maintained 3. Each node ever exists in one bucket 4. When we go down a new \"path\" we create new buckets, and prune its \"sibling\" (which we will never process again) Each bucket is a collection of items, represented by LL. Keep an auxiliary pointing from each node id to its pointer, so removal from bucket is easy and hence decrease-key is remove and insert .","title":"6.5210.5   Splay Cont., Buckets"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.5%20-%20Splay%20Cont.%2C%20Buckets/#splay-trees","text":"Insert: insert at leaf then splay Delete: delete as normal then splay parent Split: find $x$, splay, take subtree Merge of majorizing trees: Splay $\\max(T_{1}),$ attach $T_{2}$ as right subtree We can abstract insert and delete as combination of split and merge. [!warning] Splays are asymptotically \"paid for\" by potential, but inserts/deletes aren't accounted for. Recall to analyze $\\Delta \\phi$ on inserts.","title":"Splay Trees"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.5%20-%20Splay%20Cont.%2C%20Buckets/#potential-issues","text":"amortized changes on reads We can try to \"only splay on really long searches\" or \"stop splaying\" according to \"statically optimal word assignment\".","title":"Potential issues"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.5%20-%20Splay%20Cont.%2C%20Buckets/#buckets-finite-bit-items-ints","text":"Let's do Dijkstra's, but for integer length edges. For a reminder, the algorithm is - priority queue of nodes - pop-min - decrease-key for neighbors - repeat Suppose max edge length is small.","title":"Buckets (finite-bit items [ints])"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.5%20-%20Splay%20Cont.%2C%20Buckets/#dials","text":"We want a big priority queue. However, we have assumption of high collisions, so we can have a priority queue of equivalence classes. Each class has same distance from source. Classes can be represented cheaply as flat arrays, linked list, etc. If we address by mod $C+1$ distance from source, we maintain a $C+1$-length array of LL buckets, which have a total of $m$ insertions/deletions. So storage is ok for now. This yields Dial's algorithm with runtime $O(m+nC)$. Now consider tower of empty bit indicators, also called summary structures . We could populate these immediately, which would also take a lot of space, or ... we could do things lazily . 1. Keep our tower of summary structures. 2. Only one path of buckets from \"root\" to \"address\" is maintained 3. Each node ever exists in one bucket 4. When we go down a new \"path\" we create new buckets, and prune its \"sibling\" (which we will never process again) Each bucket is a collection of items, represented by LL. Keep an auxiliary pointing from each node id to its pointer, so removal from bucket is easy and hence decrease-key is remove and insert .","title":"Dial's"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/","text":"Veb Queues Let's do a straight-up RAM priority queue (for integers). If values are at most $C$ then we have $b=\\log C$ digits. Split into $upper$ and $lower$. Then: - we want get-min and delete-min on upper - we want upper to index into a lower, which also supports get-min and delete-min Then we directly recurse data structures. Then our struct $Q^{b}$ is $Q.upper=Q^{b/2}$ and $Q.lower$ which is an array of length $\\sqrt{ C }$ of $Q^{b/2}$. Insertion Adjust $Q.min$ if necessary Insert $x.upper$ into $Q.upper$, adjust $Q.upper.min$ recursively etc. Insert $x.lower$ into $Q.lower[x.upper]$. Delete-min Grab $Q.upper.min=u_{m}$, and call delete-min on $Q.lower[u_{m}]$ If $Q.lower[u_{m}]$ is now empty then call delete-min on $Q.upper$, removing $u_{m}$ from the top structure Since the depth of our PQ is $\\log(b)$, we now get $O(\\log \\log n)$ for basic operations! Corollary Now by repeated insert and delete-min, we obtain $O(n\\log \\log n)$ time for sorting arrays of integers! Hashing Now we want to save space . Suppose we have a sparse set of long words, i.e. $n$ items in $[1,m]$. Indexing by these long words sucks; let's hash them and index by the hash output instead. We have $n$ items from $[1,m]$, hash output maps to $[1,s]$. [!important] Hash families 1. Collisions: fix with LL 2. Large LLs: try to get small collisions 3. Universal hashes have adversary: Be instance dependent/random Need a small family of hashes that we expect to not have too many collisions Let's introduce some randomness into our hash. Idea: perfectly random permutation ! For a random permutation, the indicator of collision $C_{i,j}$ has EV $\\frac{1}{s}$, so average collisions per item is $O\\left( \\frac{n}{s} \\right)$. Seems pretty good! Random permutations are hard to evaluate; instead let's just pick $H(x)\\equiv a\\cdot h(x)+b \\pmod p$ for random $a,b$. Then for any $i,j$ they collide with likelihood $O\\left( \\frac{1}{s} \\right)$ still, and we're very happy. Expectation of number of collisions (and hence lookup time) is $O\\left( \\frac{n}{s} \\right)$.","title":"6.5210.6   van Emde Boas Trees and Hashing"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/#veb-queues","text":"Let's do a straight-up RAM priority queue (for integers). If values are at most $C$ then we have $b=\\log C$ digits. Split into $upper$ and $lower$. Then: - we want get-min and delete-min on upper - we want upper to index into a lower, which also supports get-min and delete-min Then we directly recurse data structures. Then our struct $Q^{b}$ is $Q.upper=Q^{b/2}$ and $Q.lower$ which is an array of length $\\sqrt{ C }$ of $Q^{b/2}$.","title":"Veb Queues"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/#insertion","text":"Adjust $Q.min$ if necessary Insert $x.upper$ into $Q.upper$, adjust $Q.upper.min$ recursively etc. Insert $x.lower$ into $Q.lower[x.upper]$.","title":"Insertion"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/#delete-min","text":"Grab $Q.upper.min=u_{m}$, and call delete-min on $Q.lower[u_{m}]$ If $Q.lower[u_{m}]$ is now empty then call delete-min on $Q.upper$, removing $u_{m}$ from the top structure Since the depth of our PQ is $\\log(b)$, we now get $O(\\log \\log n)$ for basic operations!","title":"Delete-min"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/#corollary","text":"Now by repeated insert and delete-min, we obtain $O(n\\log \\log n)$ time for sorting arrays of integers!","title":"Corollary"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.6%20-%20van%20Emde%20Boas%20Trees%20and%20Hashing/#hashing","text":"Now we want to save space . Suppose we have a sparse set of long words, i.e. $n$ items in $[1,m]$. Indexing by these long words sucks; let's hash them and index by the hash output instead. We have $n$ items from $[1,m]$, hash output maps to $[1,s]$. [!important] Hash families 1. Collisions: fix with LL 2. Large LLs: try to get small collisions 3. Universal hashes have adversary: Be instance dependent/random Need a small family of hashes that we expect to not have too many collisions Let's introduce some randomness into our hash. Idea: perfectly random permutation ! For a random permutation, the indicator of collision $C_{i,j}$ has EV $\\frac{1}{s}$, so average collisions per item is $O\\left( \\frac{n}{s} \\right)$. Seems pretty good! Random permutations are hard to evaluate; instead let's just pick $H(x)\\equiv a\\cdot h(x)+b \\pmod p$ for random $a,b$. Then for any $i,j$ they collide with likelihood $O\\left( \\frac{1}{s} \\right)$ still, and we're very happy. Expectation of number of collisions (and hence lookup time) is $O\\left( \\frac{n}{s} \\right)$.","title":"Hashing"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/","text":"[!important] Recipe for Combinatorial Optimization In turn, understand: - all solutions - characteristics of opt - verification of opt - finding opt Analysis Framing \"Gross flow\" with nonnegative $g(v,w)$ along directed edges for adjacent vertices $v,w$, we always consider $g(v,w)-g(w,v)$, we have \"Directed net flow\" that can have negative numbers along edges with both min and max , only considers $g(v,w)$ with skew symmetry Decomposition Given a flow, take edges out of source iteratively Decompose flow into complete paths and cycles Verify if a Flow is Maximal Take residual graph where each edge has new bounds This graph has 0 flow\u2014take all points we can reach from source, this creates a cut with value 0 Algorithms Repeatedly look at residual, find augmenting path. Assuming integers with capacities $\\leq U$, we get at most $EU$ path sums.","title":"6.5210.7   Max Flow"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/#analysis","text":"","title":"Analysis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/#framing","text":"\"Gross flow\" with nonnegative $g(v,w)$ along directed edges for adjacent vertices $v,w$, we always consider $g(v,w)-g(w,v)$, we have \"Directed net flow\" that can have negative numbers along edges with both min and max , only considers $g(v,w)$ with skew symmetry","title":"Framing"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/#decomposition","text":"Given a flow, take edges out of source iteratively Decompose flow into complete paths and cycles","title":"Decomposition"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/#verify-if-a-flow-is-maximal","text":"Take residual graph where each edge has new bounds This graph has 0 flow\u2014take all points we can reach from source, this creates a cut with value 0","title":"Verify if a Flow is Maximal"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.7%20-%20Max-Flow/#algorithms","text":"Repeatedly look at residual, find augmenting path. Assuming integers with capacities $\\leq U$, we get at most $EU$ path sums.","title":"Algorithms"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.8%20-%20Max-Flow%20Algorithms/","text":"Try repeated augment for rationals - do better by least common denominator for reals - super sad\u2014can't even guarantee termination Greedy Paths Let's try finding the best augmenting path ( henceforth abbreviate AP ), by binary searching for the bottleneck edge and running DFS. This way we capture at least $\\frac{1}{m}$ of the remaining max-flow, i.e. we finish in $m^{2}\\log nU$. Scaling Max Flow Inspired by rounding real numbers to rationals, we round integers to $2^{i}$. Repeatedly roll in smaller and smaller order bits. Each time we double all the capacities and sometimes add 1. Then the previous min cut now becomes at most $m$, so we run Augmenting Path $m$ times and we get optimal. Each AP is $O(m)$ so total runtime is now $O(m^{2}\\log (nU))$. Weird but often good cuz $U$ can be small. Edmonds-Karp Instead of taking any AP, we always add the shortest one. [!important] Claim No vertex gets closer to $s$ or $t$ when we take residual of shortest AP. Suppose otherwise; of all $v$ that got closer to $s$ take the now-closest one. Let its predecessor on current path be $w$. Then $$ \\begin{align } d'(v)&=d'(w)+1,\\ d'(w)&\\geq d(w). \\end{align } $$ But if $w\\to v$ existed before, then $d(v)\\leq d(w)+1$ i.e. $d'(v)\\geq d(v)$, contradiction. Hence $w\\to v$ was created \"just now\" i.e. previous path went $v\\to w$ i.e. $d(w)=d(v)+1$. Then $d'(v)=d'(w)+1\\geq d(w)+1=d(v)+2$. Wow. Runtime analysis We say an edge is saturated when all of its capacity is taken by some path. The next flow that passes through this edge must be in the opposite direction. Key insight: for $u\\to v$ followed by $v\\to u$, we have $d(v)=d(u)+1$ and $d'(u)=d'(v)+1$. Since $d'(v)\\geq d(v)\\forall v,$ we get $d'(u)\\geq d(u)+2$ and similarly $d'(v)\\geq d(v)+2$. Then the $u\\to v$ edge can be saturated at most $n$ times. BFS is $O(m)$, at most $O(mn)$ APs so total runtime at most $O(m^{2}n).$","title":"6.5210.8   Max Flow Algorithms"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.8%20-%20Max-Flow%20Algorithms/#greedy-paths","text":"Let's try finding the best augmenting path ( henceforth abbreviate AP ), by binary searching for the bottleneck edge and running DFS. This way we capture at least $\\frac{1}{m}$ of the remaining max-flow, i.e. we finish in $m^{2}\\log nU$.","title":"Greedy Paths"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.8%20-%20Max-Flow%20Algorithms/#scaling-max-flow","text":"Inspired by rounding real numbers to rationals, we round integers to $2^{i}$. Repeatedly roll in smaller and smaller order bits. Each time we double all the capacities and sometimes add 1. Then the previous min cut now becomes at most $m$, so we run Augmenting Path $m$ times and we get optimal. Each AP is $O(m)$ so total runtime is now $O(m^{2}\\log (nU))$. Weird but often good cuz $U$ can be small.","title":"Scaling Max Flow"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.8%20-%20Max-Flow%20Algorithms/#edmonds-karp","text":"Instead of taking any AP, we always add the shortest one. [!important] Claim No vertex gets closer to $s$ or $t$ when we take residual of shortest AP. Suppose otherwise; of all $v$ that got closer to $s$ take the now-closest one. Let its predecessor on current path be $w$. Then $$ \\begin{align } d'(v)&=d'(w)+1,\\ d'(w)&\\geq d(w). \\end{align } $$ But if $w\\to v$ existed before, then $d(v)\\leq d(w)+1$ i.e. $d'(v)\\geq d(v)$, contradiction. Hence $w\\to v$ was created \"just now\" i.e. previous path went $v\\to w$ i.e. $d(w)=d(v)+1$. Then $d'(v)=d'(w)+1\\geq d(w)+1=d(v)+2$. Wow.","title":"Edmonds-Karp"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.8%20-%20Max-Flow%20Algorithms/#runtime-analysis","text":"We say an edge is saturated when all of its capacity is taken by some path. The next flow that passes through this edge must be in the opposite direction. Key insight: for $u\\to v$ followed by $v\\to u$, we have $d(v)=d(u)+1$ and $d'(u)=d'(v)+1$. Since $d'(v)\\geq d(v)\\forall v,$ we get $d'(u)\\geq d(u)+2$ and similarly $d'(v)\\geq d(v)+2$. Then the $u\\to v$ edge can be saturated at most $n$ times. BFS is $O(m)$, at most $O(mn)$ APs so total runtime at most $O(m^{2}n).$","title":"Runtime analysis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.9%20-%20Blocking%20Flows/","text":"Better than Edmonds-Karp, we can say more. Admissible Paths Do BFS to get distance of all nodes, arrange into \"layers\". Admissible edges are those pass \"forward\". The set of shortest paths $s\\to t$ is all paths composed only of admissible edges. We can try to \"batch\" our AP strategy by considering entire flows. Blocking Flows [!important] Blocking Flows Call a blocking flow one that saturates every admissible path. Then $d(s,t)$ increases after augmentation. The augmenting path creates new arcs, but they only point backwards. The only possible new paths include a backwards arc, i.e. cannot be same length as the one that was added. Finding BFs Act greedily. [!i] Greedy Blocking Flows - DFS from $s$ to $t$ along an admissible path and saturate , blocking the current path - Every time we dead-end, backtrack and prune - Repeat until stopped Try a unit-capacity graph . For capacity $c$ replace with $c$ parallel edges. Advance and prune together is $O(m)$ and every edge is one or the other, so total cost for runtime is $O(mn)$. In the general case, a block might only destroy one edge instead of all edges along the advance, so we need to do $O(mn)$ work to find a BF. Then we get $O(mn^{2}),$ already better than Edmonds-Karp. A sexier trick we could use is scaling . When we scale we just get unit capacity graph $\\log U$ times, so our runtime is $O(mn\\log U).$ Sleator-Tarjan Brilliancy We spent work augmenting edges that weren't saturated, i.e. advanced across edges multiple times. Instead, we want a DS satisfying - keep pieces of admissible paths - Link root of tree to node of another (advance) - Cut links by retreating, going back to root - Decrease capacity - Get min Use Link-Cut trees, all above ops are $O(\\log n)$. Then every edge is processed once so finding BF is $O(m\\log n)$ yielding $O(mn\\log n)$ runtime. Then there's continuous optimization with electric flow and grad. descent. Then there's Y. Liu's '22 \"Almost Linear\" paper. Crazy guy.","title":"6.5210.9   Blocking Flows"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.9%20-%20Blocking%20Flows/#admissible-paths","text":"Do BFS to get distance of all nodes, arrange into \"layers\". Admissible edges are those pass \"forward\". The set of shortest paths $s\\to t$ is all paths composed only of admissible edges. We can try to \"batch\" our AP strategy by considering entire flows.","title":"Admissible Paths"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.9%20-%20Blocking%20Flows/#blocking-flows","text":"[!important] Blocking Flows Call a blocking flow one that saturates every admissible path. Then $d(s,t)$ increases after augmentation. The augmenting path creates new arcs, but they only point backwards. The only possible new paths include a backwards arc, i.e. cannot be same length as the one that was added.","title":"Blocking Flows"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.9%20-%20Blocking%20Flows/#finding-bfs","text":"Act greedily. [!i] Greedy Blocking Flows - DFS from $s$ to $t$ along an admissible path and saturate , blocking the current path - Every time we dead-end, backtrack and prune - Repeat until stopped Try a unit-capacity graph . For capacity $c$ replace with $c$ parallel edges. Advance and prune together is $O(m)$ and every edge is one or the other, so total cost for runtime is $O(mn)$. In the general case, a block might only destroy one edge instead of all edges along the advance, so we need to do $O(mn)$ work to find a BF. Then we get $O(mn^{2}),$ already better than Edmonds-Karp. A sexier trick we could use is scaling . When we scale we just get unit capacity graph $\\log U$ times, so our runtime is $O(mn\\log U).$","title":"Finding BFs"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/6.5210.9%20-%20Blocking%20Flows/#sleator-tarjan-brilliancy","text":"We spent work augmenting edges that weren't saturated, i.e. advanced across edges multiple times. Instead, we want a DS satisfying - keep pieces of admissible paths - Link root of tree to node of another (advance) - Cut links by retreating, going back to root - Decrease capacity - Get min Use Link-Cut trees, all above ops are $O(\\log n)$. Then every edge is processed once so finding BF is $O(m\\log n)$ yielding $O(mn\\log n)$ runtime. Then there's continuous optimization with electric flow and grad. descent. Then there's Y. Liu's '22 \"Almost Linear\" paper. Crazy guy.","title":"Sleator-Tarjan Brilliancy"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.5210/grokking%206.5210/","text":"Major results and ideas from MIT 6.5210. For DS, our toy problem is MST or shortest paths. Means some operations happen much more often than others; encouraged to get fast insert , merge etc using laziness techniques. Name Claim Idea Fib Heaps Priority Queue : $O(1)$ find-min, insert, delete-key, merge; $O(\\log n)$ delete-min Binary heap has these operations using go-down, bubble-up. Balance with a collection of heap-ordered balanced trees. Delete key: mark parent when child cut, cut parent when two children are cut. Can yield recursive cuts but has good amortized performance. Persistence Data Structure Trick : be able to read from history Naively: keep a \"fat\" timestamped array at each node in the tree. Smarter: copy the path from node to root each time, keep all paths in a single timestamped array. Smartest: copy node only after it is updated once, keep a \"fat\" array of length at most 2 at each node. Splay Tree BBST but with better amortized costs Claim: Splaying , i.e. rot node to root on each access, makes good amortized performance. More commonly accessed nodes end up at the top. Tree can become unbalanced, but only after a lot of work. Recursive DS Buckets, vEB Queues, Hashing : use recursive tables for log lookup To do a PQ for integers: assuming access to super large tables, we can log by the first half, and lookup into a smaller PQ by the second. Assuming not super large access, use recursive lazy buckets with height $\\frac{{\\log n}}{\\log \\log n}$ that \"trickle down\" when new buckets are needed. Max Flows Algorithms, Equivalent Problems : APs, residual graphs, min cut, blocking flows In integer case, suffices to find nonstop APs. Repeatedly analyze residual graph. Could create many \"reverse\" APs, so take shortest AP: $O(m^{2}n)$. Each edge saturates at most $n$ times; distances to $s$ or $t$ cannot decrease. Alternatively, take \"blocking flows\" that cover all current SAPs, of which we will need at most $n$: $O(mn^{2})$. Well-known that max flow == min cut. Min Cost Max Flow Min Cost Circ, Re-framings : using max flow for MCMF After taking max flow, find cycles in the graph with negative cost. After solving this \"min cost circulation\" we add back to get MCMF. Also can use supply/demand, vertex capacities, etc. all kinds of tricks to adapt various problems to MCMF. LPs","title":"Grokking 6.5210"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.10%20-%20Memory/","text":"We can \"convolve across time,\" or we can just have some \"time-dependent hidden variable.\" We take some \"update\" function where new hidden variable takes in old hidden and current input. For gradients of shared parameters, the \"real\" gradient is computed by summing gradients for each of the original values. However, if we want memory that lasts for a long time, we need to look at really long gradients, but these can be noisy. There's this old hand-crafted LSTM model, where the cell state - first multiplied by sigmoid of some linear map (forget/remember) - grab some new information (tanh) and consider what to forget/remember (sigmoid) and then add to cell state - finally, tanh(cell state) and sigmoid again Because sigmoid is used as multiplicative \"filter\" function for memory, default is 1, i.e. remembering. Long Context This is the better, newer, more modern form of memory. Sparsifying the attention matrix to make memory better. Videos Types of memory We have parameters that are learned (slow memory), statistics of the dataset. We also have activations that are evaluated (fast memory), statistics of the data point. There are weird intuitional things going on here, e.g. hypernets, or whatever. Summary Memory is solved with - CNNs for sequences - RNNs for recurrent memory - LSTMs (old technology) for remembering stuff - Sequence models (attention) for selective analysis of sequences","title":"6.7960.10   Memory"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.10%20-%20Memory/#long-context","text":"This is the better, newer, more modern form of memory. Sparsifying the attention matrix to make memory better.","title":"Long Context"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.10%20-%20Memory/#videos","text":"","title":"Videos"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.10%20-%20Memory/#types-of-memory","text":"We have parameters that are learned (slow memory), statistics of the dataset. We also have activations that are evaluated (fast memory), statistics of the data point. There are weird intuitional things going on here, e.g. hypernets, or whatever.","title":"Types of memory"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.10%20-%20Memory/#summary","text":"Memory is solved with - CNNs for sequences - RNNs for recurrent memory - LSTMs (old technology) for remembering stuff - Sequence models (attention) for selective analysis of sequences","title":"Summary"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.11%20-%20Reconstructive%20Representations/","text":"Autoencoder/decoder: bottleneck with reconstruction loss Prediction with imputation (infilling) or filling in patched pixels Clustering: typical stats tool, $K$-means clustering; partition into $K$ clusters, minimize distance to mean of cluster. Instead of bottleneck by hypersphere, bottleneck by integers","title":"6.7960.11   Reconstructive Representations"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.12%20-%20Similarity%20Representations/","text":"Last time, we tried to learn representations by \"forcing\" them through encoder/decoder and constraining output to be close to input. New framing: contrastive learning , i.e. learn some distance metric. - Map data points to element on hypersphere $S^{d-1}$ - Pick an anchor point and consider some set of positive (similar) examples and negative (dissimilar) examples For supervised learning, we can have classes and learn a softmax classifier, or assign similarity scores by RL. Alternatively, for self-supervised contrastive image representation learning: - augment data samples for positive - everything else is negative and this generates representations that put similar things together! Can also do co-occurence; e.g. get RGB images and depth images to return similar representations Mathematical analysis A couple sample loss functions: - Triplet loss: taking the farthest positive and closest negative example, grad descent on these distances - InfoNCE: taking one positive and all negatives, treat as softmax classifier with logits corresponding to distance to examples We care about alignment (putting similar stuff together), separation (putting farther stuff apart), uniformity (having representations spread uniformly through $S^{d-1}$). The loss function does (in theory) regularize for this stuff. Contrastive learning is also more \"descriptive\" than classifier learning because it's more expressive (has distance)","title":"6.7960.12   Similarity Representations"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.12%20-%20Similarity%20Representations/#mathematical-analysis","text":"A couple sample loss functions: - Triplet loss: taking the farthest positive and closest negative example, grad descent on these distances - InfoNCE: taking one positive and all negatives, treat as softmax classifier with logits corresponding to distance to examples We care about alignment (putting similar stuff together), separation (putting farther stuff apart), uniformity (having representations spread uniformly through $S^{d-1}$). The loss function does (in theory) regularize for this stuff. Contrastive learning is also more \"descriptive\" than classifier learning because it's more expressive (has distance)","title":"Mathematical analysis"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.4%20-%20CNNs/","text":"Better than MLPs, specifically for images. Big matrix applied around a point, note that coordinates are not reversed unlike actual coordinates. five ways of seeing translation equivariance patch processing image (signal, wtv.) local filter (akin to e.g. local inverse fourier) sparse mlp matrix process variable-size tensors conv tricks Can pool in the traditional way (pool 3x3 into single pixel) or also across channels . E.g. if we have a linear boundary detection filter at various orientations, but then pool over all these orientations. Note that pooling usually doesn't downsample , purpose is to increase stability. Can do max pool, mean pool, etc. We do strides , which reduces the overlap between patches. We can do dilation , which \"spreads out\" the convolution. We can get bigger patches for the same price. So we can have this complicated formula for 2D convnets in terms of stride $s$, in channels $c_{i}$, out channels $c_{o}$, pooling patch side length $p$, kernel side length $k$, and dilation $d$. encoder-decoder model Increase channel count as conv-layers are applied, compress into a representation. Then upsample, decreasing channel count and increasing resolution. Precise spatial information is lost, so resnet the \"reflected\" channel information to the symmetric \"other side\" of the network. This way information sees more information at the same \"level\". applications Can convolve across: - time - direction (for 3D reps) As with any other model, if we want to not be location equivariant we can pass in coordinates to the layer.","title":"6.7960.4   CNNs"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.4%20-%20CNNs/#five-ways-of-seeing","text":"translation equivariance patch processing image (signal, wtv.) local filter (akin to e.g. local inverse fourier) sparse mlp matrix process variable-size tensors","title":"five ways of seeing"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.4%20-%20CNNs/#conv-tricks","text":"Can pool in the traditional way (pool 3x3 into single pixel) or also across channels . E.g. if we have a linear boundary detection filter at various orientations, but then pool over all these orientations. Note that pooling usually doesn't downsample , purpose is to increase stability. Can do max pool, mean pool, etc. We do strides , which reduces the overlap between patches. We can do dilation , which \"spreads out\" the convolution. We can get bigger patches for the same price. So we can have this complicated formula for 2D convnets in terms of stride $s$, in channels $c_{i}$, out channels $c_{o}$, pooling patch side length $p$, kernel side length $k$, and dilation $d$.","title":"conv tricks"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.4%20-%20CNNs/#encoder-decoder-model","text":"Increase channel count as conv-layers are applied, compress into a representation. Then upsample, decreasing channel count and increasing resolution. Precise spatial information is lost, so resnet the \"reflected\" channel information to the symmetric \"other side\" of the network. This way information sees more information at the same \"level\".","title":"encoder-decoder model"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.4%20-%20CNNs/#applications","text":"Can convolve across: - time - direction (for 3D reps) As with any other model, if we want to not be location equivariant we can pass in coordinates to the layer.","title":"applications"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.5%20-%20GNNs/","text":"We want something that is graph invariant, i.e. given adjacency matrix, it should be invariant to node permutations. Similar to CNNs, each node \"listens to\" i.e. interactions with its neighbors. Algo for node $v$ for certain stage: Aggregate MLP $M_{1}(h_{v},h_{u})$ for every neighbor $u$ of $v$ Sum all $M_{1}$ MLPs and run through $M_{2}$ Maybe normalize wtv. Update MLP $M_{3}(h_{v},m_{v})$ to get the new encoding All aggregates use the same MLP for each edge, all updates use same MLP for each pair (encoding, intermediate aggregate). Update just \"locks in\" all the relevant changes.","title":"6.7960.5   GNNs"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.5%20-%20GNNs/#aggregate","text":"MLP $M_{1}(h_{v},h_{u})$ for every neighbor $u$ of $v$ Sum all $M_{1}$ MLPs and run through $M_{2}$ Maybe normalize wtv.","title":"Aggregate"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.5%20-%20GNNs/#update","text":"MLP $M_{3}(h_{v},m_{v})$ to get the new encoding All aggregates use the same MLP for each edge, all updates use same MLP for each pair (encoding, intermediate aggregate). Update just \"locks in\" all the relevant changes.","title":"Update"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/","text":"[!warning] Issue We want to get some sparse, global index. E.g.: - MLP that doesn't look at everything - CNN that looks at stuff across the image How can we achieve this? Variable attention . Idea #1: Tokens First, we redefine a token as a bundle of vectorized data. [!important] Token Intuition A vector of scalars that describe information about one single object (e.g. blob of audio or patch of pixels). At the input layer, we cut our data (images, text, sound) into pieces and map each piece into a token using a \"domain expert\". But then tokens can be combined in this transformer system. Structure Similar to a fully connected GNN, we want all of our tokens to talk to each other, so: - first AGGREGATE (linear combination of tokens using attention ) - then NORM (layernorm or wtv.. why is it the right thing to do ??) - finally UPDATE (some learned MLP or other \"token-wise nonlinearity\" applied to each token ) Idea #2: Attention We don't know which tokens we actually care about . We use a query-key-value system to figure out which tokens we care about. [!important] Query-Key-Value - One single Query vector corresponds to \"what's the question\"? Dotted with the - Key vector generated from each token, determines all the similarity values, encoded in a vector $s$. The $s$ values are used to take a weighted average of - Value vectors, which after summed give us the final output token. Self-Attention One particular strategy is to have our query come from the input tokens. [!important] Self-Attention - Learn $W_{q},W_{k},W_{v}$ which returns query, key, and value for each token - Get all queries, all keys, all values by just three matmuls (since all tokens are stacked row-wise in matrix $T_{in}$) - Attention matrix $A$ obtained by - Attention (pairwise dots between keys and queries) - Normalization (division then softmax) Now have covered three linear layers: MLP, CNN, Self Attn. Extra Tricks Skip connections, multi-head self atten (parallelizing). Hardware loves matmuls. Autoregressive token prediction/log likelihood loss for prediction models. Idea #3: Positional Encoding Various ways to do, e.g. learned or hardcoded, but we can do Fourier basis: $$\\sin(x), \\sin\\left( \\frac{x}{B} \\right),\\sin\\left( \\frac{x}{B^{2}} \\right),\\dots$$ Domain expertise is crucial here: human engineers apply inductive bias here.","title":"6.7960.8   Transformers"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#idea-1-tokens","text":"First, we redefine a token as a bundle of vectorized data. [!important] Token Intuition A vector of scalars that describe information about one single object (e.g. blob of audio or patch of pixels). At the input layer, we cut our data (images, text, sound) into pieces and map each piece into a token using a \"domain expert\". But then tokens can be combined in this transformer system.","title":"Idea #1: Tokens"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#structure","text":"Similar to a fully connected GNN, we want all of our tokens to talk to each other, so: - first AGGREGATE (linear combination of tokens using attention ) - then NORM (layernorm or wtv.. why is it the right thing to do ??) - finally UPDATE (some learned MLP or other \"token-wise nonlinearity\" applied to each token )","title":"Structure"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#idea-2-attention","text":"We don't know which tokens we actually care about . We use a query-key-value system to figure out which tokens we care about. [!important] Query-Key-Value - One single Query vector corresponds to \"what's the question\"? Dotted with the - Key vector generated from each token, determines all the similarity values, encoded in a vector $s$. The $s$ values are used to take a weighted average of - Value vectors, which after summed give us the final output token.","title":"Idea #2: Attention"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#self-attention","text":"One particular strategy is to have our query come from the input tokens. [!important] Self-Attention - Learn $W_{q},W_{k},W_{v}$ which returns query, key, and value for each token - Get all queries, all keys, all values by just three matmuls (since all tokens are stacked row-wise in matrix $T_{in}$) - Attention matrix $A$ obtained by - Attention (pairwise dots between keys and queries) - Normalization (division then softmax) Now have covered three linear layers: MLP, CNN, Self Attn.","title":"Self-Attention"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#extra-tricks","text":"Skip connections, multi-head self atten (parallelizing). Hardware loves matmuls. Autoregressive token prediction/log likelihood loss for prediction models.","title":"Extra Tricks"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.8%20-%20Transformers/#idea-3-positional-encoding","text":"Various ways to do, e.g. learned or hardcoded, but we can do Fourier basis: $$\\sin(x), \\sin\\left( \\frac{x}{B} \\right),\\sin\\left( \\frac{x}{B^{2}} \\right),\\dots$$ Domain expertise is crucial here: human engineers apply inductive bias here.","title":"Idea #3: Positional Encoding"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.9%20-%20DL%20Hacking/","text":"Data Make sure you preprocess correctly Test loading thoroughly (einops) Much of the work is reshaping tensors\u2014make sure you do it correctly Augmentation Tradeoff axes: Dimension Generalization Cost Levers: Augmentation Associated data Models Don't work too hard Simple models can have unseen advantages (clearer advantages) Translate to other solved problems Other tricks Know your constants: ln(0.5)=-0.69, ln(0.1)=-2.3 Adding tricks is like cooking. Takes intuition, time, practice, etc. to figure out what to do","title":"6.7960.9   DL Hacking"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.9%20-%20DL%20Hacking/#data","text":"Make sure you preprocess correctly Test loading thoroughly (einops) Much of the work is reshaping tensors\u2014make sure you do it correctly Augmentation Tradeoff axes: Dimension Generalization Cost Levers: Augmentation Associated data","title":"Data"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.9%20-%20DL%20Hacking/#models","text":"Don't work too hard Simple models can have unseen advantages (clearer advantages) Translate to other solved problems","title":"Models"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/6.7960.9%20-%20DL%20Hacking/#other-tricks","text":"Know your constants: ln(0.5)=-0.69, ln(0.1)=-2.3 Adding tricks is like cooking. Takes intuition, time, practice, etc. to figure out what to do","title":"Other tricks"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/F24/6.7960/Untitled/","text":"[[Pytorch Manual]]","title":"Untitled"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/MAS.885/","text":"learning to apply engineering principles to biology - hierarchical composability - standardized \"parts\" - plug-and-play for manufacturing, etc. all kinds of stuff","title":"MAS.885"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/","text":"Problem 1.1 (a) (i) The probability that $s=s_{0}$ is $$ p_{s}(s_{0})=\\sum_{x_{0}=0}^{s_{0}}p_{x}(x_{0})p_{y}(s_{0}-x_{0})=\\sum_{x_{0}=0}^{s_{0}}p^{2}(1-p)^{s_{0}}=(s_{0}+1)p^{2}(1-p)^{s_{0}}. $$ The graph of $p(s)$ limits to 0, starts at $p^{2},$ but initially doesn't fall off as quickly as $p(x)$ or $p(y)$. (ii) As computed above, any pair of $x,y$ summing to a given $s_{0}$ is equally probably (occurring with likelihood $p^{2}(1-p)^{s_{0}}$) so it's uniform over $0,1,\\dots,s_{0}.$ (iii) The average of uniform over $0,1,\\dots,s_{0}$ is $\\frac{s}{2}.$ (iv) This is exactly equal to half of the random variable $s$ so this PMF hits all $\\frac{n}{2}$ for nonnegative integers $n$ with probability $(n+1)p^{2}(1-p)^{n}.$ Problem 1.2 (a) Integrate over all possibilities for $x,y$. Namely $$ \\begin{align } \\iint_{x^{2}+y^{2}\\leq \\sigma^{2}} \\frac{1}{2\\pi\\sigma^{2}} e^{-(x^{2}+y^{2})/2\\sigma^{2}}\\,dxdy &= \\iint_{r\\leq\\sigma} \\frac{1}{2\\pi\\sigma^{2}}e^{-r^{2}/2\\sigma^{2}}r\\,dr d\\theta\\ &= \\frac{1}{2\\pi\\sigma^{2}}\\cdot 2\\pi \\cdot \\int_{0}^{\\sigma}re^{-r^{2}/2\\sigma^{2}}\\,dr\\ &=\\frac{1}{\\sigma^{2}} \\cdot \\left. -\\sigma^{2}e^{-r^{2}/2\\sigma^{2}} \\right|_{0}^{\\sigma }\\ &= 1-e^{-1/2}. \\end{align } $$ (b) The two likelihoods\u2014that $x\\geq_{}0$ and $y\\geq 0$\u2014are independent, so we get $\\frac{1}{4}.$ (c) Since the two likelihoods are independent (by rotational symmetry) we get the same answer as that from part (a), i.e. $1-e^{-1/2}.$ (d) We take the differential $dxdy=r dr d\\theta.$ Since $p_{r,\\Theta}(r,\\Theta) dr d\\theta=p(x,y)\\,dxdy$ we get $p_{r,\\Theta}=p(x,y)r.$ Then $p(x,y)=\\frac{1}{2\\pi\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}$ so $p_{r,\\Theta}=\\frac{r}{2\\pi\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}.$ By symmetry, since the joint is independent of $\\Theta,$ we get $p_{\\Theta}=\\frac{1}{2\\pi}.$ Finally, the marginal in $r$ is the joint multiplied by $2\\pi$ i.e. $p_{r}=\\frac{r}{\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}.$ Problem 1.3 (a) The decision rule is based on the likelihood ratio. Likelihood for $H_{0}$ is $\\frac{{\\mathbb 1_{[-1,1]}(y)}}{2}\\cdot \\frac{1}{\\sqrt{ \\frac{\\pi}{2} }}e^{-2z^{2}}$ and for $H_{1}$ is $\\frac{{\\mathbb 1_{[-2,2]}(y)}}{4}\\cdot \\frac{1}{\\sqrt{ \\frac{\\pi}{2} }}e^{-2(z-1)^{2}}.$ The ratio is then $$ \\frac{p(H_{0}|y,z)}{p(H_{1}|y,z)}= \\frac{{2\\mathbb 1_{[-1,1]}(y)}}{\\mathbb 1_{{[-2,2]}}(y)} \\cdot e^{-2(2z-1)}. $$ Then: 1. $y$ is always in $[-2,2].$ If $y\\not\\in [-1,1]$ then we must have $H_{1}$. 2. Otherwise the likelihood ratio is $2e^{2-4z},$ so we get $H_{0}$ if $z\\leq \\frac{{1+\\ln_{}2}}{2}$ and $H_{1}$ otherwise. (b) If the truth is $H_{0}$ then probability of error is equal to $z\\geq \\frac{{1+\\ln2}}{2}$ i.e. $Q(1+\\ln_{2})$ and if the truth is $H_{1}$ then probability of error is equal to $y \\in [-1,1]$ and $z\\leq \\frac{{1+\\ln2}}{2}$ i.e. $\\frac{1}{2} Q(1-\\ln2),$ hence the total probability of error is $$ \\frac{1}{2}Q(1+\\ln 2)+\\frac{1}{4}Q(1-\\ln2). $$ Problem 1.4 (a) Since they're generated independently, the coin flip is \"arbitrary\" and opening the second envelope is equivalent to sampling from the PDF again. Then you switch envelopes if and only if the amount of money in the first envelope is less than the median of the PDF. (b) Sample a random number $z$ from the normal Gaussian. Then switch envelopes if the envelope you open has money less than $z,$ and keep otherwise. - If both envelopes have less than $z$ before you flip the coin, then this strategy doesn't change the likelihood of winning. - Similarly if both have more than $z$ this strategy doesn't change the likelihood of winning. - However if one envelope has more than $z$ and the other has less than $z$ then this strategy guarantees you win. Since there is a nonzero probability of drawing some $z$ in between the values contained in the two envelopes, this strategy yields a strictly better than half chance of winning.","title":"6.7800 Pset 1"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#problem-11","text":"","title":"Problem 1.1"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#a","text":"","title":"(a)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#i","text":"The probability that $s=s_{0}$ is $$ p_{s}(s_{0})=\\sum_{x_{0}=0}^{s_{0}}p_{x}(x_{0})p_{y}(s_{0}-x_{0})=\\sum_{x_{0}=0}^{s_{0}}p^{2}(1-p)^{s_{0}}=(s_{0}+1)p^{2}(1-p)^{s_{0}}. $$ The graph of $p(s)$ limits to 0, starts at $p^{2},$ but initially doesn't fall off as quickly as $p(x)$ or $p(y)$.","title":"(i)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#ii","text":"As computed above, any pair of $x,y$ summing to a given $s_{0}$ is equally probably (occurring with likelihood $p^{2}(1-p)^{s_{0}}$) so it's uniform over $0,1,\\dots,s_{0}.$","title":"(ii)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#iii","text":"The average of uniform over $0,1,\\dots,s_{0}$ is $\\frac{s}{2}.$","title":"(iii)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#iv","text":"This is exactly equal to half of the random variable $s$ so this PMF hits all $\\frac{n}{2}$ for nonnegative integers $n$ with probability $(n+1)p^{2}(1-p)^{n}.$","title":"(iv)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#problem-12","text":"","title":"Problem 1.2"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#a_1","text":"Integrate over all possibilities for $x,y$. Namely $$ \\begin{align } \\iint_{x^{2}+y^{2}\\leq \\sigma^{2}} \\frac{1}{2\\pi\\sigma^{2}} e^{-(x^{2}+y^{2})/2\\sigma^{2}}\\,dxdy &= \\iint_{r\\leq\\sigma} \\frac{1}{2\\pi\\sigma^{2}}e^{-r^{2}/2\\sigma^{2}}r\\,dr d\\theta\\ &= \\frac{1}{2\\pi\\sigma^{2}}\\cdot 2\\pi \\cdot \\int_{0}^{\\sigma}re^{-r^{2}/2\\sigma^{2}}\\,dr\\ &=\\frac{1}{\\sigma^{2}} \\cdot \\left. -\\sigma^{2}e^{-r^{2}/2\\sigma^{2}} \\right|_{0}^{\\sigma }\\ &= 1-e^{-1/2}. \\end{align } $$","title":"(a)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#b","text":"The two likelihoods\u2014that $x\\geq_{}0$ and $y\\geq 0$\u2014are independent, so we get $\\frac{1}{4}.$","title":"(b)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#c","text":"Since the two likelihoods are independent (by rotational symmetry) we get the same answer as that from part (a), i.e. $1-e^{-1/2}.$","title":"(c)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#d","text":"We take the differential $dxdy=r dr d\\theta.$ Since $p_{r,\\Theta}(r,\\Theta) dr d\\theta=p(x,y)\\,dxdy$ we get $p_{r,\\Theta}=p(x,y)r.$ Then $p(x,y)=\\frac{1}{2\\pi\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}$ so $p_{r,\\Theta}=\\frac{r}{2\\pi\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}.$ By symmetry, since the joint is independent of $\\Theta,$ we get $p_{\\Theta}=\\frac{1}{2\\pi}.$ Finally, the marginal in $r$ is the joint multiplied by $2\\pi$ i.e. $p_{r}=\\frac{r}{\\sigma^{2}} e^{-r^{2}/2\\sigma^{2}}.$","title":"(d)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#problem-13","text":"","title":"Problem 1.3"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#a_2","text":"The decision rule is based on the likelihood ratio. Likelihood for $H_{0}$ is $\\frac{{\\mathbb 1_{[-1,1]}(y)}}{2}\\cdot \\frac{1}{\\sqrt{ \\frac{\\pi}{2} }}e^{-2z^{2}}$ and for $H_{1}$ is $\\frac{{\\mathbb 1_{[-2,2]}(y)}}{4}\\cdot \\frac{1}{\\sqrt{ \\frac{\\pi}{2} }}e^{-2(z-1)^{2}}.$ The ratio is then $$ \\frac{p(H_{0}|y,z)}{p(H_{1}|y,z)}= \\frac{{2\\mathbb 1_{[-1,1]}(y)}}{\\mathbb 1_{{[-2,2]}}(y)} \\cdot e^{-2(2z-1)}. $$ Then: 1. $y$ is always in $[-2,2].$ If $y\\not\\in [-1,1]$ then we must have $H_{1}$. 2. Otherwise the likelihood ratio is $2e^{2-4z},$ so we get $H_{0}$ if $z\\leq \\frac{{1+\\ln_{}2}}{2}$ and $H_{1}$ otherwise.","title":"(a)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#b_1","text":"If the truth is $H_{0}$ then probability of error is equal to $z\\geq \\frac{{1+\\ln2}}{2}$ i.e. $Q(1+\\ln_{2})$ and if the truth is $H_{1}$ then probability of error is equal to $y \\in [-1,1]$ and $z\\leq \\frac{{1+\\ln2}}{2}$ i.e. $\\frac{1}{2} Q(1-\\ln2),$ hence the total probability of error is $$ \\frac{1}{2}Q(1+\\ln 2)+\\frac{1}{4}Q(1-\\ln2). $$","title":"(b)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#problem-14","text":"","title":"Problem 1.4"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#a_3","text":"Since they're generated independently, the coin flip is \"arbitrary\" and opening the second envelope is equivalent to sampling from the PDF again. Then you switch envelopes if and only if the amount of money in the first envelope is less than the median of the PDF.","title":"(a)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/S25/6.7800/6.7800%20Pset%201/#b_2","text":"Sample a random number $z$ from the normal Gaussian. Then switch envelopes if the envelope you open has money less than $z,$ and keep otherwise. - If both envelopes have less than $z$ before you flip the coin, then this strategy doesn't change the likelihood of winning. - Similarly if both have more than $z$ this strategy doesn't change the likelihood of winning. - However if one envelope has more than $z$ and the other has less than $z$ then this strategy guarantees you win. Since there is a nonzero probability of drawing some $z$ in between the values contained in the two envelopes, this strategy yields a strictly better than half chance of winning.","title":"(b)"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/W25/Matrix%20Calc/","text":"Crucially high-dimensional derivative is defined as as linear form $$ f'(x)[dx] = f(x+dx)-f(x) $$ where $f'(x)$ is a linear form on $[dx].$ In vector spaces equipped with inner products the linear form can be represented as an inner product with an element of the vector space, and said element becomes the gradient. Key thing to know is good ol product rule, which is derived only from distributive property and not commutativity hence we can use in matrix calc. Abstractions like Kronecker product and $vec$ are nice but at the end of it all is still Einstein summation lol. Kronecker product $A\\otimes B$ is where each element $a_{ij}$ is replaced by $a_{ij}B$ so that output is $2\\times 2$ mat with product'd dims. Then $vec(M)$ flattens a matrix into a vector columnwise. Then for three matrices of the same dim, note that $$ (A \\otimes B) \\cdot vec(C^{\\dagger}) $$ can be partitioned by rows of $A$. (Note that $vec C^{\\dagger}$ is just flattening rowwise to match Kronecker rows of $B$.) For the first row of $A$ (hence first chunk of output) we get $$ A_{1j}B_{kl}C_{jl} $$ for element number $k$. Then rearranging yields $$ vec(M)=(A \\otimes B) \\cdot vec(C^{\\dagger})\\implies M_{ik}=A_{kj}B_{il}C_{jl}=B_{il}C^{\\dagger} {lj}A^{\\dagger} {jk}, $$ hence $$ (A\\otimes B)\\cdot vec(C)=vec(BCA^{\\dagger}). $$ Can also do finite difference stuff. Sure. Can also do standard exponent tricks, e.g. $d(AA^{-1})=(dA)A^{-1}+A(dA^{-1})=0$ hence $dA^{-1}=-A^{-1}dAA^{-1}.$ Also remember that $\\mathrm{Tr}(AB)=\\mathrm{Tr}(BA)$ (just do sum of product elementwise of the \"square intersection\"). In the vector space of matrices with fixed dim, standard Frobenius inner product is $\\left< vec(A),vec(B) \\right> =Trace(A^{\\dagger}B)$ and hence Frobenius norm is $\\sqrt{ \\mathrm{Tr}(A^{\\dagger}A) }$. Also recall that second derivative of $\\mathbb{R}^{n}\\to \\mathbb{R}$ is called Hessian, gradient of $\\mathbb{R}^{n}\\to \\mathbb{R}^{m}$ is Jacobian with shape $m\\times n.$ Autograd It's really cool. When using with libraries, could get a big boost from writing custom autograd for those parts, e.g. especially with Newton solvers (just implicit differentiate instead). Goal is to get gradients of an output in terms of an input. For long chains represented as computational DAGs there two ways\u2014forward-mode diff and backward-mode diff. Forward means start from input. Cost scales with number of inputs (for every elem compute change at elem for each input). Backward means start from output, cost scales with outputs (for every elem compute change output for that elem). Still chain rule multiplicative, always making gradient longer with mul (not shorter with division). In ML output is almost always 1 so backward is the best\u2014gradient attached to each param has size 1. Note that Hessian matrix is really annoying for $f: \\mathbb{R}^{n}\\to \\mathbb{R}$ since $f'$ is $\\mathbb{R}^{n}\\to \\mathbb{R}^{n}$ and so backward/forward on the second is the same. Hence \"forward-over-backward\". Use forward or backward depending on input/output shape. Special Matrix Ops Don't forget determinant: defined recursively using either - permutation parity - recursively using \"minors\" (drop out row/column), sum using \"cofactors\" (use $-1^{(i+j)}$, sum along any row/column) Then $$ \\nabla \\det A=\\det(A)A^{-\\dagger}=\\text{adj}(A^{\\dagger})=\\text{cofactor}(A) $$ where $\\text{cofactor}$ is the transpose of matrix of elementwise cofactors. Note that $A\\cdot\\text{cofactor}(A)=\\det(A)I_{n}$ (self-evident). Proof 1. Note that $\\det(I+dA)-\\det(I)=\\mathrm{Tr}(dA),$ so $$ \\det(A+dA)=\\det(A)(\\det(I+A^{-1}dA))=\\det(A)+\\det(A)\\mathrm{Tr}(A^{-1}dA) $$ and hence $$ d \\det(A)=\\left< \\det(A) A^{-\\dagger},dA\\right>. $$ Proof 2. $\\det(A)=\\sum_{j}A_{ij}C_{ij}$ for any $i$ so $d \\det(A)=C_{ij}$ (lmao). Functions $\\mathbb{R}^{n}\\to \\mathbb{R}$ as Banach Spaces Couple ideas: calculus of variations, random functions, second derivatives. As always, the key idea is that a derivative is a different type; its a linear form, as oppposed to \"another vector space element\". Calculus of variations: we can have functions $u: (\\mathbb{R}\\to \\mathbb{R})\\to \\mathbb{R}$ that map functions $f$ to reals. Generally we can take $\\int G(f,f',\\dots,t)\\,dx$. Then we want $\\nabla u[df]=u(f+df)-u(f).$ Then we also care about $df', df'',\\dots$ the derivatives of $f'$ and we can feed them directly into $G$, i.e. $$ \\nabla u=\\int \\nabla G[df,df',df'',\\dots]\\,dx $$ Note that here, $df',df'',$ etc. are no longer \"independent\" in the usual sense. It's just that the $x$-wise term $\\Delta G$ needs to know $\\Delta (f')$ and that's expressed in $df'$. Euler-Lagrange equations: if we have some \"potential\" expressed as $$ f(u)=\\int F(u,u',x)\\,dx $$ with fixed endpoints, then at equilibrium for all $du$ we get $$ 0=df=\\int \\left( \\frac{dF}{du}[du]+\\frac{dF}{du'}[du']\\right)\\,dx=\\left. \\frac{dF}{du'} du \\right| + \\int \\left( \\frac{dF}{du}- \\left( \\frac{dF}{du'}\\right)' \\right)du\\,dx $$ hence $$ \\int \\frac{dF}{du}=\\int \\left( \\frac{dF}{du'} \\right) '. $$ There's random functions, i.e. we feed some input and get a distribution back (that we sample from). Maybe we want to measure delta of expectation or smth, using Monte Carlo diffs. But, it can be very noisy if our two samples are uncorrelated. Hence correlate them with some confounding variable that preserved marginal but makes the joint difference really small, e.g. use CDF inverse. Then there's bilinear maps. Take a function $f:\\mathbb{R}^{n}\\to \\mathbb{R}.$ Second derivative could be represented as \"differentiate first in $x_{i}$ then $x_{j}$\" i.e. just Hessian matrix all over again. However this doesn't work all that well. Consider taking second derivative of $\\det(A).$ Then linear form is $\\det(A)A^{-1}$ as derived earlier (either linear expansion around 0 or cofactor expansion). The second differential we denote $dA'$, so $$ \\begin{align } d^{2}\\det(A) &=d(\\left< \\det(A)A^{-\\dagger}, dA \\right> )[dA']\\ &=d(\\det(A)\\mathrm{Tr}(A^{-1}dA))[dA']\\ &=d(\\det(A))[dA']\\cdot\\mathrm{Tr}(A^{-1}dA)+\\det(A)\\cdot d(\\mathrm{Tr}(A^{-1}dA))[dA']\\ &=\\det(A)\\mathrm{Tr}(A^{-1}dA')\\mathrm{Tr}(A^{-1}dA) + \\det(A)\\mathrm{Tr}(d(A^{-1})[dA']dA)\\ &=\\det(A)\\left(\\mathrm{Tr}(A^{-1}dA')\\mathrm{Tr}(A^{-1}dA) - \\mathrm{Tr}(A^{-1}dA'A^{-1}dA) \\right). \\end{align } $$ (I did that correctly first try wow) This is how it's done. Now $d^{2}\\det(A)$ becomes a generalized bilinear form on $dA$ and $dA'$ as opposed to the dual of some linear form. We can use these bilinear forms in Taylor expansions, i.e. $f(x)+f'(x)[\\Delta x] + \\frac{1}{2}f''(x)[\\Delta x,\\Delta x]$. Can also use this for \"perfect gradient descent\" i.e. Newton's method on the gradient; given gradient $g$ and Hessian $H$ just set $\\Delta x=H^{-1}g.$","title":"Matrix Calc"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/W25/Matrix%20Calc/#autograd","text":"It's really cool. When using with libraries, could get a big boost from writing custom autograd for those parts, e.g. especially with Newton solvers (just implicit differentiate instead). Goal is to get gradients of an output in terms of an input. For long chains represented as computational DAGs there two ways\u2014forward-mode diff and backward-mode diff. Forward means start from input. Cost scales with number of inputs (for every elem compute change at elem for each input). Backward means start from output, cost scales with outputs (for every elem compute change output for that elem). Still chain rule multiplicative, always making gradient longer with mul (not shorter with division). In ML output is almost always 1 so backward is the best\u2014gradient attached to each param has size 1. Note that Hessian matrix is really annoying for $f: \\mathbb{R}^{n}\\to \\mathbb{R}$ since $f'$ is $\\mathbb{R}^{n}\\to \\mathbb{R}^{n}$ and so backward/forward on the second is the same. Hence \"forward-over-backward\". Use forward or backward depending on input/output shape.","title":"Autograd"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/W25/Matrix%20Calc/#special-matrix-ops","text":"Don't forget determinant: defined recursively using either - permutation parity - recursively using \"minors\" (drop out row/column), sum using \"cofactors\" (use $-1^{(i+j)}$, sum along any row/column) Then $$ \\nabla \\det A=\\det(A)A^{-\\dagger}=\\text{adj}(A^{\\dagger})=\\text{cofactor}(A) $$ where $\\text{cofactor}$ is the transpose of matrix of elementwise cofactors. Note that $A\\cdot\\text{cofactor}(A)=\\det(A)I_{n}$ (self-evident). Proof 1. Note that $\\det(I+dA)-\\det(I)=\\mathrm{Tr}(dA),$ so $$ \\det(A+dA)=\\det(A)(\\det(I+A^{-1}dA))=\\det(A)+\\det(A)\\mathrm{Tr}(A^{-1}dA) $$ and hence $$ d \\det(A)=\\left< \\det(A) A^{-\\dagger},dA\\right>. $$ Proof 2. $\\det(A)=\\sum_{j}A_{ij}C_{ij}$ for any $i$ so $d \\det(A)=C_{ij}$ (lmao).","title":"Special Matrix Ops"},{"location":"Material%20Knowledge/MIT%20Coursework/notes/W25/Matrix%20Calc/#functions-mathbbrnto-mathbbr-as-banach-spaces","text":"Couple ideas: calculus of variations, random functions, second derivatives. As always, the key idea is that a derivative is a different type; its a linear form, as oppposed to \"another vector space element\". Calculus of variations: we can have functions $u: (\\mathbb{R}\\to \\mathbb{R})\\to \\mathbb{R}$ that map functions $f$ to reals. Generally we can take $\\int G(f,f',\\dots,t)\\,dx$. Then we want $\\nabla u[df]=u(f+df)-u(f).$ Then we also care about $df', df'',\\dots$ the derivatives of $f'$ and we can feed them directly into $G$, i.e. $$ \\nabla u=\\int \\nabla G[df,df',df'',\\dots]\\,dx $$ Note that here, $df',df'',$ etc. are no longer \"independent\" in the usual sense. It's just that the $x$-wise term $\\Delta G$ needs to know $\\Delta (f')$ and that's expressed in $df'$. Euler-Lagrange equations: if we have some \"potential\" expressed as $$ f(u)=\\int F(u,u',x)\\,dx $$ with fixed endpoints, then at equilibrium for all $du$ we get $$ 0=df=\\int \\left( \\frac{dF}{du}[du]+\\frac{dF}{du'}[du']\\right)\\,dx=\\left. \\frac{dF}{du'} du \\right| + \\int \\left( \\frac{dF}{du}- \\left( \\frac{dF}{du'}\\right)' \\right)du\\,dx $$ hence $$ \\int \\frac{dF}{du}=\\int \\left( \\frac{dF}{du'} \\right) '. $$ There's random functions, i.e. we feed some input and get a distribution back (that we sample from). Maybe we want to measure delta of expectation or smth, using Monte Carlo diffs. But, it can be very noisy if our two samples are uncorrelated. Hence correlate them with some confounding variable that preserved marginal but makes the joint difference really small, e.g. use CDF inverse. Then there's bilinear maps. Take a function $f:\\mathbb{R}^{n}\\to \\mathbb{R}.$ Second derivative could be represented as \"differentiate first in $x_{i}$ then $x_{j}$\" i.e. just Hessian matrix all over again. However this doesn't work all that well. Consider taking second derivative of $\\det(A).$ Then linear form is $\\det(A)A^{-1}$ as derived earlier (either linear expansion around 0 or cofactor expansion). The second differential we denote $dA'$, so $$ \\begin{align } d^{2}\\det(A) &=d(\\left< \\det(A)A^{-\\dagger}, dA \\right> )[dA']\\ &=d(\\det(A)\\mathrm{Tr}(A^{-1}dA))[dA']\\ &=d(\\det(A))[dA']\\cdot\\mathrm{Tr}(A^{-1}dA)+\\det(A)\\cdot d(\\mathrm{Tr}(A^{-1}dA))[dA']\\ &=\\det(A)\\mathrm{Tr}(A^{-1}dA')\\mathrm{Tr}(A^{-1}dA) + \\det(A)\\mathrm{Tr}(d(A^{-1})[dA']dA)\\ &=\\det(A)\\left(\\mathrm{Tr}(A^{-1}dA')\\mathrm{Tr}(A^{-1}dA) - \\mathrm{Tr}(A^{-1}dA'A^{-1}dA) \\right). \\end{align } $$ (I did that correctly first try wow) This is how it's done. Now $d^{2}\\det(A)$ becomes a generalized bilinear form on $dA$ and $dA'$ as opposed to the dual of some linear form. We can use these bilinear forms in Taylor expansions, i.e. $f(x)+f'(x)[\\Delta x] + \\frac{1}{2}f''(x)[\\Delta x,\\Delta x]$. Can also use this for \"perfect gradient descent\" i.e. Newton's method on the gradient; given gradient $g$ and Hessian $H$ just set $\\Delta x=H^{-1}g.$","title":"Functions $\\mathbb{R}^{n}\\to \\mathbb{R}$ as Banach Spaces"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/","text":"Already Mastered h,l for left, right j,k for down, up y,p for copy, paste combine as in y3w , use p/P v for visual o,O for open new line, after, before i,a obvious w,b known, capitalize for only whitespace no punc. . repeat, ; next using find character f , , to find previous >>,<< for indents, pref with N to apply below lines 0,^,$ for line movement Use / to search, n,N for movement Use ? to search backwards i for inner (no whitespace) e.g. diw , a for outer, also can do diW etc. V for visual by line e for word end C,D delete to end; cc,dd delete whole; but Y,yy are equiv. A for append to end of sentence t basically fh % to jump to matching ({[ :reg to see list of registers; \"1p to paste yank before this, \"ayw to yank word to \"a J used to join selected lines with space delim cs{sel}([ make a selection, replace surrounding ( with [ similarly ds , or ys to add CTRL-O to old positions, CTRL-I to new ones CTRL-U , CTRL-D Learning ge for back word end {,[( for first new whitespace after last para (,) for { plus firstline para gU<>,gu<>,~ for case (last toggle) use q[x] to record macro to key [x] , use with @[x] <UP> to try previous searches from search history m[x] create mark, '[x] to go to mark :grep to use grep inside current file s{char}{char} v-sneak, can use op, in Obs just cl ]] next { in first column; [[ next } in first column [( for previous unmatched ( and [) for next unmatched ) ; aka. previous/next in this \"scope\" <CTRL-O> for one-bullet hybrid Insert-Normal mode <CTRL-R> for paste from register in Insert mode (use with ctrl-o :reg ) replace mode with R WTF <CTRL-W> S/V to split (horizontal line) or (vertical line) <CTRL-W> h/j/k/l to move windows <CTRL-A> to inc. next number <CTRL-X> to dec. next number","title":"Vim Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#already-mastered","text":"h,l for left, right j,k for down, up y,p for copy, paste combine as in y3w , use p/P v for visual o,O for open new line, after, before i,a obvious w,b known, capitalize for only whitespace no punc. . repeat, ; next using find character f , , to find previous >>,<< for indents, pref with N to apply below lines 0,^,$ for line movement Use / to search, n,N for movement Use ? to search backwards i for inner (no whitespace) e.g. diw , a for outer, also can do diW etc. V for visual by line e for word end C,D delete to end; cc,dd delete whole; but Y,yy are equiv. A for append to end of sentence t basically fh % to jump to matching ({[ :reg to see list of registers; \"1p to paste yank before this, \"ayw to yank word to \"a J used to join selected lines with space delim cs{sel}([ make a selection, replace surrounding ( with [ similarly ds , or ys to add CTRL-O to old positions, CTRL-I to new ones CTRL-U , CTRL-D","title":"Already Mastered"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#learning","text":"ge for back word end {,[( for first new whitespace after last para (,) for { plus firstline para gU<>,gu<>,~ for case (last toggle) use q[x] to record macro to key [x] , use with @[x] <UP> to try previous searches from search history m[x] create mark, '[x] to go to mark :grep to use grep inside current file s{char}{char} v-sneak, can use op, in Obs just cl ]] next { in first column; [[ next } in first column [( for previous unmatched ( and [) for next unmatched ) ; aka. previous/next in this \"scope\" <CTRL-O> for one-bullet hybrid Insert-Normal mode <CTRL-R> for paste from register in Insert mode (use with ctrl-o :reg ) replace mode with R","title":"Learning"},{"location":"Material%20Knowledge/Sciences/Coding/Vim%20Tricks/#wtf","text":"<CTRL-W> S/V to split (horizontal line) or (vertical line) <CTRL-W> h/j/k/l to move windows <CTRL-A> to inc. next number <CTRL-X> to dec. next number","title":"WTF"},{"location":"Material%20Knowledge/Sciences/Coding/JS%20hacking/Karina%27s%20thoughts/","text":"console hacking grab elements; querySelector via class or id manipulate objects automate stuff like this greasemonkey for using custom/existing js scripts download cookies/python automate things like this","title":"Karina's thoughts"},{"location":"Material%20Knowledge/Sciences/Coding/JS%20hacking/NODE/","text":"npm is the package installer node allows you to run js files just in the terminal like any other programming language still need to figure out browser interactions","title":"NODE"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/","text":"Rust is a parallel language to C++, with many similar ideas (static typing, compiled language) but also many new ones (very strong compiler guarantees, safe/unsafe code). Here are some the of the key ideas. Rust Compiler Stronk Ownership Every item is owned by some variable name. The code must always \"know\" what each object is owned by. For instance, a vector owns all of its constituents. These are the permitted operations: - Immutable borrow (must be \"returned\") - Mutable borrow (can write, nobody else can borrow, must be \"returned\") - Move (change ownership, previous variable gets \"consumed\") When passing &var or &mut var in a function signature, that's a borrow. But when directly passing var , that's a move, and the original variable cannot be accessed anymore. Consuming can be a good API for when an object should not be used anymore after a function call, i.e. all of the important guts got \"moved\" out. There's lots of work that goes into moving objects that can't be clone() , such as remove() and swap() . Lifetimes There are also lifetimes, which the compiler usually infers but sometimes special things need to be done. - Suppose A has a skateboard, B borrows the skateboard from A, and C borrows a sticker on the skateboard from B. Then C gives the sticker to D (function return). - The first borrow must last at least as long as the second borrow, i.e. the reference must live at least as long. This is only used when change of scope happens and the second reference moves out of the scope e.g. function calls. If the sticker is borrowed in \"inline\" code the skateboard reference is immediately required to live that long. Safety paradigms C++ makes the choice of making the programmer check guarantees at runtime . The Rust compiler enforces certain guarantees at compile-time so that those bugs can be found earlier by the compiler, rather than in production. The errors thrown at compiletime are usually panics , such as division by zero, unwrapping a None value, Typing choices Traits Rust borrows a lot from type theory, while mixing in some of the compiled-language efficiency of C++. Namely, all objects have static types, defined with the struct keyword. Traits are a \"property\" that multiple types can satisfy. Traits can provide \"associated types\" and methods. Functions have their arguments be a certain type, but they can also have arguments satisfy certain traits. Generics Syntactically, generic types are integrated into function signatures and traits in a way different from other languages. Rust's rigorous type checking encourages frequent use of generics. Generics can also be required to have trait bounds, so that e.g. an argument assumed to be of a certain type satisfying a given trait can call the methods from that trait, and Rust will know those methods exist. Traits with different generics become different traits, so MyTrait<i32> and MyTrait<bool> can both be implemented by the same type. Enums An enum is syntactic sugar for some type to be one of a list of hard-coded types. There is special match syntax that allows for concise control flow depending on the type of an enum. The Option<T> type The Null type is just horrible. Hence, the Option<T> , which is an enum for either None or T . Calling None.unwrap() will panic!() . Trait objects There is a special &dyn Trait keyword that allows for pointers to a set of objects that satisfy a given trait. Then these objects could be any type that implements the trait, allowing for expandable features. This is implemented under the hood with a vtable matching types to function pointers. Smart pointers There's Box , Mutex , Rc , Arc , when the reference doesn't suffice. Just google if necessary. Specifically, the Box moves its contents to heap and adds a pointer to the object on the stack, returning the pointer. It puts the object in a box. Other Rust specifics Iterators Oh, my god! these are awesome. Calling .iter() on a collection will create an object satisfying the Iterator trait, i.e. it has next() etc. creating immutable borrow. Then there's iter_mut() which returns mutable borrows, and into_iter() creating iterators that own the respective objects. Unsafe code When exposing some key functionality such as splitting a vector (consuming a mutable slice and returning two halves of that slice) unsafe code must be used. It's wrapped in a unsafe code block, and it's up to the programmer to check that the contracts of that function are satisfied. Tests Rust has builtin configs such as #[cfg(test)] that makes unit tests and integration tests very easy. Routing Code is organized into crates. Crates are split into \"library\" denoted by src and \"binary\" denoted by bin . The binaries are meant for production, and libraries as APIs for binaries or other projects. Cyclic imports between crates is not allowed, but any imports within one crate is fine. Imports from other crates need to be declared in Cargo.toml , along with other compilation and testing options. mod creates a module, and use imports things. Obviously, the placement and naming of files can determine their import paths. Macros Every #[...] generates code of some kind, using a TokenStream of the code block below it. All the macro!() lines apply some pattern to generate code in place at compile time, in a way similar to functions, except the assembly is quite literally expanded. Parallelism Can initiate multiple threads. Object must implement Send if its ownership is going to be transferred to another thread, and must implement Sync if its references can be sent between threads (i.e. multiple threads can read/write). So constants are Send, and Arc/Mutex are Sync, etc.","title":"Rust notes"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#rust-compiler-stronk","text":"","title":"Rust Compiler Stronk"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#ownership","text":"Every item is owned by some variable name. The code must always \"know\" what each object is owned by. For instance, a vector owns all of its constituents. These are the permitted operations: - Immutable borrow (must be \"returned\") - Mutable borrow (can write, nobody else can borrow, must be \"returned\") - Move (change ownership, previous variable gets \"consumed\") When passing &var or &mut var in a function signature, that's a borrow. But when directly passing var , that's a move, and the original variable cannot be accessed anymore. Consuming can be a good API for when an object should not be used anymore after a function call, i.e. all of the important guts got \"moved\" out. There's lots of work that goes into moving objects that can't be clone() , such as remove() and swap() .","title":"Ownership"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#lifetimes","text":"There are also lifetimes, which the compiler usually infers but sometimes special things need to be done. - Suppose A has a skateboard, B borrows the skateboard from A, and C borrows a sticker on the skateboard from B. Then C gives the sticker to D (function return). - The first borrow must last at least as long as the second borrow, i.e. the reference must live at least as long. This is only used when change of scope happens and the second reference moves out of the scope e.g. function calls. If the sticker is borrowed in \"inline\" code the skateboard reference is immediately required to live that long.","title":"Lifetimes"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#safety-paradigms","text":"C++ makes the choice of making the programmer check guarantees at runtime . The Rust compiler enforces certain guarantees at compile-time so that those bugs can be found earlier by the compiler, rather than in production. The errors thrown at compiletime are usually panics , such as division by zero, unwrapping a None value,","title":"Safety paradigms"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#typing-choices","text":"","title":"Typing choices"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#traits","text":"Rust borrows a lot from type theory, while mixing in some of the compiled-language efficiency of C++. Namely, all objects have static types, defined with the struct keyword. Traits are a \"property\" that multiple types can satisfy. Traits can provide \"associated types\" and methods. Functions have their arguments be a certain type, but they can also have arguments satisfy certain traits.","title":"Traits"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#generics","text":"Syntactically, generic types are integrated into function signatures and traits in a way different from other languages. Rust's rigorous type checking encourages frequent use of generics. Generics can also be required to have trait bounds, so that e.g. an argument assumed to be of a certain type satisfying a given trait can call the methods from that trait, and Rust will know those methods exist. Traits with different generics become different traits, so MyTrait<i32> and MyTrait<bool> can both be implemented by the same type.","title":"Generics"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#enums","text":"An enum is syntactic sugar for some type to be one of a list of hard-coded types. There is special match syntax that allows for concise control flow depending on the type of an enum.","title":"Enums"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#the-optiont-type","text":"The Null type is just horrible. Hence, the Option<T> , which is an enum for either None or T . Calling None.unwrap() will panic!() .","title":"The Option&lt;T&gt; type"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#trait-objects","text":"There is a special &dyn Trait keyword that allows for pointers to a set of objects that satisfy a given trait. Then these objects could be any type that implements the trait, allowing for expandable features. This is implemented under the hood with a vtable matching types to function pointers.","title":"Trait objects"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#smart-pointers","text":"There's Box , Mutex , Rc , Arc , when the reference doesn't suffice. Just google if necessary. Specifically, the Box moves its contents to heap and adds a pointer to the object on the stack, returning the pointer. It puts the object in a box.","title":"Smart pointers"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#other-rust-specifics","text":"","title":"Other Rust specifics"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#iterators","text":"Oh, my god! these are awesome. Calling .iter() on a collection will create an object satisfying the Iterator trait, i.e. it has next() etc. creating immutable borrow. Then there's iter_mut() which returns mutable borrows, and into_iter() creating iterators that own the respective objects.","title":"Iterators"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#unsafe-code","text":"When exposing some key functionality such as splitting a vector (consuming a mutable slice and returning two halves of that slice) unsafe code must be used. It's wrapped in a unsafe code block, and it's up to the programmer to check that the contracts of that function are satisfied.","title":"Unsafe code"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#tests","text":"Rust has builtin configs such as #[cfg(test)] that makes unit tests and integration tests very easy.","title":"Tests"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#routing","text":"Code is organized into crates. Crates are split into \"library\" denoted by src and \"binary\" denoted by bin . The binaries are meant for production, and libraries as APIs for binaries or other projects. Cyclic imports between crates is not allowed, but any imports within one crate is fine. Imports from other crates need to be declared in Cargo.toml , along with other compilation and testing options. mod creates a module, and use imports things. Obviously, the placement and naming of files can determine their import paths.","title":"Routing"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#macros","text":"Every #[...] generates code of some kind, using a TokenStream of the code block below it. All the macro!() lines apply some pattern to generate code in place at compile time, in a way similar to functions, except the assembly is quite literally expanded.","title":"Macros"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Rust%20notes/#parallelism","text":"Can initiate multiple threads. Object must implement Send if its ownership is going to be transferred to another thread, and must implement Sync if its references can be sent between threads (i.e. multiple threads can read/write). So constants are Send, and Arc/Mutex are Sync, etc.","title":"Parallelism"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/Terminal%20shit/","text":"ps aux | grep __ pkill has a bunch of options -f for final -x for command -9 for \"must die\" -u for user","title":"Terminal shit"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-30/","text":"source here rust new version of std::sort, called driftsort for small cases (<=20 elem) it does either insertion sort (for NonFreeze types that can be mutated during comparison, or Arc that can't be copied byte by byte) in general it does quicksort using a weird $n^{\\log_{8}3}$ recursive median of 3s algorithm they do a fun thing with incrementing lower pointer, decrementing larger pointer, swap when bump into a swap that can be made and if a $\\sqrt{ n }$ run is found it does some merging using mergesort takeaways: - lots of hardware-centric optimizations - branchless arithmetic computations - small compiled code for instruction-cache - different sorting algorithms depending on level of safety and then ofc quicksort is good for cache locality to begin with also learned about CMOVE which only moves data on a conditional, instead of branching on a conditional - branch mispredictions can be expensive there are some sort9/sort13 things hardcoded in for smallsort Quicksort there's two ways to execute quicksort partition - hoare, have left pointer coming right and right pointer coming left which swap - lomuto, have pointer coming right and \"last swapped\" pointer which is last below item, swap with first higher when encounter new lower https://github.com/Voultapher/sort-research-rs/blob/main/writeup/lomcyc_partition/text.md swap optimizations: - \"branchless\": instead of doing conditional branches (high overhead cuz of mispredict), just make copies of both and choose using arithmetic - \"cyclic permutation\": a swap is acutally \"store, move, move\" so amortized 3 per swap - if you just put a \"gap\" in and manage correctly then only amortized 2 per swap also about \"sort safety\", apparently C people think users should make sure their sort functions are weak strict sorting, as opposed to","title":"Nysrg 6 30"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-30/#quicksort","text":"there's two ways to execute quicksort partition - hoare, have left pointer coming right and right pointer coming left which swap - lomuto, have pointer coming right and \"last swapped\" pointer which is last below item, swap with first higher when encounter new lower https://github.com/Voultapher/sort-research-rs/blob/main/writeup/lomcyc_partition/text.md swap optimizations: - \"branchless\": instead of doing conditional branches (high overhead cuz of mispredict), just make copies of both and choose using arithmetic - \"cyclic permutation\": a swap is acutally \"store, move, move\" so amortized 3 per swap - if you just put a \"gap\" in and manage correctly then only amortized 2 per swap also about \"sort safety\", apparently C people think users should make sure their sort functions are weak strict sorting, as opposed to","title":"Quicksort"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%206-9%20%2B%206-16/","text":"SQL NoSQL NewSQL care about ACID, atomicity (transactions don't complete partially), consistency (always correct), isolation (txs don't inteerfere with each other), durability (data put now is there later) FoundationDB: a \"simple\" \"minimalistic\" key-value store that first built in 2009, Apple acquired, open-sourced in 2021 distributed database ![[Pasted image 20240616144010.png]] client talks to proxy, asking for a read-only snapshot, proxy returns read version e.g. tx A has rv 100, tx B has rv 101, then tx A will receive commit version 102 i.e. at v102 tx A's thing would be \"done\" there's a resolver that figures out what conflicts exist and what commit time is OK keeps a hybrid clock of some kind e.g. if tx B,C,D use rv 102, 103, 104, then tx A gets cv 105 actually scratch that client asks proxy for rv. greater than any previous issued cv then goes and asks storage system (through ls through proxy) for certain reads at that rv finally submits read/write sets to proxy, proxy asks sequencer for a commit version in ekzhang's head: - coordinators form a distributed quorum storing config, control, etc. - delegate a sequencer, ratekeeper, data distributor (?) -","title":"Nysrg 6 9 + 6 16"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%207-7/","text":"Okay, just finished reading Redis version 1.3.6 with ekzhang's NYSRG. Some of the things that I learned are... This person put an entire database, socket, query, protocol, all in basically a single 9000 C file with a bunch of internalizers. So, there's a picture of the server that will probably be placed here. But, basically it just has all the pieces that you need just put together, wrapped together. So, it supports virtual memory, so you can swap from memory onto disk. It supports flushing memory onto disk. It supports what's called RDB, Redis Database. So, it will just copy everything to disk periodically. It holds the entire database in memory, which is kind of weird. I suppose that's what makes it kind of easy slash small in the sense that it can be stored entirely in a 9000-line C file because everything is just kind of done in memory. Also, relative to its 2010 launch, it had the innovation of supporting lists, sets, large datasets. I guess this time reading code, I learned about paying attention to structs and paying attention to config files and comments. The comments in the config files can kind of direct your attention as to what to pay attention to. In particular, when trying to dissect large libraries, the important thing is to figure out which structs and methods are important and then frame your intuition around those structs. Create a mental model that frames around some key elements and then fill in details one at a time when reading other methods. file:///Users/alex-zhao/Coding/assorted/redis/doc/README.html ![[Pasted image 20240707205010.png]]","title":"Nysrg 7 7"},{"location":"Material%20Knowledge/Sciences/Coding/Systems/nysrg%20week%2031/","text":"mainframe -> aws compute warehouse -> disaggregated db warehouse model - several vms each with their own cpu, ram, disk, gpu, etc. - each separated, just rent your own - ISSUE: some tasks use only lots fo ram, lots of disk, lots of gpu (ML) etc. disaggregated model - large block of cpu, ram, disk that gets cut up into pieces for each job dynamically -","title":"Nysrg week 31"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/","text":"Recursion+DP Transition How do I build a state from a previous state? What information needs to be preserved? What information can be disposed of? How do I \"unpeel\" it by layers? Iterate? Backtracking If I want to explore multiple \"children\" of the current state, how do I return to the parent? how to keep track of the children? DFS traversal of the tree, in essence Memoization ==What computations are repeated?== What's a naive DP idea? How can I ==add info== to make a valid transition? Specific Tricks Permutations can be bitmasked Subarray DPs, such as digit substrings, or XORs from big-small index Use dx/dy for grid problems Utilize spatial locality If potential MLE discard DP layers no longer in use Almost always write iteratively Unless it's tree, in which case write recursively Data Structures Set, MultiSet are ordered BBST Unordered are hash tables ==Cheat with PBDS==, good for sliding window Fenwick Tree: ==last 2^n==, for range queries (not used) Segment Tree: just fill out to power of 2; consider \"lazy Segment\" or \"Segment of Segment\" Remember PURS, RUPF, if needed consult Plat for RURS Use explicit representation (in 1D flattened array) for random access Stack, Queue, Deque: much better constant-time performance than other containers Priority Queue maintained by Heap, maintaining greater than children Graph Reps Adjacency Matrix: weight[i][j] Adjacency List: adj[i] lists all neighbors and corresponding weights of i Union-Find Disjoint Set: rep. partition of a collection as a forest, root of each tree is representative for set Greedy with Dijkstra's or Kruskal's using ==PQ== Tree Augmented DFS Includes Tree DP and Euler Tour Always Root Use DP Mindset-what can I memoize? ==Start with naive recursion== $\\sqrt n$ Divide and Conquer Range queries: rep. as pairs $(a,b),$ sliding window by semantic order Basically Meet in Middle for $\\sqrt N$ time","title":"USACO Notes"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#recursiondp","text":"","title":"Recursion+DP"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#transition","text":"How do I build a state from a previous state? What information needs to be preserved? What information can be disposed of? How do I \"unpeel\" it by layers? Iterate?","title":"Transition"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#backtracking","text":"If I want to explore multiple \"children\" of the current state, how do I return to the parent? how to keep track of the children? DFS traversal of the tree, in essence","title":"Backtracking"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#memoization","text":"==What computations are repeated?== What's a naive DP idea? How can I ==add info== to make a valid transition?","title":"Memoization"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#specific-tricks","text":"Permutations can be bitmasked Subarray DPs, such as digit substrings, or XORs from big-small index Use dx/dy for grid problems Utilize spatial locality If potential MLE discard DP layers no longer in use Almost always write iteratively Unless it's tree, in which case write recursively","title":"Specific Tricks"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#data-structures","text":"Set, MultiSet are ordered BBST Unordered are hash tables ==Cheat with PBDS==, good for sliding window Fenwick Tree: ==last 2^n==, for range queries (not used) Segment Tree: just fill out to power of 2; consider \"lazy Segment\" or \"Segment of Segment\" Remember PURS, RUPF, if needed consult Plat for RURS Use explicit representation (in 1D flattened array) for random access Stack, Queue, Deque: much better constant-time performance than other containers Priority Queue maintained by Heap, maintaining greater than children","title":"Data Structures"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#graph-reps","text":"Adjacency Matrix: weight[i][j] Adjacency List: adj[i] lists all neighbors and corresponding weights of i Union-Find Disjoint Set: rep. partition of a collection as a forest, root of each tree is representative for set Greedy with Dijkstra's or Kruskal's using ==PQ==","title":"Graph Reps"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#tree-augmented-dfs","text":"Includes Tree DP and Euler Tour Always Root Use DP Mindset-what can I memoize? ==Start with naive recursion==","title":"Tree Augmented DFS"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/#sqrt-n-divide-and-conquer","text":"Range queries: rep. as pairs $(a,b),$ sliding window by semantic order Basically Meet in Middle for $\\sqrt N$ time","title":"$\\sqrt n$ Divide and Conquer"},{"location":"Material%20Knowledge/Sciences/Coding/USACO/Syntax%20Details/","text":"Pre/Post Increment: --x, x-- (first in cmdname happens first) struct : class with all public attributes Initialize Stick to template for segtree Cool template goto bits/stdc++.h.gch use lambdas creatively, e.g. sorting int *ptr creates pointer ptr to int object (automatic reference for C-type array) 1-index for graph or tree problems! And 1-index segtrees","title":"Syntax Details"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/","text":"sana, again openvla pizero source sent by ge moviegen prime video transformer arch flows, euler flow, etc. read flux codebase flow-dpm titans from gr boyuan diffusion papers prime olmo flashinfer srush tensor puzzles meta lingua deepseek paper again learn triton triton softmax srush triton puzzles read math in prob ml - [x] ditto - can backprop differentiable losses on diffusion model outputs to noise inputs! and get desired outputs Large Memory Layers with Product Keys https://arxiv.org/pdf/1907.05242 Patchscope: A Unifying Framework for Inspecting Hidden Representations of LLMs https://arxiv.org/pdf/2401.06102 Recovering the Pre-Fine-Tuning Weights of Generative Models https://arxiv.org/abs/2402.10208 Location Verification for AI chips https://static1.squarespace.com/static/64edf8e7f2b10d716b5ba0e1/t/6670467ebe2a477eb1554f40/1718634112482/Location%2BVerification%2Bfor%2BAI%2BChips.pdf Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks https://arxiv.org/pdf/1703.10593 Improving Alignment and Robustness with Circuit Breakers (s/o rowan for coauthoring) https://arxiv.org/pdf/2406.04313 Quantization Model of Neural Scaling (s/o Uzay) https://arxiv.org/pdf/2303.13506 Gradient Routing (s/o Jacob) https://arxiv.org/pdf/2410.04332 Binlear MLPs for Interp https://openreview.net/pdf?id=gI0kPklUKS Not really AI: Anatomy of a Bit: Information in a Time Series Observation https://arxiv.org/pdf/1105.2988 Prediction, Retrodiction, and The Amount of Information Stored in the Present https://arxiv.org/abs/0905.3587 sasha rush awesome (outdated?) https://github.com/huggingface/awesome-papers long awesome ML https://github.com/aimerou/awesome-ai-papers?tab=readme-ov-file o1-related https://github.com/srush/awesome-o1/ awesome ML tools https://github.com/srush/awesome-machine-learning awesome discrete diffusion https://github.com/kuleshov-group/awesome-discrete-diffusion-models sys/inference optimization, large text models [ ] PRIME [ ] grok 1 [ ] siglip [x] flashattention want to focus on minimizing i/o writes to and from gpu vram the goal is to do chunked attention QV muls in the cache close to the gpu make multiple passes over the output, each time updating by adjusting softmax denom and adding new term obv. keep running log of the softmax denom and have to re-read on every block prefetch/kernel fuse as desired [x] olmo training curriculum with general text/fineweb-edu, then text for specific tasks to push benchmark scores stability tricks: Z-score regularization, gradient clipping, KV-norm, RMSnorm, no biases, model soups, careful model init, low lr (1e-8), post-norm, weight decay, dropout what i learned from william brandon: nvidia gpus have two caches, L2 and L1 each streaming multiprocessor has its own L1 physically colocated with the SM in the time it takes to send data between L1 and SM, can do 100 tensor operations four datatypes to be aware of: fp32, horrible; tf32, better than bf16 but takes more memory fp16, really not great; bf16, what everyone uses bf16 (if not careful) will get converted into fp32 which is horrible V100s are horrible for the above reason also talked about modular duality, lipschitz stuff from jeremy + laker newhouse, new optimizers life advice: do stuff, make stuff, remake primitives from scratch, when possible if i'm scared of something just do it also learn cuda in 100hrs in 6.S894 labs (use 1xRTX A4000) [ ] ray python dist package [ ] deepspeed [ ] megatron [ ] jetformer [ ] ddp, fsdp, megatron, gpipe [ ] what to knwo about cpu memory [ ] flexattention [x] streamingllm, linformer linformer just projects $n\\times d$ key, weight matrices into $k\\times d$ using direct linear $n\\to k$ linear map (\"squishing\" vectors to make the seq shorter) streamingllm analyzed attention magnitudes, found that \"leftover\" attention would be dumped into \"attention sinks\" in the softmax which would then be ignored added initial tokens back to inference, perplexity fixed (compared to sliding window) [ ] speculative decoding [ ] yarn uses RoPE (instead of adding pos. embedding, multiply by $D/2$ complex numbers with AP magnitude) pretrain on 10K context/RoPE, then interpolate arguments of complex numbers in pos. embedding carefully preserve higher frequency numbers and move lower [ ] lolcats [ ] deepseek v3 MTP, Yarn, \"long-winded and overly reflective\" R1 with system prompt during SFT for transfer, carefully load balanced MoE [x] hymba combine SSM and transformer side by side as opposed to in sequence, has good \"synergy\" because of opposing behaviors Transformers arch with LN, LinUp, split SSM/Attn, LinDown, (+), LN, FFN, (+) (N-3)/2 SWA Hymba blocks on each side of global attention interp [ ] automated circuit discovery [ ] path patching [ ] the logit lens (blogpost) [ ] nnsight docs [x] achyuta's sts paper uses cross-layer-attribution matrix based on gradient of next layer on each neuron in current take top $k$ in each layer locally, then iteratively refine by taking last layer, grab most influential neurons from prev, etc. go backwards, then go forwards, until circuit stops changing stich images from two classes into a single image, analyze behavior of both circuits under this hybrid image FROM TAMAR [x] https://anishk23733.github.io/vl-interp/ VLM embeds images and places before text in transformer decoder hence image embeddings are in the same \"space\" as text embeddings goal: measure \"internal confidence\" about an object in an image, potentially ablate defines \"internal confidence\" about an object in an image as maximum of logit lens (result of applying final unembedding matrix and grabbing logit for token for that object) over all image embeddings (one for each patch) over all layers in the VLM cutoff at a certain internal confidence, and then ablate/activate based on that questions: why are all layers in the model treated equally? can we learn a better combination of information to get internal confidence in a self-supervised way? what about image encoders that don't keep spatial information in an easily interpretable way, how to check segmentation for these? logit lens seems \"naive\"\u2014why do we expect image tokens to speak \"exactly\" the \"same language\"? should we try to learn adjustments to the embedding matrix, esp. wrt. position? good results for end task\u2014hallucination reduction\u2014but gap between hallucation reduction v.s. cd reduction seems small, perhaps can be made more effective with ablation engineering or internal confidence engineering [x] https://vision-of-vlm.github.io/ \"the mechanisms underlying how VLMs process visual information remain largely unexplored.\" through a creative \"knockout\" experiment, demonstrate implicit image information through text tokens is sufficient, alternatively image tokens only needed for layers 20-40 in layers 20-40 attention values make an attempt at segmentation able to get \"compressed context\" by, for each layer, take only the image tokens with top-K attention values notably works much better with InternVL2-76B (patches into 40x40) than with LLaVa-1.5-7B (patches in 16x16), suggesting that large patching is just very redundant/inefficient and should no longer be pursued question: if we take the best tokens for each layer then perhaps over all layers we end up including all image tokens at least once? even worse, if we change which image tokens are included on inference for each text token, perhaps over all text tokens, each layer eventually sees all image tokens? also perhaps these results are to be \"expected\" because of dropout training schedules? [x] https://arxiv.org/pdf/2410.07149 adapter VLM that maps CLIP patch embeddings into language token space refers to a register token hypothesis\u2014that blank background patches collect global information; relegates as a side effect uses three methods of checking token understanding, finds that ablating object tokens (replacing with global mean) removes understanding\u2014expected; how to know there's a dog when there are no dog patches? uses logit lens, finds that tokens become aligned with corresponding text embeddings as layers progress (surprising! no compulsion to align with text, some implicit discovery of \"best coding\" for each patch) most valuable communication between image and text tokens happens in later layers (attention knockout) i'm surprised they discard register hypothesis so quickly, i believe register [x] https://arxiv.org/abs/2310.10348 general program of ACDC: Automated Circuit DisCovery activation patching does one forward pass for every edge/node/wtv. that has to be tweaked, replacing activation at that node with either than average ablation or the activation from some other \"corrupted\" prompt, sees what the change in metric is attribution patching uses derivative of metric in activation at each node/edge to construct a circuit represent network as a computational graph, differentiate between modules, nodes, edges specifically use EAP, edge attribution patching measure false positive rate/true positive rate from human expert baseline, both increase as %pruning increases two approaches to calculating relevance: take a gradient all the way to the top, or take gradient of next layer(s) from this layer and compute recursively recursive is easier to ensure selected subgraph is actually connected [ ] A Mathematical Framework for Transformer Circuits rl agents/automation [ ] sakana [ ] diffusion policy use diffusion to denoise action latents that guide some agent in rl for whatever reason diffusion is rlly good lol idk robotics/rl [ ] kinetix rl physics lib [ ] aloha robotics [ ] openvla [x] ppo want your policy to change but not by too much use advantage weighted log likelihood updates, but clip the upside to some $\\epsilon$ makes it a lot more stable [ ] grpo/deepseekmath data mix using iterative pipeline word n-gram neural model, averages all n-gram embeddings and mlp classification collect dataset, do grpo on DeepSeekCoder unified rl perspective: all algos can be described in terms of space parameterization, reward signal parametrization, policy gradient coefficient, GRPO just uses rule [ ] diffusion for robotics [x] pi's new paper (read ??) [x] action latents [ ] sergey levine from berkeley has takes [ ] some yilun du papers: original EBM , his thesis , his research statement diffusion [ ] diffusion amth [ ] figure out diffusion/normalizing flow/flow forcing, fr, https://boyuan.space/diffusion-forcing/ [ ] flow matching math [ ] moviegen [ ] dall-e algorithm [ ] papers from william brandon sci of dl [ ] scaling via data manifolds [x] low rank can update matrix weights using low rank factorizations of matrices [x] modular duality by bernstein two angles for motivation: gradients are not same type as weights, they are linear functionals on the weights i.e. $\\Delta \\mathcal{L}=g^{T}w.$ Hence we should update by the result of some \"duality map\" applied to the gradient. In the gradient case where we want to optimize $g^{\\dagger}\\Delta w + \\lVert \\Delta w \\rVert^{2}$ we want $\\Delta w$ to be in the direction of the dualizer. input/outputs are normed, we want to update weights so that activations stay in the same approximate norm. Specifically we set weight norms to be dual norms on input/output using their respective norms, i.e. $\\lVert W \\rVert=\\max \\frac{{\\lVert W x_{in} \\rVert_{out}}}{\\lVert x_{in} \\rVert_{in}}.$ Generalizing, we want to maintain smoothness of our layers for easy future gradient descent. Given the gradient, want to update in the direction that gives the most \"bang for buck\" accounting for some norm on the weights. Above maximum can be interpreted as curvature, i.e. fastest ROC of output given fixed input change. Dualizer is the argmax of this norm. They arrive at several duality maps for different modules using specific norms, e.g. RMS for linear, max(RMS) over channels for convolution, $l_{1}\\to RMS$ for embedding. Using the above max expression, get spectral nom for Linear, max spectral norm for Conv2D, some other weird stuff idk lmao Formalize modules into a fwd, mass, sensitivity (curvature), and norm for the weights (defined by norms over input/output). ssm [ ] albert gu's thesis [ ] mamba2: ssm > transformers [ ] mamba is attentino audio/image/video specific in audio \"conv subsampling\" is just decreasing spatial resolution with convs [x] conformer FFM, MHA, Conv, FFM Conv is actually kinda MLP, with linear up channel (pointwise), activation, norm, conv, activatoin, linear down [x] moondream uses image patches with crops of fixed sizes, vit, recombine crops to get image embedding image adapter, prompt prefix, text tokens [ ] pixtral (vlm) [x] Stable Diffusion audio codec (400-700 bps!!) patch ViT style, 300 ish blocks of strided conv followed by lots of transformers (sliding attention, 128 tokens) fully scalar quantization\u2014bottleneck into even lower dim, and then quantize each dim -> one token either learn N scalars to snap to, or parametrize (they use floor tanh) quantizer-dropout occasionally [x] Hiera develops a new fully-transformer based vit encoder/decoder for classification/segmentation tasks on image/video on each downsize, instead of using strided conv, use Q-pooled attention, similar to cross attention but Q terms get pooled via maxpool masked loss, mask tokens don't get fed into the encoder only the decoder (they get \"filled in\") use local window attention at lower resolutions, then global later on [x] Nvidia Cosmos 2 AE and generative start with Haar 3D wavelet in time/space to downsample spatially alternating factorized resblock and factorized attention with downsampling ends at latent dimension 16 for continuous (diffusion) and 6 for discrete (autoregressive) every video token is predicted autoregressively after flattening in sequence, use finite scalar quantization to quantize to total 64K vocab size because attention is used on \"every layer\" context length is only 34 frames-quite sad lmao use 8x16x16 compression for autoregressive and 8x8x8 for diffusion autoregressive: hybrid 3D RoPE with absolute embeddings per head for each token, cross attention from text prompt (NOT concat, think abt why) diffusion: typical DiT, patchify using 1x2x2 and project/flatten, then text CA plus adaLN from time like suno, best is to use autoregressive to generate discrete tokens, then use \"diffusion decoder\"\u2014condition on autoregressive tokens and regenerate continuous latents using diffusion [x] MAE Make AE modeling a masked task by masking some of the patches feed into encoder (with pos embeddings) without mask tokens, feed into decoder with mask tokens, allows huge speedup downstream ViT task still great, AE learns to encode global \"semantic properties\" cuz of masking task [x] meta visual tokenizers opposing the conv/attention interleave strategy used by cosmos/SD, try full send single conv with 8x16x16 stride=kernel (aka patching) into ~1k feature dim, then full ViT with 3D RoPE/SwiGLU, then bottleneck works well, however performance does not scale with encoder/decoder size (why???) [x] original dit u-nets are great but let's try transformers instead standard latent diffusion, break latent into $p\\times p$ patches (use $p=2$) classifier-free guidance, i.e. constraining $p(c|x)$ implicitly by using $\\log p(x | c) - \\log p(x|\\emptyset)$ since $p(c|x)=\\frac{{p(x|c)p(c)}}{p(x)}$, then setting $\\mathcal{L}=p(x)+\\gamma p^*(c|x)$ for $\\gamma>1$ various conditioning strategies including cross-attention adaptive layernorm with \"zero scaling\" (scaling right before residual by multiplying by MLP output $\\alpha$ initialized to 0) straight concat Transformer decoder with just a linear and rearrange to get predicted $\\epsilon$ and diagonal $\\Sigma.$ [ ] original vit [x] sana huge compression vae, with F32C32P1 instead of standard F8C8P2, leads to 4x fewer tokens (1/16 \"patches\", 4x channels, 4x patches) linear attention using $ReLU(Q)ReLU(K)^{T}$ instead of softmax, then instead of computing $n\\times n$ matrix can do $ReLU(K)^{T}V$ first to get $d\\times d$ matrix for linear time full sequence training inference is $O(1)$\u2014additional K,V vectors add to the new $d\\times d$ matrix, then mv-mul with new Q vector add nonlinearity with 1x1 convs along channel and 3x3 conv along channel, token index and GLU no positional encoding! rely on 3x3 conv for implicit positional info bit quantization, triton cuda shit, for faster inference [x] some Flow-DPM math \u23eb \ud83d\udcc5 2025-01-31 \u2705 2025-02-01 [x] wav2vec architecture: 6-deep convnet for latent representation (stride 20ms, receptive field 25ms at 16k hz), then quantize using product codebook w linear projection (gumbel softmax to get $V\\times G$ logits for codebook), causal transformer on latents to get context representation train by masking some latents with learned mask token, cosine similarity for cross-entropy loss between \"real\" masked quantized latent and several distractors mask $\\approx$ half of latents intuition: can reconstruct missing latent (when quantized) i.e. latents contain useful info (note no reconstruction!) quantization isn't used for one-hot encodings autoregressive-style, rather is used to \"regularize\" the latents with some amount of \"smoothness\" fine-tune final contextual representations into text token classes with a linear layer (includes blank token) uses Connectionist Temporal Classification loss i.e. takes \"best possible\" alignment allowing for repetitions and blank tokens, takes log prob [ ] seq-2-seq (in comparison to wave2vec) [ ] non-spiky ctc loss basically, it's easy to just say blank by default, want to keep blank use priors that prefer unigram i.e. non-blank so that probability for seeing given character is divided by prior probability to incentivize picking unigrams more often [x] ctc loss CTC probability is sum over all alignments of getting the desired output, because this is subset of all possible labellings of all things this is guaranteed $\\leq 1$ use DP to compute sum of probabilities of all alignments ending at given target sequence index, for given frame introduce blank token and merging adjacent equal tokens to get \"skips\" [ ] HuBERT cnn encoder ofc (it's 2021) idea: masked loss training on quantized targets but what are the targets? a: really degenerate bootstrapped middle activation start with random quantization with some linear map do the best you can, then use middle layer of transformer as representation, k-means cluster try again [ ] tracks-to-4d [ ] mqvae miscell [ ] DPO [ ] transfer learning survey [ ] shampoo [ ] decision transformer [x] dqvae [ ] vqgan [x] vqvae [ ] deepseek prover [ ] little book of dl [x] BERT and original transformers attention (2017): add in positional encodings, also mask attention before softmax using causal, transformers solve text gen BERT: full self-attention with masks using masked language modeling (MLM) loss; cross-entropy on logits on masked tokens","title":"ML papers"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#sysinference-optimization-large-text-models","text":"[ ] PRIME [ ] grok 1 [ ] siglip [x] flashattention want to focus on minimizing i/o writes to and from gpu vram the goal is to do chunked attention QV muls in the cache close to the gpu make multiple passes over the output, each time updating by adjusting softmax denom and adding new term obv. keep running log of the softmax denom and have to re-read on every block prefetch/kernel fuse as desired [x] olmo training curriculum with general text/fineweb-edu, then text for specific tasks to push benchmark scores stability tricks: Z-score regularization, gradient clipping, KV-norm, RMSnorm, no biases, model soups, careful model init, low lr (1e-8), post-norm, weight decay, dropout what i learned from william brandon: nvidia gpus have two caches, L2 and L1 each streaming multiprocessor has its own L1 physically colocated with the SM in the time it takes to send data between L1 and SM, can do 100 tensor operations four datatypes to be aware of: fp32, horrible; tf32, better than bf16 but takes more memory fp16, really not great; bf16, what everyone uses bf16 (if not careful) will get converted into fp32 which is horrible V100s are horrible for the above reason also talked about modular duality, lipschitz stuff from jeremy + laker newhouse, new optimizers life advice: do stuff, make stuff, remake primitives from scratch, when possible if i'm scared of something just do it also learn cuda in 100hrs in 6.S894 labs (use 1xRTX A4000) [ ] ray python dist package [ ] deepspeed [ ] megatron [ ] jetformer [ ] ddp, fsdp, megatron, gpipe [ ] what to knwo about cpu memory [ ] flexattention [x] streamingllm, linformer linformer just projects $n\\times d$ key, weight matrices into $k\\times d$ using direct linear $n\\to k$ linear map (\"squishing\" vectors to make the seq shorter) streamingllm analyzed attention magnitudes, found that \"leftover\" attention would be dumped into \"attention sinks\" in the softmax which would then be ignored added initial tokens back to inference, perplexity fixed (compared to sliding window) [ ] speculative decoding [ ] yarn uses RoPE (instead of adding pos. embedding, multiply by $D/2$ complex numbers with AP magnitude) pretrain on 10K context/RoPE, then interpolate arguments of complex numbers in pos. embedding carefully preserve higher frequency numbers and move lower [ ] lolcats [ ] deepseek v3 MTP, Yarn, \"long-winded and overly reflective\" R1 with system prompt during SFT for transfer, carefully load balanced MoE [x] hymba combine SSM and transformer side by side as opposed to in sequence, has good \"synergy\" because of opposing behaviors Transformers arch with LN, LinUp, split SSM/Attn, LinDown, (+), LN, FFN, (+) (N-3)/2 SWA Hymba blocks on each side of global attention","title":"sys/inference optimization, large text models"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#interp","text":"[ ] automated circuit discovery [ ] path patching [ ] the logit lens (blogpost) [ ] nnsight docs [x] achyuta's sts paper uses cross-layer-attribution matrix based on gradient of next layer on each neuron in current take top $k$ in each layer locally, then iteratively refine by taking last layer, grab most influential neurons from prev, etc. go backwards, then go forwards, until circuit stops changing stich images from two classes into a single image, analyze behavior of both circuits under this hybrid image FROM TAMAR [x] https://anishk23733.github.io/vl-interp/ VLM embeds images and places before text in transformer decoder hence image embeddings are in the same \"space\" as text embeddings goal: measure \"internal confidence\" about an object in an image, potentially ablate defines \"internal confidence\" about an object in an image as maximum of logit lens (result of applying final unembedding matrix and grabbing logit for token for that object) over all image embeddings (one for each patch) over all layers in the VLM cutoff at a certain internal confidence, and then ablate/activate based on that questions: why are all layers in the model treated equally? can we learn a better combination of information to get internal confidence in a self-supervised way? what about image encoders that don't keep spatial information in an easily interpretable way, how to check segmentation for these? logit lens seems \"naive\"\u2014why do we expect image tokens to speak \"exactly\" the \"same language\"? should we try to learn adjustments to the embedding matrix, esp. wrt. position? good results for end task\u2014hallucination reduction\u2014but gap between hallucation reduction v.s. cd reduction seems small, perhaps can be made more effective with ablation engineering or internal confidence engineering [x] https://vision-of-vlm.github.io/ \"the mechanisms underlying how VLMs process visual information remain largely unexplored.\" through a creative \"knockout\" experiment, demonstrate implicit image information through text tokens is sufficient, alternatively image tokens only needed for layers 20-40 in layers 20-40 attention values make an attempt at segmentation able to get \"compressed context\" by, for each layer, take only the image tokens with top-K attention values notably works much better with InternVL2-76B (patches into 40x40) than with LLaVa-1.5-7B (patches in 16x16), suggesting that large patching is just very redundant/inefficient and should no longer be pursued question: if we take the best tokens for each layer then perhaps over all layers we end up including all image tokens at least once? even worse, if we change which image tokens are included on inference for each text token, perhaps over all text tokens, each layer eventually sees all image tokens? also perhaps these results are to be \"expected\" because of dropout training schedules? [x] https://arxiv.org/pdf/2410.07149 adapter VLM that maps CLIP patch embeddings into language token space refers to a register token hypothesis\u2014that blank background patches collect global information; relegates as a side effect uses three methods of checking token understanding, finds that ablating object tokens (replacing with global mean) removes understanding\u2014expected; how to know there's a dog when there are no dog patches? uses logit lens, finds that tokens become aligned with corresponding text embeddings as layers progress (surprising! no compulsion to align with text, some implicit discovery of \"best coding\" for each patch) most valuable communication between image and text tokens happens in later layers (attention knockout) i'm surprised they discard register hypothesis so quickly, i believe register [x] https://arxiv.org/abs/2310.10348 general program of ACDC: Automated Circuit DisCovery activation patching does one forward pass for every edge/node/wtv. that has to be tweaked, replacing activation at that node with either than average ablation or the activation from some other \"corrupted\" prompt, sees what the change in metric is attribution patching uses derivative of metric in activation at each node/edge to construct a circuit represent network as a computational graph, differentiate between modules, nodes, edges specifically use EAP, edge attribution patching measure false positive rate/true positive rate from human expert baseline, both increase as %pruning increases two approaches to calculating relevance: take a gradient all the way to the top, or take gradient of next layer(s) from this layer and compute recursively recursive is easier to ensure selected subgraph is actually connected [ ] A Mathematical Framework for Transformer Circuits","title":"interp"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#rl-agentsautomation","text":"[ ] sakana [ ] diffusion policy use diffusion to denoise action latents that guide some agent in rl for whatever reason diffusion is rlly good lol idk","title":"rl agents/automation"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#roboticsrl","text":"[ ] kinetix rl physics lib [ ] aloha robotics [ ] openvla [x] ppo want your policy to change but not by too much use advantage weighted log likelihood updates, but clip the upside to some $\\epsilon$ makes it a lot more stable [ ] grpo/deepseekmath data mix using iterative pipeline word n-gram neural model, averages all n-gram embeddings and mlp classification collect dataset, do grpo on DeepSeekCoder unified rl perspective: all algos can be described in terms of space parameterization, reward signal parametrization, policy gradient coefficient, GRPO just uses rule [ ] diffusion for robotics [x] pi's new paper (read ??) [x] action latents [ ] sergey levine from berkeley has takes [ ] some yilun du papers: original EBM , his thesis , his research statement","title":"robotics/rl"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#diffusion","text":"[ ] diffusion amth [ ] figure out diffusion/normalizing flow/flow forcing, fr, https://boyuan.space/diffusion-forcing/ [ ] flow matching math [ ] moviegen [ ] dall-e algorithm [ ] papers from william brandon","title":"diffusion"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#sci-of-dl","text":"[ ] scaling via data manifolds [x] low rank can update matrix weights using low rank factorizations of matrices [x] modular duality by bernstein two angles for motivation: gradients are not same type as weights, they are linear functionals on the weights i.e. $\\Delta \\mathcal{L}=g^{T}w.$ Hence we should update by the result of some \"duality map\" applied to the gradient. In the gradient case where we want to optimize $g^{\\dagger}\\Delta w + \\lVert \\Delta w \\rVert^{2}$ we want $\\Delta w$ to be in the direction of the dualizer. input/outputs are normed, we want to update weights so that activations stay in the same approximate norm. Specifically we set weight norms to be dual norms on input/output using their respective norms, i.e. $\\lVert W \\rVert=\\max \\frac{{\\lVert W x_{in} \\rVert_{out}}}{\\lVert x_{in} \\rVert_{in}}.$ Generalizing, we want to maintain smoothness of our layers for easy future gradient descent. Given the gradient, want to update in the direction that gives the most \"bang for buck\" accounting for some norm on the weights. Above maximum can be interpreted as curvature, i.e. fastest ROC of output given fixed input change. Dualizer is the argmax of this norm. They arrive at several duality maps for different modules using specific norms, e.g. RMS for linear, max(RMS) over channels for convolution, $l_{1}\\to RMS$ for embedding. Using the above max expression, get spectral nom for Linear, max spectral norm for Conv2D, some other weird stuff idk lmao Formalize modules into a fwd, mass, sensitivity (curvature), and norm for the weights (defined by norms over input/output).","title":"sci of dl"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#ssm","text":"[ ] albert gu's thesis [ ] mamba2: ssm > transformers [ ] mamba is attentino","title":"ssm"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#audioimagevideo-specific","text":"in audio \"conv subsampling\" is just decreasing spatial resolution with convs [x] conformer FFM, MHA, Conv, FFM Conv is actually kinda MLP, with linear up channel (pointwise), activation, norm, conv, activatoin, linear down [x] moondream uses image patches with crops of fixed sizes, vit, recombine crops to get image embedding image adapter, prompt prefix, text tokens [ ] pixtral (vlm) [x] Stable Diffusion audio codec (400-700 bps!!) patch ViT style, 300 ish blocks of strided conv followed by lots of transformers (sliding attention, 128 tokens) fully scalar quantization\u2014bottleneck into even lower dim, and then quantize each dim -> one token either learn N scalars to snap to, or parametrize (they use floor tanh) quantizer-dropout occasionally [x] Hiera develops a new fully-transformer based vit encoder/decoder for classification/segmentation tasks on image/video on each downsize, instead of using strided conv, use Q-pooled attention, similar to cross attention but Q terms get pooled via maxpool masked loss, mask tokens don't get fed into the encoder only the decoder (they get \"filled in\") use local window attention at lower resolutions, then global later on [x] Nvidia Cosmos 2 AE and generative start with Haar 3D wavelet in time/space to downsample spatially alternating factorized resblock and factorized attention with downsampling ends at latent dimension 16 for continuous (diffusion) and 6 for discrete (autoregressive) every video token is predicted autoregressively after flattening in sequence, use finite scalar quantization to quantize to total 64K vocab size because attention is used on \"every layer\" context length is only 34 frames-quite sad lmao use 8x16x16 compression for autoregressive and 8x8x8 for diffusion autoregressive: hybrid 3D RoPE with absolute embeddings per head for each token, cross attention from text prompt (NOT concat, think abt why) diffusion: typical DiT, patchify using 1x2x2 and project/flatten, then text CA plus adaLN from time like suno, best is to use autoregressive to generate discrete tokens, then use \"diffusion decoder\"\u2014condition on autoregressive tokens and regenerate continuous latents using diffusion [x] MAE Make AE modeling a masked task by masking some of the patches feed into encoder (with pos embeddings) without mask tokens, feed into decoder with mask tokens, allows huge speedup downstream ViT task still great, AE learns to encode global \"semantic properties\" cuz of masking task [x] meta visual tokenizers opposing the conv/attention interleave strategy used by cosmos/SD, try full send single conv with 8x16x16 stride=kernel (aka patching) into ~1k feature dim, then full ViT with 3D RoPE/SwiGLU, then bottleneck works well, however performance does not scale with encoder/decoder size (why???) [x] original dit u-nets are great but let's try transformers instead standard latent diffusion, break latent into $p\\times p$ patches (use $p=2$) classifier-free guidance, i.e. constraining $p(c|x)$ implicitly by using $\\log p(x | c) - \\log p(x|\\emptyset)$ since $p(c|x)=\\frac{{p(x|c)p(c)}}{p(x)}$, then setting $\\mathcal{L}=p(x)+\\gamma p^*(c|x)$ for $\\gamma>1$ various conditioning strategies including cross-attention adaptive layernorm with \"zero scaling\" (scaling right before residual by multiplying by MLP output $\\alpha$ initialized to 0) straight concat Transformer decoder with just a linear and rearrange to get predicted $\\epsilon$ and diagonal $\\Sigma.$ [ ] original vit [x] sana huge compression vae, with F32C32P1 instead of standard F8C8P2, leads to 4x fewer tokens (1/16 \"patches\", 4x channels, 4x patches) linear attention using $ReLU(Q)ReLU(K)^{T}$ instead of softmax, then instead of computing $n\\times n$ matrix can do $ReLU(K)^{T}V$ first to get $d\\times d$ matrix for linear time full sequence training inference is $O(1)$\u2014additional K,V vectors add to the new $d\\times d$ matrix, then mv-mul with new Q vector add nonlinearity with 1x1 convs along channel and 3x3 conv along channel, token index and GLU no positional encoding! rely on 3x3 conv for implicit positional info bit quantization, triton cuda shit, for faster inference [x] some Flow-DPM math \u23eb \ud83d\udcc5 2025-01-31 \u2705 2025-02-01 [x] wav2vec architecture: 6-deep convnet for latent representation (stride 20ms, receptive field 25ms at 16k hz), then quantize using product codebook w linear projection (gumbel softmax to get $V\\times G$ logits for codebook), causal transformer on latents to get context representation train by masking some latents with learned mask token, cosine similarity for cross-entropy loss between \"real\" masked quantized latent and several distractors mask $\\approx$ half of latents intuition: can reconstruct missing latent (when quantized) i.e. latents contain useful info (note no reconstruction!) quantization isn't used for one-hot encodings autoregressive-style, rather is used to \"regularize\" the latents with some amount of \"smoothness\" fine-tune final contextual representations into text token classes with a linear layer (includes blank token) uses Connectionist Temporal Classification loss i.e. takes \"best possible\" alignment allowing for repetitions and blank tokens, takes log prob [ ] seq-2-seq (in comparison to wave2vec) [ ] non-spiky ctc loss basically, it's easy to just say blank by default, want to keep blank use priors that prefer unigram i.e. non-blank so that probability for seeing given character is divided by prior probability to incentivize picking unigrams more often [x] ctc loss CTC probability is sum over all alignments of getting the desired output, because this is subset of all possible labellings of all things this is guaranteed $\\leq 1$ use DP to compute sum of probabilities of all alignments ending at given target sequence index, for given frame introduce blank token and merging adjacent equal tokens to get \"skips\" [ ] HuBERT cnn encoder ofc (it's 2021) idea: masked loss training on quantized targets but what are the targets? a: really degenerate bootstrapped middle activation start with random quantization with some linear map do the best you can, then use middle layer of transformer as representation, k-means cluster try again [ ] tracks-to-4d [ ] mqvae","title":"audio/image/video specific"},{"location":"Material%20Knowledge/Sciences/ML/ML%20papers%20to%20read/#miscell","text":"[ ] DPO [ ] transfer learning survey [ ] shampoo [ ] decision transformer [x] dqvae [ ] vqgan [x] vqvae [ ] deepseek prover [ ] little book of dl [x] BERT and original transformers attention (2017): add in positional encodings, also mask attention before softmax using causal, transformers solve text gen BERT: full self-attention with masks using masked language modeling (MLM) loss; cross-entropy on logits on masked tokens","title":"miscell"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/","text":"Useful properties of current game state There exists an infinite sequence of proof steps such that at any point in this sequence, all available moves for the current player are reversible. Black and White can play in such a way that at any point in the sequence, any available move is reversible. In particular, Black can choose to move their knight between a8, b6, d5 only and avoid breaking 3rep draw rule (see below). Under this setup, at any point in the sequence the only pieces Black could move are the king, either knight (unable to reach any White pieces if White plays mirror) and the rook, all of which are reversible moves. As a remark, we claim this is a necessary condition for such a sequence to exist. What follows for this remark is not entirely rigorous, but the main idea is here. Any proof game is valid for only finitely many prefixes of the sequence, otherwise the entire sequence is valid with that proof game. However, each of the infinitely many prefixes is valid with some proof game. Thus, for any $N$ there are infinitely many proof games that are valid with the first $N$ moves of the sequence. From any board state there are finitely many possible previous moves. Construct the tree whose root is the given board state, and where the children of each node are all possible previous board states, assuming the follow-up is the path to root and 3rep is not violated. Then for any $N$ there exist nodes of arbitrary depth such that the corresponding proof \"suffix\" plus the first $N$ moves of the sequence is valid. Suppose this condition is not satisfied, i.e. beyond some fixed depth $D$ in this tree we are forced to make an irreversible move $M$. If $M$ is irreversible, it cannot be replayed, i.e. it cannot occur again in the sequence. Hence $M$ serves as a \"separator\" between all preceding and succeeding moves; because $M$ cannot be replayed it cannot occur in a 3rep violation, so for any proof game and any sequence prefix, we can consider the moves before $M$ and after $M$ separately. Hence of the finitely many proof game suffixes with length $D,$ at least one of them is valid with infinitely many sequence prefixes, i.e. is valid with the entire sequence. Since we are guaranteed to have an irreversible move after length $D$ suffix, all \"subsequent\" moves (moving backwards in time) are irrelevant, so (almost) any finite progression from initial board state to $-D$ state will work with the entire sequence, which is a contradiction. Setup and intuition We construct an infinite \"forced history\" $P$ using a binary sequence that satisfies the 3rep rule, constructed below. We then construct the post-sequence $Q$ recursively. Denote the $i$th move before the current game state as $-i$. When processing the $i$th step of $P$ (move $-i$), we append to $Q$ in such a way that making any other move at time $-i$ would result in a 3rep violation, but the intended move is ok. Because the forced history never goes to the initial game state, any proof game must \"hop onto\" the forced history, and by construction there is a 3rep violation starting from that timestep (since moving backwards it is \"stepping off of\" the forced history). However, for any finite prefix of $Q$, some finite suffix $[-N,0]$ of the forced history $P$ is valid with that prefix, so we only need to make some fixed progression of moves to get to timestep $-N$ to generate a valid proof game. Bit sequence construction First we recursively construct an infinite sequence of bits that is \"fine\" under the 3rep draw rule. Start with $A=0$ and $B=1$. Repeatedly replace $$ A,B:=AAB,ABB $$ so that the two values start at $0,1$ then become $001, 011$, then $001001011, 001011011$ etc. Note that at timestep $i$ the new value of $A$ contains the previous value of $A$ as its prefix. Hence we can define the limit of $A$ to be our infinite sequence. We claim no subsequence of this limiting sequence $S$ breaks the 3rep rule. Breaking the infinite sequence into chunks of length 3, the boundary of each chunk is the only place where $1$ becomes $0$. Hence given any slice with length at least $3$ there exists such a boundary $10$, and hence we can uniquely determine the starting index of that slice modulo $3$. Suppose some subsequence $s$ breaks the 3rep rule. Since $s$ is finite, we can say it is a subset of $A_{t}$ for some $t.$ Clearly the subunit with length $\\frac{|s|}{3}$ cannot have length 1, and by inspection cannot have length 2. Suppose it has length at least 3. Then we claim the three slices must have the same starting indices modulo $3,$ i.e. we have $3\\mid s.$ Suppose the starting index is not divisible by $3,$ i.e. it doesn't \"begin\" on a block. Note that since the slice has length divisible by $3,$ it must cover one chunk on one side by 2 (wlog right side), and one chunk on the other side by 1. Note that knowing the middle number in any chunk suffices to inform us what the other digits in that chunk are, since first digit of every chunk is $0$, and last digit of every chunk is $1$. Hence if we simply shift all three slices one index to the right, they will still all be equal, but they will also be aligned with the chunks. Now replace every occurrence of $001$ with $0$ and every occurrence of $011$ with $1$ (equivalently remove all occurrences of $10$). Then the slices decrease in size by $3,$ and they become slices in $A_{t-1}$ that break the 3-rule. Induction immediately finishes to prove our claim. Translating binary sequence (bits) into forced history (chess) Here we define the sequence $P$ (imagine moving backwards in time, from current game state). The only piece that moves will be the knight at a8 , and it only ever travels between a8 , b6 , and d5 . First move to b6 . Then follow the infinite sequence constructed above. A $0$ corresponds to b6->a8->b6 and a $1$ corresponds to b6->d5->b6 . Similarly to above, if there are any 3rep violations we can easily align them to these 2-move chunks, and then translate back into binary-space, and arrive at a contradiction. Make mirror moves for White using the knight at h1 . We have hence arrived at a construction for a \"forced history\" $P$ that doesn't violate 3rep. Using forced history to generate sequence Now we have a target forced history $P$. Now the goal is to design an algorithm yielding a sequence $Q$, along with \"timestamps\" $r_{i},$ such that the prefix of timestamps $[0,r_{i}]$ of $Q$ permits the suffix $[-i, 0]$ of $P,$ but permits no other suffixes of length $i$. At any timestep $-(i-1)$ the only diff from the current game state is the position of the knights on a8 and h1 . The a8 knight can be on b6 or d5 and the h1 knight can be on g3 or e4 . We wish to prohibit all moves at timestep $-i$ other than the one we have prescribed. Either: the rook that is free to move takes its one possible action the king whose turn it is to move takes its one possible action the other knight who has never moved takes its one possible action We prohibit these in turn. Let the sequence developed so far be $s,$ defined by all actions in $[-(i-1),r_{i-1}].$ For each of the above three possible actions: let the action be $A$ (in the \"forward\" time direction, e.g. Raa7 .) Pick another \"irrelevant\" reversible action. Either the \"other knight\" action or the king action is always available, call that $B$. let $s'$ be the sequence of moves formed by$$ ss ^{-1}A ^{-1}Ass ^{-1}A ^{-1}Ass ^{-1}A ^{-1}BB ^{-1}A. $$ Note that $s'$ still has $s$ as a prefix, so take $s'[len(s):]$ and append that to $Q.$ Let the other color mirror. Intuition: note that prepending the action $A$ creates a 3rep of the subsequence $Ass ^{-1}A,$ but because of the $BB ^{-1}$ at the end there is no violation of 3rep using only the subsequence (at first inspection, more details later). Do this for every timestamp $i$, for both Black and White, for every \"off-script past action\" available to the current player. This generates an infinite sequence $Q$, and it is apparent from construction that there are no proof games. As argued earlier, any proof game must come \"onto\" the forced history, and at that point would create a 3rep violation. Proving correctness It remains to show that a finite proof game exists for any finite prefix of $Q$. Equivalently we show that the bidirectional infinite sequence $P$ followed by $Q$ contains no 3rep violations. If this holds, then for any prefix $Q$ of length $N$ we take a $P$-suffix of length approximately $3N$ so that then move a rook and move a pawn.","title":"Elliot Glazer's chess problem"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#useful-properties-of-current-game-state","text":"There exists an infinite sequence of proof steps such that at any point in this sequence, all available moves for the current player are reversible. Black and White can play in such a way that at any point in the sequence, any available move is reversible. In particular, Black can choose to move their knight between a8, b6, d5 only and avoid breaking 3rep draw rule (see below). Under this setup, at any point in the sequence the only pieces Black could move are the king, either knight (unable to reach any White pieces if White plays mirror) and the rook, all of which are reversible moves. As a remark, we claim this is a necessary condition for such a sequence to exist. What follows for this remark is not entirely rigorous, but the main idea is here. Any proof game is valid for only finitely many prefixes of the sequence, otherwise the entire sequence is valid with that proof game. However, each of the infinitely many prefixes is valid with some proof game. Thus, for any $N$ there are infinitely many proof games that are valid with the first $N$ moves of the sequence. From any board state there are finitely many possible previous moves. Construct the tree whose root is the given board state, and where the children of each node are all possible previous board states, assuming the follow-up is the path to root and 3rep is not violated. Then for any $N$ there exist nodes of arbitrary depth such that the corresponding proof \"suffix\" plus the first $N$ moves of the sequence is valid. Suppose this condition is not satisfied, i.e. beyond some fixed depth $D$ in this tree we are forced to make an irreversible move $M$. If $M$ is irreversible, it cannot be replayed, i.e. it cannot occur again in the sequence. Hence $M$ serves as a \"separator\" between all preceding and succeeding moves; because $M$ cannot be replayed it cannot occur in a 3rep violation, so for any proof game and any sequence prefix, we can consider the moves before $M$ and after $M$ separately. Hence of the finitely many proof game suffixes with length $D,$ at least one of them is valid with infinitely many sequence prefixes, i.e. is valid with the entire sequence. Since we are guaranteed to have an irreversible move after length $D$ suffix, all \"subsequent\" moves (moving backwards in time) are irrelevant, so (almost) any finite progression from initial board state to $-D$ state will work with the entire sequence, which is a contradiction.","title":"Useful properties of current game state"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#setup-and-intuition","text":"We construct an infinite \"forced history\" $P$ using a binary sequence that satisfies the 3rep rule, constructed below. We then construct the post-sequence $Q$ recursively. Denote the $i$th move before the current game state as $-i$. When processing the $i$th step of $P$ (move $-i$), we append to $Q$ in such a way that making any other move at time $-i$ would result in a 3rep violation, but the intended move is ok. Because the forced history never goes to the initial game state, any proof game must \"hop onto\" the forced history, and by construction there is a 3rep violation starting from that timestep (since moving backwards it is \"stepping off of\" the forced history). However, for any finite prefix of $Q$, some finite suffix $[-N,0]$ of the forced history $P$ is valid with that prefix, so we only need to make some fixed progression of moves to get to timestep $-N$ to generate a valid proof game.","title":"Setup and intuition"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#bit-sequence-construction","text":"First we recursively construct an infinite sequence of bits that is \"fine\" under the 3rep draw rule. Start with $A=0$ and $B=1$. Repeatedly replace $$ A,B:=AAB,ABB $$ so that the two values start at $0,1$ then become $001, 011$, then $001001011, 001011011$ etc. Note that at timestep $i$ the new value of $A$ contains the previous value of $A$ as its prefix. Hence we can define the limit of $A$ to be our infinite sequence. We claim no subsequence of this limiting sequence $S$ breaks the 3rep rule. Breaking the infinite sequence into chunks of length 3, the boundary of each chunk is the only place where $1$ becomes $0$. Hence given any slice with length at least $3$ there exists such a boundary $10$, and hence we can uniquely determine the starting index of that slice modulo $3$. Suppose some subsequence $s$ breaks the 3rep rule. Since $s$ is finite, we can say it is a subset of $A_{t}$ for some $t.$ Clearly the subunit with length $\\frac{|s|}{3}$ cannot have length 1, and by inspection cannot have length 2. Suppose it has length at least 3. Then we claim the three slices must have the same starting indices modulo $3,$ i.e. we have $3\\mid s.$ Suppose the starting index is not divisible by $3,$ i.e. it doesn't \"begin\" on a block. Note that since the slice has length divisible by $3,$ it must cover one chunk on one side by 2 (wlog right side), and one chunk on the other side by 1. Note that knowing the middle number in any chunk suffices to inform us what the other digits in that chunk are, since first digit of every chunk is $0$, and last digit of every chunk is $1$. Hence if we simply shift all three slices one index to the right, they will still all be equal, but they will also be aligned with the chunks. Now replace every occurrence of $001$ with $0$ and every occurrence of $011$ with $1$ (equivalently remove all occurrences of $10$). Then the slices decrease in size by $3,$ and they become slices in $A_{t-1}$ that break the 3-rule. Induction immediately finishes to prove our claim.","title":"Bit sequence construction"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#translating-binary-sequence-bits-into-forced-history-chess","text":"Here we define the sequence $P$ (imagine moving backwards in time, from current game state). The only piece that moves will be the knight at a8 , and it only ever travels between a8 , b6 , and d5 . First move to b6 . Then follow the infinite sequence constructed above. A $0$ corresponds to b6->a8->b6 and a $1$ corresponds to b6->d5->b6 . Similarly to above, if there are any 3rep violations we can easily align them to these 2-move chunks, and then translate back into binary-space, and arrive at a contradiction. Make mirror moves for White using the knight at h1 . We have hence arrived at a construction for a \"forced history\" $P$ that doesn't violate 3rep.","title":"Translating binary sequence (bits) into forced history (chess)"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#using-forced-history-to-generate-sequence","text":"Now we have a target forced history $P$. Now the goal is to design an algorithm yielding a sequence $Q$, along with \"timestamps\" $r_{i},$ such that the prefix of timestamps $[0,r_{i}]$ of $Q$ permits the suffix $[-i, 0]$ of $P,$ but permits no other suffixes of length $i$. At any timestep $-(i-1)$ the only diff from the current game state is the position of the knights on a8 and h1 . The a8 knight can be on b6 or d5 and the h1 knight can be on g3 or e4 . We wish to prohibit all moves at timestep $-i$ other than the one we have prescribed. Either: the rook that is free to move takes its one possible action the king whose turn it is to move takes its one possible action the other knight who has never moved takes its one possible action We prohibit these in turn. Let the sequence developed so far be $s,$ defined by all actions in $[-(i-1),r_{i-1}].$ For each of the above three possible actions: let the action be $A$ (in the \"forward\" time direction, e.g. Raa7 .) Pick another \"irrelevant\" reversible action. Either the \"other knight\" action or the king action is always available, call that $B$. let $s'$ be the sequence of moves formed by$$ ss ^{-1}A ^{-1}Ass ^{-1}A ^{-1}Ass ^{-1}A ^{-1}BB ^{-1}A. $$ Note that $s'$ still has $s$ as a prefix, so take $s'[len(s):]$ and append that to $Q.$ Let the other color mirror. Intuition: note that prepending the action $A$ creates a 3rep of the subsequence $Ass ^{-1}A,$ but because of the $BB ^{-1}$ at the end there is no violation of 3rep using only the subsequence (at first inspection, more details later). Do this for every timestamp $i$, for both Black and White, for every \"off-script past action\" available to the current player. This generates an infinite sequence $Q$, and it is apparent from construction that there are no proof games. As argued earlier, any proof game must come \"onto\" the forced history, and at that point would create a 3rep violation.","title":"Using forced history to generate sequence"},{"location":"Material%20Knowledge/Sciences/ML/random%20shit/Elliot%20Glazer%27s%20chess%20problem/#proving-correctness","text":"It remains to show that a finite proof game exists for any finite prefix of $Q$. Equivalently we show that the bidirectional infinite sequence $P$ followed by $Q$ contains no 3rep violations. If this holds, then for any prefix $Q$ of length $N$ we take a $P$-suffix of length approximately $3N$ so that then move a rook and move a pawn.","title":"Proving correctness"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/","text":"Formula sheet is here . Oscillations on a Spring-like Material\u2014Wave Equation Key example: Take a 2D rectangular mesh of beads on strings, oscillating laterally. We drop the BCs and extend to infinite plane\u2014the translation is up/down/left/right so we can just do $z(x,y)=e^{i(k_{1}x+k_{2}y)}$ and match up the coefficients. The equation looks like $$\\ddot{{z(t)}}=\\frac{T}{L}\\nabla^{2}z(x,y).$$ The $\\nabla^{2}$ is the continuous version of a $\\begin{bmatrix}1&-2&1\\end{bmatrix}$ matrix $K$ representing forces due to neighbors being attached with springs oscillating longitudinally. In this case we get a (straightforward) $M^{-1}K \\frac{\\partial^{2}x}{\\partial x^{2}}\\vec{z}=\\dot{z}\\dot{},$ with normal modes being represented by $e^{-i\\omega t}v$ with $v$ being eigenvector of $M^{-1}K$ with eigenvalue $\\omega^{2}$. Next we have incoming waves, crossing a border and creating a transmitted and reflected wave. Based on tension on the string on the other side/refraction index on the other side/wtv, the wavenumber $k_{2}$ on the other side will be different but reflection wavenumber will be $-k$. Then we match boundary conditions on the border: - fixed point: value at 0 is fixed at whatever - moving loop on rod: values are equal, derivatives are equal - moving weight: values are equal, delta in derivatives provides force matching $f''(0)$ And solve for the coefficients on the waves we care about. Travelling Waves, Wavepackets Earlier we just had standing waves as normal modes. With sum-to-product or vice versa we can write standing waves as a sum of travelling waves in opposite directions, or travelling waves as standing in opposite phase. Wavepackets are defined by wavenumber distribution $\\Phi(k)$ and dispersion relation $v_{p}=v(k)$. Then the wavepacket is defined by $$ \\int \\Phi(k)e^{i(kx-v(k)kt)}\\,dk. $$ If we assume $\\Phi$ has a peak at $k_{0}$ then we require $kx-v(k)t$ to be \"stationary\" near $k_{0}$ i.e. $\\frac{dx}{dt}=\\frac{d\\omega}{dk}$. We thus have a distinction between the phase velocity at $k$ and the group velocity at $k$. Now we have Fourier analysis . We have to be careful about the bounds. We do this when we have a moving pattern on a string. Depending on the medium we have a dispersion relation $k(\\omega)=\\frac{\\omega}{v}$. Then after we Fourier on the original wave we find the velocities at each wavenumber and then reconstruct the wavepacket at a given time in the future. To avoid dispersion we invented a thing called \"AM Radio\", meaning if the signal we want to send is $f_{s}$ we actually send $f_{s}\\cos(\\Omega t)$ for large $\\Omega$, so that the shape now becomes a very pointy trig function with the peaks tracing out the previous values of $f_{s}$. \"Uncertainty Principle\": if we send a Gaussian wave, the Fourier transform of wavenumber distribution is also a Gaussian with width inverse to that of the original Gaussian. EM Waves and Optics Wave Equation The wave equations are all in the formula sheet, but can be rederived by plugging Maxwell's to get $\\left( \\nabla^{2}-\\frac{1}{c^{2}}\\right)\\vec{E}=\\left( \\nabla^{2}-\\frac{1}{c^{2}}\\right)\\vec{B}=0.$ Also $\\vec{E}=c \\vec{B}\\times k, \\vec{B}=\\frac{1}{c}\\hat{k}\\times \\vec{E}$ for the direction of the wave. Polarization This is all 2D EM, as mentioned in 8.02. $\\vec{E}$ oscillates in one direction, and $\\vec{B}$ oscillates in the other direction perpendicular to both $\\vec{E}$ and $\\hat{k}$. However in 3D, we can compose two rays in the same direction, but oscillating in $\\hat{x}$ and $\\hat{y}$, and out of phase, meaning $$ \\vec{E}=\\left( \\hat{x}+i\\hat{y}\\right) e^{i(kz-\\omega t)}. $$ This generates a $\\vec{E}$ that spins in a circle. By varying the coefficients of $\\hat{x}$ and $\\hat{y}$ we can get different shapes, ellipses etc. We have polarizers which force EM waves to be linearly polarized in a given direction, taking the parallel component of $\\vec{E}$ and discarding the perpendicular component. Hence when two polarizers are chained, resulting $I\\alpha \\cos\\theta^{2}$. There's also quarter wave plates , with a FAST axis where $\\vec{E}$ in that direction goes through straight, and a perpendicular SLOW axis where $\\vec{E}$ in that direction is delayed by some phase $\\delta$. Energy Density Intensity $I \\alpha E^{2}$. Poynting is $\\vec{S}=\\frac{1}{\\mu_{0}}\\vec{E}\\times \\vec{B}$ (to get the $\\hat{k}$ vector for direction of propagation). This is all on the formula sheet. Reflection Don't forget index of refraction, Snell's Law, speed laws, i.e. $n_{1}\\sin\\theta_{1}=n_{2}\\sin\\theta_{2}$ and $n_{1}v_{1}=n_{2}v_{2}$. Because $n_{i}$ determines speed, it has a similar function to tension $T$ in different string regimes: when light passes between different materials with different $n_{i}$ the frequency stays the same to preserve BC but wavenumber changes. Interference and Diffraction Big idea behind interference is light from different correlated sources interfere with each other due to optical path difference. Thin Slit Just light coming from thin slits, i.e. examining wave property of light through small holes on a wall behind. Through the small slits light emanates spherically (Hyugen's). For a given destination $D$ on the wall behind we assume its distance is large relative to the size of the hole, i.e. angle from any point inside the hole to $D$ is fixed. Then we just integrate the phase \"difference\". For discrete point-shaped slits the phase diff is $\\delta=\\frac{{d\\sin\\theta}}{\\lambda}2\\pi,$ so we get $1+e^{i\\delta}+e^{2i\\delta}+\\dots$ which somehow becomes $\\frac{\\sin(\\beta)}{\\beta}$ for some reason. For 2D slits it's a 2D Fourier transform because we care about the \"convolution\" with $e^{i(k\\cdot x)}$ which we can get from the Fourier transform. The final type of interference is thin-film interference caused by two material boundary layers: reflection off the first layer and then the second layer. Hence peak is when $2h=n\\lambda$ and trough (0) is when $2h=\\left( n+\\frac{1}{2} \\right)\\lambda$.","title":"8.03 ASE notes"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#oscillations-on-a-spring-like-materialwave-equation","text":"Key example: Take a 2D rectangular mesh of beads on strings, oscillating laterally. We drop the BCs and extend to infinite plane\u2014the translation is up/down/left/right so we can just do $z(x,y)=e^{i(k_{1}x+k_{2}y)}$ and match up the coefficients. The equation looks like $$\\ddot{{z(t)}}=\\frac{T}{L}\\nabla^{2}z(x,y).$$ The $\\nabla^{2}$ is the continuous version of a $\\begin{bmatrix}1&-2&1\\end{bmatrix}$ matrix $K$ representing forces due to neighbors being attached with springs oscillating longitudinally. In this case we get a (straightforward) $M^{-1}K \\frac{\\partial^{2}x}{\\partial x^{2}}\\vec{z}=\\dot{z}\\dot{},$ with normal modes being represented by $e^{-i\\omega t}v$ with $v$ being eigenvector of $M^{-1}K$ with eigenvalue $\\omega^{2}$. Next we have incoming waves, crossing a border and creating a transmitted and reflected wave. Based on tension on the string on the other side/refraction index on the other side/wtv, the wavenumber $k_{2}$ on the other side will be different but reflection wavenumber will be $-k$. Then we match boundary conditions on the border: - fixed point: value at 0 is fixed at whatever - moving loop on rod: values are equal, derivatives are equal - moving weight: values are equal, delta in derivatives provides force matching $f''(0)$ And solve for the coefficients on the waves we care about.","title":"Oscillations on a Spring-like Material\u2014Wave Equation"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#travelling-waves-wavepackets","text":"Earlier we just had standing waves as normal modes. With sum-to-product or vice versa we can write standing waves as a sum of travelling waves in opposite directions, or travelling waves as standing in opposite phase. Wavepackets are defined by wavenumber distribution $\\Phi(k)$ and dispersion relation $v_{p}=v(k)$. Then the wavepacket is defined by $$ \\int \\Phi(k)e^{i(kx-v(k)kt)}\\,dk. $$ If we assume $\\Phi$ has a peak at $k_{0}$ then we require $kx-v(k)t$ to be \"stationary\" near $k_{0}$ i.e. $\\frac{dx}{dt}=\\frac{d\\omega}{dk}$. We thus have a distinction between the phase velocity at $k$ and the group velocity at $k$. Now we have Fourier analysis . We have to be careful about the bounds. We do this when we have a moving pattern on a string. Depending on the medium we have a dispersion relation $k(\\omega)=\\frac{\\omega}{v}$. Then after we Fourier on the original wave we find the velocities at each wavenumber and then reconstruct the wavepacket at a given time in the future. To avoid dispersion we invented a thing called \"AM Radio\", meaning if the signal we want to send is $f_{s}$ we actually send $f_{s}\\cos(\\Omega t)$ for large $\\Omega$, so that the shape now becomes a very pointy trig function with the peaks tracing out the previous values of $f_{s}$. \"Uncertainty Principle\": if we send a Gaussian wave, the Fourier transform of wavenumber distribution is also a Gaussian with width inverse to that of the original Gaussian.","title":"Travelling Waves, Wavepackets"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#em-waves-and-optics","text":"","title":"EM Waves and Optics"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#wave-equation","text":"The wave equations are all in the formula sheet, but can be rederived by plugging Maxwell's to get $\\left( \\nabla^{2}-\\frac{1}{c^{2}}\\right)\\vec{E}=\\left( \\nabla^{2}-\\frac{1}{c^{2}}\\right)\\vec{B}=0.$ Also $\\vec{E}=c \\vec{B}\\times k, \\vec{B}=\\frac{1}{c}\\hat{k}\\times \\vec{E}$ for the direction of the wave.","title":"Wave Equation"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#polarization","text":"This is all 2D EM, as mentioned in 8.02. $\\vec{E}$ oscillates in one direction, and $\\vec{B}$ oscillates in the other direction perpendicular to both $\\vec{E}$ and $\\hat{k}$. However in 3D, we can compose two rays in the same direction, but oscillating in $\\hat{x}$ and $\\hat{y}$, and out of phase, meaning $$ \\vec{E}=\\left( \\hat{x}+i\\hat{y}\\right) e^{i(kz-\\omega t)}. $$ This generates a $\\vec{E}$ that spins in a circle. By varying the coefficients of $\\hat{x}$ and $\\hat{y}$ we can get different shapes, ellipses etc. We have polarizers which force EM waves to be linearly polarized in a given direction, taking the parallel component of $\\vec{E}$ and discarding the perpendicular component. Hence when two polarizers are chained, resulting $I\\alpha \\cos\\theta^{2}$. There's also quarter wave plates , with a FAST axis where $\\vec{E}$ in that direction goes through straight, and a perpendicular SLOW axis where $\\vec{E}$ in that direction is delayed by some phase $\\delta$.","title":"Polarization"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#energy-density","text":"Intensity $I \\alpha E^{2}$. Poynting is $\\vec{S}=\\frac{1}{\\mu_{0}}\\vec{E}\\times \\vec{B}$ (to get the $\\hat{k}$ vector for direction of propagation). This is all on the formula sheet.","title":"Energy Density"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#reflection","text":"Don't forget index of refraction, Snell's Law, speed laws, i.e. $n_{1}\\sin\\theta_{1}=n_{2}\\sin\\theta_{2}$ and $n_{1}v_{1}=n_{2}v_{2}$. Because $n_{i}$ determines speed, it has a similar function to tension $T$ in different string regimes: when light passes between different materials with different $n_{i}$ the frequency stays the same to preserve BC but wavenumber changes.","title":"Reflection"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#interference-and-diffraction","text":"Big idea behind interference is light from different correlated sources interfere with each other due to optical path difference.","title":"Interference and Diffraction"},{"location":"Material%20Knowledge/Sciences/Physics/8.03%20ASE%20notes/#thin-slit","text":"Just light coming from thin slits, i.e. examining wave property of light through small holes on a wall behind. Through the small slits light emanates spherically (Hyugen's). For a given destination $D$ on the wall behind we assume its distance is large relative to the size of the hole, i.e. angle from any point inside the hole to $D$ is fixed. Then we just integrate the phase \"difference\". For discrete point-shaped slits the phase diff is $\\delta=\\frac{{d\\sin\\theta}}{\\lambda}2\\pi,$ so we get $1+e^{i\\delta}+e^{2i\\delta}+\\dots$ which somehow becomes $\\frac{\\sin(\\beta)}{\\beta}$ for some reason. For 2D slits it's a 2D Fourier transform because we care about the \"convolution\" with $e^{i(k\\cdot x)}$ which we can get from the Fourier transform. The final type of interference is thin-film interference caused by two material boundary layers: reflection off the first layer and then the second layer. Hence peak is when $2h=n\\lambda$ and trough (0) is when $2h=\\left( n+\\frac{1}{2} \\right)\\lambda$.","title":"Thin Slit"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Important%20Equations/","text":"[!important] GOOD TO REMEMBER $$E^2-p^2c^2=m^2c^4,\\qquad E=\\frac{p^2}{2m}.$$ $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda,$$ $$E_0=mc^2=h\\nu=h\\frac c\\lambda=\\hbar\\omega,\\text{ (Compton)}$$ $$p=\\hbar k,\\qquad\\lambda=\\frac{2\\pi}k=\\frac hp.\\text{ (de Broglie)}$$ $$\\Psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ $$\\hat p=\\frac{\\hbar}{i}\\frac{\\partial}{\\partial x},\\qquad \\hat E=\\frac{\\hat p^2}{2m}=i\\hbar\\frac{\\partial}{\\partial t}.$$ $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+V\\right)\\Psi.$$ $$J=\\frac{\\hbar}m\\text{Im}(\\Psi^*\\nabla\\Psi),\\qquad \\frac{\\partial \\rho}{\\partial t}+\\nabla\\cdot J=0.$$ $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ $$(\\psi,\\varphi)=\\int \\psi\\bar\\varphi,\\qquad (\\psi,\\hat A\\varphi)=(\\hat A^\\dagger\\psi,\\varphi).$$ $$ \\begin{align } f(x)=\\frac1{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{ikx}\\tilde f(k)\\,dk,\\qquad\\tilde f(k)=\\frac1{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{ikx}f(x)\\,dk \\end{align } $$ $$\\delta(x)=0 \\iff x\\neq 0,\\qquad\\int\\delta \\,dx=1$$ $$ \\begin{align } \\langle F\\rangle=\\mathbb E(F)=\\int \\Psi^2(x)F(x)\\,dx\\ \\Delta x=\\text{Var}(x)=\\sqrt{\\langle x^2\\rangle-\\langle x\\rangle^2}\\ [\\hat A,\\hat B]=\\hat A\\hat B-\\hat B\\hat A\\ \\hat p=-i\\hbar\\frac{\\partial}{\\partial x}\\ \\hat E \\,\\Psi(x,t)=i\\hbar\\frac\\partial{\\partial t}\\Psi(x,t)=\\frac1{2m}\\hat p^2+V(x)\\ (f|g) = \\int f^ g\\,dx \\end{align*} $$","title":"Important Equations"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/","text":"Fun with States States are $\\mathbb C$-vectors. Mach-Zehdner interferometer ![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood. E-V bombs Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation. Schrodinger's Photon Energy [!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$. Matter wave and Schrodinger's We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$. Manipulating States Interpretation of States $\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$. Probability Flow $J$ Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point. Hermitian Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$ Fourier In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum.","title":"8.04.A"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#fun-with-states","text":"States are $\\mathbb C$-vectors.","title":"Fun with States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#mach-zehdner-interferometer","text":"![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood.","title":"Mach-Zehdner interferometer"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#e-v-bombs","text":"Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation.","title":"E-V bombs"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#schrodingers","text":"","title":"Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#photon-energy","text":"[!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$.","title":"Photon Energy"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#matter-wave-and-schrodingers","text":"We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$.","title":"Matter wave and Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#manipulating-states","text":"","title":"Manipulating States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#interpretation-of-states","text":"$\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$.","title":"Interpretation of States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#probability-flow-j","text":"Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point.","title":"Probability Flow $J$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#hermitian","text":"Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$","title":"Hermitian"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.A/#fourier","text":"In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum.","title":"Fourier"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/","text":"Inner Product Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$ Expectation ROC $$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$ Problem Solving [!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$ Recall that if the potential is finite then the derivative of $\\Psi$ must be continuous, i.e. for square well etc. there is a BC for value equality and another BC for derivatives matching up. If $V=\\infty$ then we get $\\Psi=0$. 1D Problem Solving Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$ Free Particle on Circle On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$ Square Well(s) Now we have domain $\\mathbb{R}$ and experiment with $V(x)$. Infinite case $$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$. Finite case $$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states. Delta(s) Simple case $$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$. Simple Harmonic Oscillator Solving Setup $$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree. Factorization Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$","title":"8.04.B"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#inner-product","text":"Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$","title":"Inner Product"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#expectation-roc","text":"$$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$","title":"Expectation ROC"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#problem-solving","text":"[!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$ Recall that if the potential is finite then the derivative of $\\Psi$ must be continuous, i.e. for square well etc. there is a BC for value equality and another BC for derivatives matching up. If $V=\\infty$ then we get $\\Psi=0$.","title":"Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#1d-problem-solving","text":"Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$","title":"1D Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#free-particle-on-circle","text":"On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$","title":"Free Particle on Circle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#square-wells","text":"Now we have domain $\\mathbb{R}$ and experiment with $V(x)$.","title":"Square Well(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#infinite-case","text":"$$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$.","title":"Infinite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#finite-case","text":"$$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states.","title":"Finite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#deltas","text":"","title":"Delta(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#simple-case","text":"$$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$.","title":"Simple case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#simple-harmonic-oscillator","text":"","title":"Simple Harmonic Oscillator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#solving-setup","text":"$$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree.","title":"Solving Setup"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.B/#factorization","text":"Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$","title":"Factorization"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/","text":"Unidirectional transmission/scattering Set up exponential ansatz for each domain and solve BCs. Example: Step Potential Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying). Example: Finite square well, resonance Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well. Reflective half-domain scattering Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$. Note that the ansatz is $e^{i\\delta}\\sin(kx+\\delta)=\\frac{{e^{-ikx}+e^{2i\\delta+ikx}}}{2i},$ hence incoming is always $e^{-ikx}$ and outgoing is $e^{ikx}$ with time delay $\\delta$ in the form $e^{2i\\delta+ikx}$. Wavepacket $\\Delta t$ Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$ Levinson's Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states. Some weird experimentation/construction We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense. Weird magic If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance.","title":"8.04.C"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#unidirectional-transmissionscattering","text":"Set up exponential ansatz for each domain and solve BCs.","title":"Unidirectional transmission/scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#example-step-potential","text":"Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying).","title":"Example: Step Potential"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#example-finite-square-well-resonance","text":"Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well.","title":"Example: Finite square well, resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#reflective-half-domain-scattering","text":"Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$. Note that the ansatz is $e^{i\\delta}\\sin(kx+\\delta)=\\frac{{e^{-ikx}+e^{2i\\delta+ikx}}}{2i},$ hence incoming is always $e^{-ikx}$ and outgoing is $e^{ikx}$ with time delay $\\delta$ in the form $e^{2i\\delta+ikx}$.","title":"Reflective half-domain scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#wavepacket-delta-t","text":"Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$","title":"Wavepacket $\\Delta t$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#levinsons","text":"Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states.","title":"Levinson's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#some-weird-experimentationconstruction","text":"We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense.","title":"Some weird experimentation/construction"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.C/#weird-magic","text":"If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance.","title":"Weird magic"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/","text":"Hydrogen atom Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$ Angular momentum Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist. Radial ansatz Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"8.04.D"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#hydrogen-atom","text":"Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$","title":"Hydrogen atom"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#angular-momentum","text":"Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist.","title":"Angular momentum"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04.D/#radial-ansatz","text":"Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"Radial ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/","text":"Formula Sheet: https://ocw.mit.edu/courses/8-04-quantum-physics-i-spring-2016/47367602c26c4559e9c267f8694203bf_MIT8_04S16_FinalTest_2015.pdf Fun with States States are $\\mathbb C$-vectors. They can be vectors or wavefunctions. Mach-Zehdner interferometer ![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood. E-V bombs Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation. Schrodinger's Photon Energy [!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$. Matter wave and Schrodinger's We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$. Manipulating States Interpretation of States $\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$. Probability Flow $J$ Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point. Hermitian Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$ Fourier In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum. Inner Product Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$ Expectation ROC $$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$ Problem Solving [!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$ Recall that if the potential is finite then the derivative of $\\Psi$ must be continuous, i.e. for square well etc. there is a BC for value equality and another BC for derivatives matching up. If $V=\\infty$ then we get $\\Psi=0$. 1D Problem Solving Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$ Free Particle on Circle On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$ Square Well(s) Now we have domain $\\mathbb{R}$ and experiment with $V(x)$. Infinite case $$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$. Finite case $$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states. Delta(s) Simple case $$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$. Simple Harmonic Oscillator Solving Setup $$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree. Factorization Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$ Unidirectional transmission/scattering Set up exponential ansatz for each domain and solve BCs. Example: Step Potential Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying). Example: Finite square well, resonance Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well. Reflective half-domain scattering Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$. Note that the ansatz is $e^{i\\delta}\\sin(kx+\\delta)=\\frac{{e^{-ikx}+e^{2i\\delta+ikx}}}{2i},$ hence incoming is always $e^{-ikx}$ and outgoing is $e^{ikx}$ with time delay $\\delta$ in the form $e^{2i\\delta+ikx}$. Wavepacket $\\Delta t$ Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$ Levinson's Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states. Some weird experimentation/construction We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense. Weird magic If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance. Hydrogen atom Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$ Angular momentum Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist. Radial ansatz Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. We use the ansatz $R=\\frac{u(r)}{r}$ cuz it's cleaner. The cleaner equation is on the formula sheet. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"8.04"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#fun-with-states","text":"States are $\\mathbb C$-vectors. They can be vectors or wavefunctions.","title":"Fun with States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#mach-zehdner-interferometer","text":"![[Pasted image 20230907105403.png]] In this case, top/bottom is a state in $\\mathbb{C}^2$. Then suppose we take the matrices $$ \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&1\\1&-1 \\end{bmatrix},\\qquad \\frac{1}{\\sqrt{ 2 }} \\begin{bmatrix} 1&-1\\1&1 \\end{bmatrix}, $$ then their product will be $$ \\begin{bmatrix} 0&1\\1&0 \\end{bmatrix} $$ meaning that in this scenario, all the light ends up at detector 0. This probabilistic framework resolves the paradox of wave-particle duality and nondeterminism in this setup; a photon is 50/50 either way after the first splitter, but \"uncanny action\" (actually interference mathematically represented) makes it appear at detector 0 with 100% likelihood.","title":"Mach-Zehdner interferometer"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#e-v-bombs","text":"Suppose we have a bomb that either works or does not work. If it works, the detector blocks light and is activated when light hits it. If it does not work, light passes through the detector. Putting the bomb in the bottom bridge side allows us to detect it without setting it off; if it is live and we send a photon it is 50% detector 0/1, and independently 50% top bridge, i.e. 25% chance it goes to detector 1 without detonation.","title":"E-V bombs"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#schrodingers","text":"","title":"Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#photon-energy","text":"[!important] Photonic Physics $$ E=h \\nu=h \\frac{c}{\\lambda},\\qquad p=\\frac{h}{\\lambda}=\\frac{h\\nu}{c}. $$ By relativity $E^{2}-p^{2}c^{2}=m^{2}c^{4};$ for small $p$ we get $E-mc^{2}=\\frac{1}{2}p^{2}m$ as expected. Recall Lorentz factor $\\gamma=\\frac{1}{\\sqrt{ 1-\\frac{v^{2}}{c^{2}} }},$ and that $E=mc^{2}\\gamma, p=mv\\gamma$ compared to classical. The de Broglie wavelength is that of a photon with same momentum i.e. $\\lambda_{db}=\\frac{h}{p},$ whereas Compton wavelength is that with same energy i.e. $\\lambda_{c}=\\frac{hc}{E}=\\frac{h\\gamma}{mc}$. Compton scattering is when high-energy photons ionize electrons beyond free energy; new wavelength computed using $h\\nu_{f}=E_{f}=h\\nu-E_{p}$.","title":"Photon Energy"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#matter-wave-and-schrodingers","text":"We want our particles to be plane waves $e^{i(k\\cdot x+\\omega t)}$. Then wavelength $\\lambda=\\frac{2\\pi}{k}=\\frac{h}{p}$ and frequency $\\nu=\\frac{\\omega}{2\\pi}=\\frac{E}{h},$ so $k=\\frac{p}{\\hbar}$ and $\\omega =\\frac{E}{\\hbar }$. Then $$ \\Psi=e^{i(\\frac{px}{\\hbar }-\\frac{Et}{\\hbar})}. $$ Then $\\hat{p}\\Psi=\\frac{\\hbar \\partial \\Psi}{i\\partial x}$ and $\\hat E\\Psi=\\frac{i\\hbar\\partial \\Psi}{\\partial t}.$ Thus, [!important] Schrodinger Operators $$k=\\frac{p}{\\hbar},\\qquad\\omega =\\frac{E}{\\hbar }.$$ $$ \\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},\\qquad \\hat{E}=-\\frac{\\hbar}{i} \\frac{\\partial}{\\partial t}. $$ Then $E=KE+V=\\frac{p^{2}}{2m}+V$ yields $$\\frac{\\hat{p}^{2}}{2m}+V=\\hat{ E},$$ specifically [!important] Schrodinger's Equation $$i\\hbar \\frac{\\partial}{\\partial t}\\Psi=\\left( -\\frac{\\hbar^2}{2m} \\nabla ^2+V \\right) \\Psi.$$ RHS we call the Hamiltonian $\\hat{H}$.","title":"Matter wave and Schrodinger's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#manipulating-states","text":"","title":"Manipulating States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#interpretation-of-states","text":"$\\Psi$ is a norm-able vector determining distribution, i.e. $\\int \\Psi^{2} \\, dx=1$ and PDF of particle is $\\Psi^{2}.$ Thus $\\mathbb{E}(X)=\\int \\Psi^{2}X(x) \\, dx$. We can define variance in the usual way, $\\sigma_{Q}^{2}=\\mathbb{E}(Q^{2})-\\mathbb{E}(Q)^{2}$. Use Schrodinger's to verify that $\\mathcal N=\\int \\Psi^{2} \\, dx$ is conserved over $t$.","title":"Interpretation of States"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#probability-flow-j","text":"Define [!important] Probability Flow $$ J=\\frac{\\hbar}{m}\\text{Im}\\left( \\Psi^*\\nabla \\Psi \\right), $$ then $$ \\begin{align } \\frac{\\partial}{\\partial t}\\mathcal N &=\\int \\frac{\\Psi \\partial\\Psi^ }{\\partial t}+\\frac{\\Psi^ \\partial\\Psi}{\\partial t} \\, dx \\ &= \\int -\\nabla \\cdot J\\, dx \\ &=0. \\end{align } $$ In particular, $$\\frac{\\partial |\\Psi|^{2}}{\\partial t}+\\nabla \\cdot J=0.$$ Thus the probability flow $J$ is the ROC of cumulative prob distribution below that point.","title":"Probability Flow $J$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#hermitian","text":"Operator $\\hat{ A}$ is Hermitian if $$ \\int \\phi^ \\hat{A} \\psi \\, dx =\\int \\psi^ \\hat{A}\\phi \\, dx . $$ Clearly $\\hat{x}$ is Hermitian. $\\hat{p}$ is a little trickier: $$ \\begin{align} \\int \\frac{\\partial}{\\partial x}\\left( \\phi^ \\psi \\right) \\, dx &=0,\\ \\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=-\\int \\phi i\\frac{\\partial}{\\partial x}\\psi^ \\, dx,\\ \\int \\psi^ \\hat{p}\\phi \\, dx =\\frac{\\hbar}{i}\\int \\psi^ i\\frac{\\partial}{\\partial x}\\phi \\, dx &=\\frac{\\hbar}{i}\\int \\phi^ i\\frac{\\partial}{\\partial x}\\psi \\, dx=\\int \\phi^*\\hat{p}\\psi \\, dx .\\ \\end{align} $$ Then, given Hermitian $\\hat{A},\\hat{B},$ we have $$ \\begin{align} \\sigma_{A}\\sigma_{B} &\\geq \\int |A\\Psi|^{2} \\, dx \\int |B\\Psi|^{2} \\, dx \\ &\\geq \\int |A\\Psi| |B\\Psi| \\, dx \\ &\\geq \\left| \\text{Im}\\left( \\int (A\\Psi)^ B\\Psi \\, dx \\right) \\right| \\ &=\\left| \\int \\frac{(A\\Psi)^ B\\Psi-(B\\Psi)^ A\\Psi}{2i} \\, dx \\right| \\ &=\\left| \\frac{\\int \\Psi^ BA\\Psi-\\Psi^*AB\\Psi \\, dx }{2} \\right| \\ &= \\frac{\\left| \\left< [A,B] \\right> \\right|}{2}. \\end{align} $$ Recall that $\\left< \\hat{x},\\hat{p} \\right> =\\frac{\\hbar}{i},$ so $\\sigma_{x}\\sigma_{p}\\geq \\frac{\\hbar}{2},$ and $k=\\frac{p}{\\hbar}$ so [!important] Heisenberg Uncertainty $$ \\sigma_{x}\\sigma_{k}\\geq \\frac{1}{2}. $$","title":"Hermitian"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#fourier","text":"In typical Fourier Inversion we have $2\\pi i$ in the exponent. In 8.04 we dislike the $2\\pi$ so we distribute it: [!important] Fourier in Wavenumber Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Phi(x)e^{ikx} \\, dk ,\\ \\Phi(k)&=\\frac{1}{\\sqrt{ 2\\pi }}\\int \\Psi(x)e^{-ikx} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of wavenumbers. This is because $\\Psi(x)$ is composed of $e^{ipx/\\hbar}$ with magnitudes $\\tilde{\\Phi}(p)$. Then recall $k=\\frac{p}{\\hbar},$ so we can do [!important] Fourier in Momentum Space $$ \\begin{align} \\Psi(x)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Phi(x)e^{ipx/\\hbar} \\, dp ,\\ \\Phi(p)&=\\frac{1}{\\sqrt{ 2\\pi \\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar} \\, dx . \\end{align} $$ Here $\\Psi$ is the state, and $\\Phi$ is the distribution of momentum. Recall that wavenumber relates to wavelength, which relates to momentum.","title":"Fourier"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#inner-product","text":"Usual $\\left< \\psi,\\phi \\right>=\\int \\psi^ \\phi$. Then the conjugate $\\hat Q^ $ of $Q$ satisfies $\\left< \\phi,\\hat{Q}\\psi \\right> =\\left< \\hat{Q}^ \\phi,\\psi \\right>$. Then $\\hat{Q}$ is Hermitian iff. $\\hat{Q}=\\hat{Q}^ $. [!important] Copenhagen Measurement Postulate Given $\\left< \\phi,\\phi \\right> =1,$ the measured outcome of $\\hat{Q}$ is drawn from distribution of eigenvalues $\\lambda_{i}$ with probability coefficient of eigenvector $\\phi=\\sum_{i}c_{i}v_{i}.$","title":"Inner Product"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#expectation-roc","text":"$$ \\begin{align} \\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> &=\\frac{\\partial}{\\partial t}\\int \\Phi^ \\hat{Q}\\Phi \\, dx \\ &=\\int \\frac{\\partial}{\\partial t}\\left( \\Phi^ \\hat{Q}\\Phi \\right) \\, dx \\ &=\\int \\left( \\left(\\frac{\\partial}{\\partial t}\\Phi^ \\right) \\hat{Q}\\Phi \\right)+\\left( \\Phi^ \\frac{\\partial}{\\partial t}\\left( \\hat{Q}\\Phi \\right) \\right) \\, dx \\ &=\\int \\left( \\left( \\frac{1}{i\\hbar }\\hat{H}\\Phi\\right)^ \\hat{Q}\\Phi \\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\int -\\frac{1}{i\\hbar}\\left(\\Phi^ \\hat{H}\\hat{Q}\\Phi\\right) +\\frac{1}{i\\hbar}\\left( \\Phi^ \\hat{Q}\\hat{H}\\Phi \\right)+\\left( \\Phi^ \\left( \\frac{\\partial}{\\partial t}\\hat{Q} \\right) \\Phi \\right) \\, dx \\ &=\\left(\\frac{1}{i\\hbar}\\int \\Phi^ [\\hat{Q},\\hat{H}]\\Phi \\, dx \\right)+\\int \\Phi^ \\frac{\\partial\\hat{Q}}{\\partial t}\\Phi \\, dx \\ &=\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> +\\left< \\frac{\\partial\\hat{Q}}{\\partial t} \\right> . \\end{align} $$ [!important] Time-Independent Expectation ROC If $\\frac{\\partial\\hat{Q}}{\\partial t}=0$ then $$\\frac{\\partial}{\\partial t}\\left< \\hat{Q} \\right> =\\frac{1}{i\\hbar}\\left< [\\hat{Q},\\hat{H}] \\right> .$$","title":"Expectation ROC"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#problem-solving","text":"[!important] We want to solve the energy equation $$\\hat{H}\\psi=E\\psi,$$i.e. $$\\frac{\\hbar^{2}}{2m}\\nabla^{2}\\Psi=V\\Psi,$$ for the energy eigenstate $\\psi:\\mathbb{R}\\to \\mathbb{C},$ $\\hat{H}=-\\frac{\\hbar^2}{2m}\\nabla^{2}+V$ and $E\\in \\mathbb{R}.$ Then we get the stationary state $$\\psi(x,t)=e^{-iEt/\\hbar}\\psi(x).$$ Wavefunctions are piecewise continuous, and can have Dirac $\\delta$. A bound state satisfies $\\int \\Phi^{2} \\, dx$ converges, i.e. it is \"real\". A scattering state is usually $\\cos(kx)$ for $x \\to \\infty.$ Recall that if the potential is finite then the derivative of $\\Psi$ must be continuous, i.e. for square well etc. there is a BC for value equality and another BC for derivatives matching up. If $V=\\infty$ then we get $\\Psi=0$.","title":"Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#1d-problem-solving","text":"Our equation is [!important] 1D Energy Equation $$\\psi''+\\frac{2m}{\\hbar^{2}}(E-V(x))\\psi=0.$$","title":"1D Problem Solving"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#free-particle-on-circle","text":"On a looped domain $x \\in[0,L]$, we get eigenstates $e^{ikx}$, matching boundaries $kL=2\\pi n$ and $E_{n}=\\frac{{2\\pi^{2}\\hbar n^{2}}}{mL^{2}}.$","title":"Free Particle on Circle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#square-wells","text":"Now we have domain $\\mathbb{R}$ and experiment with $V(x)$.","title":"Square Well(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#infinite-case","text":"$$ V=\\begin{cases} 0&x \\in(0,a) \\ \\infty &else \\end{cases} $$ Then $\\Phi=0$ at $else,$ so $\\frac{d^2\\psi}{dx^2}=-\\frac{{2mE}}{\\hbar^{2}}\\psi.$ Get $\\sin(kx)$, check BCs and find $k=\\frac{n\\pi}{a},$ $E_{n}=\\frac{{\\pi^{2}\\hbar n^{2}}}{2ma^{2}}$ for all $n \\in \\mathbb{Z}$.","title":"Infinite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#finite-case","text":"$$ V=\\begin{cases} -V_{0}&|x| \\leq a\\ 0 &else \\end{cases} $$ We want $E \\in[-V_{0},0]$ yielding positive KE inside the well, negative KE outside. Then letting $k=\\sqrt{ \\frac{2m}{\\hbar^{2}}\\left( V_{0}-|E| \\right) }$ and $\\kappa=\\sqrt{ \\frac{{2m|E|}}{\\hbar^{2}} },$ we get $$ \\Phi=\\begin{cases} Ae^{\\pm ikx}&in\\ Be^{\\pm \\kappa x} &out \\end{cases} $$ Higher bound energy $E$ means slower decay $\\kappa$ and faster oscillation $k$. Let $\\eta=ka,\\xi=\\kappa a,$ then cancelling $A,B$ from matching values and $\\frac{\\partial}{\\partial x}$ yields $$ \\begin{align} \\eta^{2}+\\xi^{2}&=\\frac{{2mV_{0}a^{2}}}{\\hbar^{2}},\\ \\eta \\tan \\eta&=\\xi. \\end{align} $$ This yields finitely many stationary states.","title":"Finite case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#deltas","text":"","title":"Delta(s)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#simple-case","text":"$$ V=-V_{0}\\delta(x). $$ Then $\\psi''=\\frac{2m}{\\hbar^{2}}(V_{0}\\delta(x)-E)\\psi,$ and we must have $E<0$. Integrating near $0$ means $$ \\Delta \\psi'| {0}=V {0}\\frac{2m}{\\hbar^{2}}\\psi|_{0}, $$ and everywhere else $\\psi''=-E\\psi.$ We still have to be continuous for $\\psi''$ to be defined, so we get $e^{-|kx|}$.","title":"Simple case"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#simple-harmonic-oscillator","text":"","title":"Simple Harmonic Oscillator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#solving-setup","text":"$$ V(x)=\\frac{1}{2}m\\omega^{2}x^{2}. $$ This one is a banger. Substitution yields $$ \\psi''=\\frac{2m}{\\hbar^{2}}\\left( \\frac{1}{2}m\\omega^{2}x^{2}-E \\right) \\psi. $$ Take $u=x\\sqrt{ \\frac{m\\omega}{\\hbar}, }$ so that letting $\\mathcal E=\\sqrt{ \\frac{2}{\\hbar \\omega}E}$ yields $$\\frac{{\\partial^{2}\\psi}}{\\partial u^{2}}=\\left( u^{2} -\\mathcal E^{2}\\right) \\psi.$$ Then plug in $\\psi(u)=e^{{-x^{2}}/2}h$ to get $$h''-2uh'+(u^{2}-1)h=(u^{2}-\\mathcal E^{2})h,$$ so $h''=2uh'+(1-\\mathcal E^{2})h,$ which can be solved in polynomial for arb. degree.","title":"Solving Setup"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#factorization","text":"Let $\\hat{a},\\hat{a}^ =\\left( \\hat{x}\\pm \\frac{{i \\hat{p}}}{m\\omega}\\right)\\sqrt{ \\frac{m\\omega}{2\\hbar} }.$ Then $[a,a^ ]=-\\frac{2i}{\\hbar}\\left< \\hat{x},\\hat{p}\\right>=1.$ Recalling $\\hat{p}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x},$ $$ \\hat{H}=-\\frac{\\hbar^{2}}{2m} \\frac{{\\partial^{2}}}{\\partial x^{2}}+\\frac{1}{2}m\\omega^{2}x^{2}=\\frac{1}{2}m\\omega^{2}\\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right) =\\hbar\\omega \\left( \\frac{m\\omega}{2\\hbar} \\right) \\left( x^{2}+ \\frac{{\\hat{p}^{2}}}{m^{2}\\omega^{2}} \\right)= \\hbar\\omega \\left(a^*a+\\frac{1}{2}\\right) . $$ Then the ground solution $\\phi_{0}$, $$ \\frac{\\hbar\\omega}{2}=E_{0}=\\left< \\phi_{0},E\\phi_{0} \\right> =\\frac{\\hbar\\omega}{2}+\\hbar\\omega \\left< \\hat{a}\\phi_{0},\\hat{a}\\phi_{0} \\right> $$ so $\\hat{a}\\phi_{0}=0$ everywhere. Recall that $[\\hat{a},\\hat{a}^ ]=1.$ Let $\\hat{N}=\\hat{a}^ \\hat{a},$ then $\\hat{H}=\\hat{N}+\\frac{1}{2},$ and $$ [\\hat{N},(\\hat{a}^ )^{k}]=\\hat{a}^{ }\\hat{a}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{a}\\hat{a}^{ }+(\\hat{a}^{ })^{k}=[\\hat{N},(\\hat{a}^{ })^{k-1}]\\hat{a}^{ }+(\\hat{a}^{ })^{k}=k(\\hat{a}^{ })^{k}. $$ Similarly for $\\hat{a}^{k}$ yields $[\\hat N,\\hat{a}^k]=-k\\hat{a}^k.$ Let $\\phi_{k}=(\\hat{a}^{ })^{k}\\phi_{0},$ we get $$\\hat{ N}\\phi_{k}=(\\hat{N}(\\hat{a}^{ })^{k}-(\\hat{a}^{ })^{k}\\hat{N})\\phi_{0}=[\\hat{N},(\\hat{a}^{ })^{k}]\\phi_{0}=k(\\hat{a}^{*})^{k}\\phi_{0}=k\\phi_k.$$ Then $$\\hat{H}\\phi_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right) \\phi_{k},$$ so $E_{k}=\\hbar\\omega \\left( k+\\frac{1}{2} \\right).$ Putting it all together: [!important] SHO factorization Using the annihilation operator $\\hat{a}=\\left( \\hat{x}+\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} }$ and the creation operator $\\hat{a}^{ }=\\left( \\hat{x}-\\frac{{i\\hat{p}}}{m\\omega} \\right)\\sqrt{ \\frac{{m\\omega}}{2\\hbar} },$ let $\\hat{N}=\\hat{a}^{ }\\hat{a}.$ Then given normalized ground $\\phi_{0}$, we have: $$ \\begin{align} [\\hat{a},\\hat{a}^{ }]&=1,& \\hat{a}\\phi_{0}&=0,\\ \\phi_{{k+1}}&=\\frac{{\\hat{a}^{ }\\phi_{k}}}{\\sqrt{ k+1 }}, & \\phi_{k-1}&=\\frac{{\\hat{a}\\phi_{k}}}{\\sqrt{ k }}, \\ \\hat{N}\\phi_{k}&=k\\phi_{k}, & \\hat{H}&=\\hbar\\omega \\left( \\hat{N}+\\frac{1}{2} \\right), & E_{k}&=\\hbar\\omega\\left( k+\\frac{1}{2} \\right). \\end{align} $$","title":"Factorization"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#unidirectional-transmissionscattering","text":"Set up exponential ansatz for each domain and solve BCs.","title":"Unidirectional transmission/scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#example-step-potential","text":"Take $$ V=\\begin{cases} 0&x<0 \\ V_{0}&x>0 \\end{cases} $$Then we can send an \"incoming wave\" $\\Psi=Ae^{ikx}$ resulting in a reflected wave $Be^{-ikx}$ and a transmitted wave $Ce^{i\\bar{k}x}.$ Here $k^{2}=\\frac{2mE}{\\hbar^{2}}$ and $\\bar{k}^{2}=\\frac{2m(E-V_{0})}{\\hbar^{2}}$. Given this energy eigenstate we can take a pulse like \"wave packet\" and take Fourier series, then determine transmitted waves. In general, stationary phase paradigm can be used to determine location of \"hump\" of Fourier series, i.e. speed of wavepacket. For $E>V_{0}$, speeds are calculable. For $E<V_{0}$ reflected has a time delay, and there is no transmitted (only exponential decaying).","title":"Example: Step Potential"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#example-finite-square-well-resonance","text":"Suppose we're sending the same wave $Ae^{ikx}$ wave from the left through a finite square well $|x|<a$, then we get the ansatz $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ If the wavenumber $k_{2}$ inside the well resonates with the width of the well $2a$, i.e. if $2k_{2}a=n\\pi,$ then we get perfect transmission, and $E+V$ matches the $n$th bound state in the infinite square well.","title":"Example: Finite square well, resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#reflective-half-domain-scattering","text":"Consider infinite wall at $x=0$ and $V=0$ for suff. large $x$. Then send $\\Psi=e^{-ikx}$ for the usual $k^{2}=\\frac{2mE}{\\hbar^{2}}$. With time delay $\\delta(k)$ reflection is $e^{ikx+2i\\delta}$, so after constant scaling for large $x$ we get $\\Psi=\\sin(kx+\\delta)$. Note that the ansatz is $e^{i\\delta}\\sin(kx+\\delta)=\\frac{{e^{-ikx}+e^{2i\\delta+ikx}}}{2i},$ hence incoming is always $e^{-ikx}$ and outgoing is $e^{ikx}$ with time delay $\\delta$ in the form $e^{2i\\delta+ikx}$.","title":"Reflective half-domain scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#wavepacket-delta-t","text":"Devise a generalized function $\\delta(k)$ for the time delay, so that $e^{-ikx}$ becomes $e^{ikx+2i\\delta(k)}$. After applying stationary phase, time delay of a wavepacket becomes $\\Delta t=2\\hbar \\frac{{\\partial}}{\\partial E}\\delta(k_{0}).$","title":"Wavepacket $\\Delta t$"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#levinsons","text":"Add second infinite wall at $L\\to \\infty.$ For $V=0,$ $kL=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk=dn$ possible scattering states. For the $V$ we want, at a distance $\\Psi=\\sin(kx+\\delta)$ so $kL+\\delta=n\\pi,$ so interval $dk$ has $\\frac{L}{\\pi}dk+\\frac{1}{\\pi}\\frac{{d\\delta}}{dk}dk$ possible scattering states. Fix $L$. Turning on $V$ continuously/pointwise/whatever cannot change total number of eigenstates. Then turning on $V$ transforms $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ scattering states into bound states, so there are $\\frac{1}{\\pi}(\\delta(0)-\\delta(\\infty))$ bound states.","title":"Levinson's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#some-weird-experimentationconstruction","text":"We want to construct arbitrarily long positive time delay (negative violates causality). Then we want high resonance. Try a double-step negative-positive-zero potential. ![[Pasted image 20240107222205.png]] Amplitude of trapped area, plot (c), spikes when time delay is near $\\frac{\\pi}{2}$, which makes sense.","title":"Some weird experimentation/construction"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#weird-magic","text":"If we do stuff, e.g. setting $\\delta(k)$ we can artificially create high resonance.","title":"Weird magic"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#hydrogen-atom","text":"Ultimate goal: scattering or bound solutions for the spherical potential $V(r)=-\\frac{e^{2}k}{r}$ using Coulomb's $k.$","title":"Hydrogen atom"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#angular-momentum","text":"Typically $\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}.$ Then translating to quantum operators, $\\mathbf{\\hat{L}}\\equiv \\mathbf{\\hat{r}}\\times \\mathbf{\\hat{p}},$ in particular $$\\hat{L} {x}=\\hat{y}\\hat{p} {z}-\\hat{z}\\hat{p} {y}=\\frac{\\hbar}{i}\\left( y \\frac{d}{dz}-z \\frac{d}{dy} \\right) =\\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi {x}},$$where $\\phi_{x}=\\arctan\\left( \\frac{z}{y} \\right).$ Conveniently $$ [\\hat{L} {x},\\hat{L} {y}]=i\\hbar \\hat{L}_{z}. $$ Thus no nontrivial eigenstates simultaneously exist.","title":"Angular momentum"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Cleaned%20Notes/8.04/#radial-ansatz","text":"Given spherical potential $V(r)$ as in the case of hydrogen nucleus. Converting to spherical, and taking $\\Psi=R(r)Y(\\theta,\\phi)$ yields $$ \\begin{align} \\frac{\\partial}{\\partial r}\\left( r^{2} \\frac{{\\partial R}}{\\partial r} \\right)&=l(l+1)R- \\frac{{2mr^{2}}}{\\hbar^{2}}(E-V(r))R,\\ \\ -\\left( \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial\\theta}\\sin\\theta \\frac{\\partial}{\\partial\\theta} +\\frac{1}{\\sin ^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}} \\right)Y &=l(l+1)Y. \\end{align} $$ Plugging in $V(r)=-\\frac{e^{2}k}{r}$ yields massive DE spam. We use the ansatz $R=\\frac{u(r)}{r}$ cuz it's cleaner. The cleaner equation is on the formula sheet. Solutions for $Y$ are the spherical harmonics $Y_{lm}(\\theta,\\phi)=C \\cdot P_{l}^{m}(\\sin\\theta)e^{i m\\phi}$ for polynomial $P_{l}^{m}$. Solutions for $R$ are $V$-dependent, for our case it is $L_{n}^{\\alpha}(r)e^{-|r|/2n}$ using the associated Laguerre polynomials $L_{n}^{\\alpha}.$","title":"Radial ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04%20Final%20Notes/","text":"![[Important Equations]] ![[8.04.1 Superposition, norms, M-F interferometer, E-V bombs]] ![[8.04.2 Dualities of Light and Matter]] ![[8.04.3 Matter Waves]] ![[8.04.4 Schrodinger's and Wavefunctions]] ![[8.04.5 Wavefunction as Probability]] ![[8.04.6 Uncertainty and Fourier Inverse]] ![[8.04.7 Momentum space, Expectation, Uncertainty 2]] ![[8.04.8 Eigenstates, Hermitian, Observables, Measurement]] ![[8.04.9 Time Independence, Free Particle on Circle]] ![[8.04.10 Square Well]] ![[8.04.11 1D Potentials]] ![[8.04.12 Dirac Delta, Node Theorem, SHO]] ![[8.04.13 Factorizing SHO]] ![[8.04.14 Step Potential]] ![[8.04.15 Resonance, Half-Domain Scattering]] ![[8.04.16 Levinson's, Resonance, Complex k-plane]] ![[8.04.17 Angular Momentum, Radial Ansatz]] ![[8.04.18 Solving Hydrogen]]","title":"8.04 Final Notes"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.1%20Superposition%2C%20norms%2C%20M-F%20interferometer%2C%20E-V%20bombs/","text":"While physical theories have not been made compatible, quantum mechanics is a framework for operating on other fields, such as classical mechanics and EM. One nice thing is linearity; the time evolution operator $\\hat H$ in Schrodinger's is assumed to be linear, so solutions are all linear. This operator is somewhat arbitrary, by the way. States superimpose linearly, we can represent them as vectors in some abstract $\\mathbb C$-valued vector space, and if outcomes are finite we can represent as a unit vector in $\\CC^k$. Then the probability of any event is the norm of its coefficient. Because of this, states are invariant on constant scaling . Also recall entanglement; we can use $\\otimes$ to denote one outcome with several measurements; if two measurements are not independent in some state, they're entangled. Let's examine one case study: the Mach-Zehnder interferometer . ![[Pasted image 20230907105403.png]] Represent photon states with unit $\\CC^2$ column vectors. The splitters are probabilistic, they split one way vs the other with amplitudes $s,t$ satisfying $s^2+t^2=1.$ But that's only from one direction; it can also split differently depending on input from the other direction. Then each BS is a $2\\times 2$ complex matrix. It must also be an orthogonal matrix. Considering two \"correct\" orthogonal matrices with $\\frac1{\\sqrt 2},$ their product is actually $\\begin{bmatrix} 0&1\\-1&0\\end{bmatrix}.$ Then in this above setup where the initial state is all from bottom, it will end up all going to detector 0, the top. We can use this to get some \"uncanny measurement\" in an EV bomb. If we stick a bomb in the bottom path between the two splitters, and the detector does work, then it is $50\\%$ a photon ends at D1, and independently $50\\%$ a photon goes through the top path. Then given a live bomb, with $25\\%$ probability we discover that it is live without setting it off. Final note: phase shifters work by multiplying the probability amplitude by $e^{i\\delta}.$ No clue why this works. $\\require{mhchem}\\newcommand{\\CC}{\\mathbb C}$-latex","title":"8.04.1 Superposition, norms, M F interferometer, E V bombs"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/","text":"Def'n, Motivation First we'll treat unrigorously, with a potential that is a well with infinite walls. Then we'll take a limit. Inf Square Well So we have $V(x)=0$ if $x\\in(0,a),$ $V(x)=\\infty$ else. Then we must have $\\psi(x)=0$ for $x\\in(-\\infty,0]\\cup [a,\\infty)$ by continuity of $\\psi$ and energy conservation. Then in $[0,a]$ there is no potential and we just get 1D Schrodinger free equation $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi.$$ Then we do the usual, set $k^2=\\frac{2mE}{\\hbar^2},$ then $\\psi=e^{\\pm ikx}.$ After imposing boundary conditions we get $\\psi_n(x)=\\sin(\\frac{n\\pi x}{a}).$ The normalization is $\\sqrt{\\frac 2a}.$ In other words, we can take any wavefunction on this domain and Fourier expand to obtain distribution among the quantized energy distribution. Finite Square Well Let's reframe as $V(x)=-V_0$ if $|x|\\leq a,$ and $V(x)=0$ otherwise. Note that the energy eigenvalue $-V_0<E<0,$ otherwise we get a non-normalizable plane wave for $|x|>a,$ and we also want positive KE in the well. (With $E<0$ we get a decaying exponential, below) Then within the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2m(V_0-|E|)}{\\hbar^2}\\psi.$$ Outside the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi=\\frac{2m|E|}{\\hbar^2}\\psi.$$ Then with $$k=\\sqrt{\\frac{2m}{\\hbar^2}(V_0-|E|)},\\qquad\\kappa=\\sqrt{\\frac{2m|E|}{\\hbar^2}}.$$ we get $\\psi(x)=Ae^{-\\kappa|x|}$ outside the well, $e^{\\pm ikx}$ inside the well. Taking the even/odd solutions for inside yields $\\cos kx$ or $\\sin kx$. Introduce the nondimensionalized $\\eta=ka,\\xi=\\kappa a,$ then in fact $$\\eta^2+\\xi^2=\\frac{2mV_0a^2}{\\hbar^2}.$$ Then imposing $\\psi,\\psi'$ continuity, for the even case at $x>0$ we get $$\\cos \\eta=Ae^{-\\xi},-k\\sin\\eta=-\\kappa Ae^{-\\xi}\\implies k\\tan\\eta=\\kappa\\iff \\eta\\tan\\eta=\\xi.$$ For the odd case we similarly get $-\\eta\\cot\\eta=\\xi.$ Considering the constraint above, there are $O\\left(\\sqrt{\\frac{2mV_0a^2}{\\hbar^2}}\\right)$ many stationary states, one for each period of $\\eta\\tan\\eta$. Now it's clear that in the limit $k$ is the same set of values as before, and $\\kappa$ goes to infinity. General qualitative properties include - Higher bound energy is faster oscillation in well, slower decay outside well","title":"8.04.10 Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#defn-motivation","text":"First we'll treat unrigorously, with a potential that is a well with infinite walls. Then we'll take a limit.","title":"Def'n, Motivation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#inf-square-well","text":"So we have $V(x)=0$ if $x\\in(0,a),$ $V(x)=\\infty$ else. Then we must have $\\psi(x)=0$ for $x\\in(-\\infty,0]\\cup [a,\\infty)$ by continuity of $\\psi$ and energy conservation. Then in $[0,a]$ there is no potential and we just get 1D Schrodinger free equation $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi.$$ Then we do the usual, set $k^2=\\frac{2mE}{\\hbar^2},$ then $\\psi=e^{\\pm ikx}.$ After imposing boundary conditions we get $\\psi_n(x)=\\sin(\\frac{n\\pi x}{a}).$ The normalization is $\\sqrt{\\frac 2a}.$ In other words, we can take any wavefunction on this domain and Fourier expand to obtain distribution among the quantized energy distribution.","title":"Inf Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.10%20Square%20Well/#finite-square-well","text":"Let's reframe as $V(x)=-V_0$ if $|x|\\leq a,$ and $V(x)=0$ otherwise. Note that the energy eigenvalue $-V_0<E<0,$ otherwise we get a non-normalizable plane wave for $|x|>a,$ and we also want positive KE in the well. (With $E<0$ we get a decaying exponential, below) Then within the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2m(V_0-|E|)}{\\hbar^2}\\psi.$$ Outside the well, $$\\frac{d^2\\psi}{dx}=-\\frac{2mE}{\\hbar^2}\\psi=\\frac{2m|E|}{\\hbar^2}\\psi.$$ Then with $$k=\\sqrt{\\frac{2m}{\\hbar^2}(V_0-|E|)},\\qquad\\kappa=\\sqrt{\\frac{2m|E|}{\\hbar^2}}.$$ we get $\\psi(x)=Ae^{-\\kappa|x|}$ outside the well, $e^{\\pm ikx}$ inside the well. Taking the even/odd solutions for inside yields $\\cos kx$ or $\\sin kx$. Introduce the nondimensionalized $\\eta=ka,\\xi=\\kappa a,$ then in fact $$\\eta^2+\\xi^2=\\frac{2mV_0a^2}{\\hbar^2}.$$ Then imposing $\\psi,\\psi'$ continuity, for the even case at $x>0$ we get $$\\cos \\eta=Ae^{-\\xi},-k\\sin\\eta=-\\kappa Ae^{-\\xi}\\implies k\\tan\\eta=\\kappa\\iff \\eta\\tan\\eta=\\xi.$$ For the odd case we similarly get $-\\eta\\cot\\eta=\\xi.$ Considering the constraint above, there are $O\\left(\\sqrt{\\frac{2mV_0a^2}{\\hbar^2}}\\right)$ many stationary states, one for each period of $\\eta\\tan\\eta$. Now it's clear that in the limit $k$ is the same set of values as before, and $\\kappa$ goes to infinity. General qualitative properties include - Higher bound energy is faster oscillation in well, slower decay outside well","title":"Finite Square Well"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/","text":"Reminder that we're only working with stationary states, i.e. eigenstates of the Hamiltonian. Thus there's no time derivative. Our equation is $$\\psi''+\\frac{2m}{\\hbar^2}(E-V(x))\\psi=0.$$ Properties No degenerate bound states All states are full-real These are a result of being able to solve the above diffEq uniquely given $\\lim_{x\\to\\infty}\\psi(x)=0.$ Energy eigenvalue $E\\geq\\min_x V(x).$ If $E-V(x)<0$ everywhere then $\\psi$ will either be always positive + concave up or always neg + concave down. Just find somewhere satisfying $\\psi'(x)=0.$ If potential is even, we can even specify - Eigenstates must be even or odd . Slowly varying potentials Letting $E-V=K$ for KE, we get $\\psi''=-\\frac{2m}{\\hbar^2}K=-\\frac{p^2}{\\hbar^2}\\psi$ by some degen semi-classical allusion. Then $\\psi\\sim\\cos\\left(\\frac{2\\pi}{\\lambda}x\\right),$ the deBroglie wavelength $\\lambda=\\frac hp$ of $\\psi$. Superposition of momentum $\\pm p$. We make the assumption that $\\lambda(x)\\frac{dV}{dx}\\ll V(x).$ Consider classically a ball sliding around on a roller coaster; KE at a point is $E-V$ at that point; with larger KE it's moving faster, and less likely to be found there. Indeed, we find in semiclassical approximation that $$\\text{Amp}(\\psi(x))\\sim\\sqrt{\\lambda(x)}\\sim\\frac1{\\sqrt{p(x)}}.$$ The semiclassical approximation assumes $\\psi$ oscillates a lot before $V$ changes, so we can locally average out the oscillation in $\\psi$ (that's quantization for ya). You can sketch wavefunctions by analyzing sign of $\\psi''$ and intercepts &c. Shooting Method In this example, used for even/odd potentials. Our boundary condition is $\\Psi(\\pm\\infty)=0.$ Basically, binary search on possible energies. Begin with $\\Psi(0)=1,\\Psi'(0)=0$ for even states and $\\Psi(0)=0,\\Psi'(0)=1$ for odd states.","title":"8.04.11 1D Potentials"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#properties","text":"No degenerate bound states All states are full-real These are a result of being able to solve the above diffEq uniquely given $\\lim_{x\\to\\infty}\\psi(x)=0.$ Energy eigenvalue $E\\geq\\min_x V(x).$ If $E-V(x)<0$ everywhere then $\\psi$ will either be always positive + concave up or always neg + concave down. Just find somewhere satisfying $\\psi'(x)=0.$ If potential is even, we can even specify - Eigenstates must be even or odd .","title":"Properties"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#slowly-varying-potentials","text":"Letting $E-V=K$ for KE, we get $\\psi''=-\\frac{2m}{\\hbar^2}K=-\\frac{p^2}{\\hbar^2}\\psi$ by some degen semi-classical allusion. Then $\\psi\\sim\\cos\\left(\\frac{2\\pi}{\\lambda}x\\right),$ the deBroglie wavelength $\\lambda=\\frac hp$ of $\\psi$. Superposition of momentum $\\pm p$. We make the assumption that $\\lambda(x)\\frac{dV}{dx}\\ll V(x).$ Consider classically a ball sliding around on a roller coaster; KE at a point is $E-V$ at that point; with larger KE it's moving faster, and less likely to be found there. Indeed, we find in semiclassical approximation that $$\\text{Amp}(\\psi(x))\\sim\\sqrt{\\lambda(x)}\\sim\\frac1{\\sqrt{p(x)}}.$$ The semiclassical approximation assumes $\\psi$ oscillates a lot before $V$ changes, so we can locally average out the oscillation in $\\psi$ (that's quantization for ya). You can sketch wavefunctions by analyzing sign of $\\psi''$ and intercepts &c.","title":"Slowly varying potentials"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.11%201D%20Potentials/#shooting-method","text":"In this example, used for even/odd potentials. Our boundary condition is $\\Psi(\\pm\\infty)=0.$ Basically, binary search on possible energies. Begin with $\\Psi(0)=1,\\Psi'(0)=0$ for even states and $\\Psi(0)=0,\\Psi'(0)=1$ for odd states.","title":"Shooting Method"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.12%20Dirac%20Delta%2C%20Node%20Theorem%2C%20SHO/","text":"More analysis of solving 1D potentials. Dirac Delta is a \"function\" (actually it's a functional) that satisfies ... Node Theorem: the n th lowest-energy bound state has $n-1$ nodes, for $\\lim_{x\\to\\infty}V=\\infty$. Proof: Take an infinitesimally thin \"screen\" outside of which everything is 0. Then we get $n-1$ nodes for the $n$th state, because that's just square well, but infinitesimal. Then as we stretch the screen out, the $n$th state can never gain more nodes. At the endpoints of the screen we have $\\psi=0$ and so the sign of $\\psi'$ at those endpoints can't change. Also we can't have a \"double node\" getting created; if the wavefunction is tangent to the $x$-axis then $\\psi=\\psi'=0\\implies\\psi\\equiv 0.$ Done! Harmonic Oscillator: $V(x)=\\frac 12m\\omega^2 x^2.$ To solve this, we nondimensionalize the usual eigenstate equation to $$\\frac{d^2\\varphi}{du^2}=(u^2-\\mathcal E)\\varphi.$$ Substituting $\\varphi=e^{\\frac{-x^2}2}h$ is completely motivated, and after doing that we can solve for $h$ as some finite-degree polynomial. Tada!","title":"8.04.12 Dirac Delta, Node Theorem, SHO"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.13%20Factorizing%20SHO/","text":"Recall the Hermitian conjugate of an operator $A$ satisfies $$(\\varphi,A\\psi)=(A^\\dagger\\varphi,\\psi).$$ A Hermitian operator is one whose conjugate is itself. Given some potential, if we can factor the resulting Hamiltonian $\\hat H=\\hat A^\\dagger\\hat A+E_0\\,\\bf 1,$ where $\\bf 1$ is the identity operator, then we get that $\\hat H\\varphi=E\\varphi\\implies E\\geq E_0.$ Usually $\\hat A$ will be a first-order differential operator, and the ground energy state will satisfy $\\hat A\\varphi_0=0.$ We call the operator $\\hat N=\\hat A^\\dagger \\hat A$ the \"number operator.\" Simple Harmonic Oscillator Take $V=\\frac12 m\\omega x^2$ as before, then $$\\hat H=\\frac12m\\omega^2(\\hat x^2+\\frac{\\hat p^2}{m^2\\omega^2}).$$ Try $V,V^\\dagger=\\hat x\\pm \\frac{i\\hat p}{m\\omega}.$ Then notice $$[V,V^\\dagger]=-\\frac{2i}{m\\omega}[\\hat x,\\hat p]=\\frac{2\\hbar}{m\\omega}\\,\\bf1.$$ Take the dimensionless $a=V\\sqrt{\\frac{m\\omega}{2\\hbar}}$ to get [!important] Dimensionless Factorization $$\\hat H=\\hbar\\omega(a^\\dagger a+\\frac12),[a,a^\\dagger]=1.$$ Explicitly, $\\hat a =\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat x+\\frac{i\\hat p}{\\omega}\\right),$ and $\\hat a^\\dagger$ is with $-$. Equivalently (and usefully) $\\hat x=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger),$ $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Now the ground energy state $\\varphi_0$ satisfies $\\hat a\\varphi_0=0,$ since $$E=[\\varphi,E\\varphi]=[\\varphi,\\hat H\\varphi]=E_0+[a\\varphi,a\\varphi]\\geq E_0.$$ That's why we call $\\hat a$ the annihilation operator. Now note that $$[\\hat N,(a^\\dagger)^k]=a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^ka^\\dagger a.$$ Then $-a^\\dagger a=-aa^\\dagger+1,$ so we get $$a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^k a a^\\dagger+(a^\\dagger)^k=[\\hat N,(a^\\dagger)^{k-1}]a^\\dagger+(a^\\dagger)^k=k(a^\\dagger)^k.$$ Thus, [!important] Number Operator $$[\\hat N,(a^\\dagger)^k]=k(a^\\dagger)^k,[\\hat N,a^k]=-ka^k.$$ Now if we take $\\varphi_1=a^\\dagger\\varphi_0,$ and generally, $\\varphi_i=(a^\\dagger)^i\\varphi_0,$ $$\\hat N\\varphi_{i}=[\\hat N,(a^\\dagger)^i]\\varphi_0=i(a^\\dagger)^i\\varphi_0.$$ Thus $\\varphi_i$ has energy $E=\\hbar\\omega(i+\\frac12).$ Thus $a^\\dagger$ is called the creation operator. Finally notice that $$a\\varphi_n=[a,(a^\\dagger)^n]\\varphi_0=n(a^\\dagger)^{n-1}\\varphi_0=n\\varphi_{n-1}.$$ Thus [!important] Creation and Destruction $\\hat a\\varphi_{i+1}=\\sqrt{i+1}\\cdot\\varphi_i$ and $\\hat a^\\dagger\\varphi_i=\\sqrt{n+1}\\varphi_{i+1},$ not considering scaling. $a\\varphi_0=\\bf 0.$","title":"8.04.13 Factorizing SHO"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.13%20Factorizing%20SHO/#simple-harmonic-oscillator","text":"Take $V=\\frac12 m\\omega x^2$ as before, then $$\\hat H=\\frac12m\\omega^2(\\hat x^2+\\frac{\\hat p^2}{m^2\\omega^2}).$$ Try $V,V^\\dagger=\\hat x\\pm \\frac{i\\hat p}{m\\omega}.$ Then notice $$[V,V^\\dagger]=-\\frac{2i}{m\\omega}[\\hat x,\\hat p]=\\frac{2\\hbar}{m\\omega}\\,\\bf1.$$ Take the dimensionless $a=V\\sqrt{\\frac{m\\omega}{2\\hbar}}$ to get [!important] Dimensionless Factorization $$\\hat H=\\hbar\\omega(a^\\dagger a+\\frac12),[a,a^\\dagger]=1.$$ Explicitly, $\\hat a =\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat x+\\frac{i\\hat p}{\\omega}\\right),$ and $\\hat a^\\dagger$ is with $-$. Equivalently (and usefully) $\\hat x=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger),$ $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Now the ground energy state $\\varphi_0$ satisfies $\\hat a\\varphi_0=0,$ since $$E=[\\varphi,E\\varphi]=[\\varphi,\\hat H\\varphi]=E_0+[a\\varphi,a\\varphi]\\geq E_0.$$ That's why we call $\\hat a$ the annihilation operator. Now note that $$[\\hat N,(a^\\dagger)^k]=a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^ka^\\dagger a.$$ Then $-a^\\dagger a=-aa^\\dagger+1,$ so we get $$a^\\dagger a(a^\\dagger)^k-(a^\\dagger)^k a a^\\dagger+(a^\\dagger)^k=[\\hat N,(a^\\dagger)^{k-1}]a^\\dagger+(a^\\dagger)^k=k(a^\\dagger)^k.$$ Thus, [!important] Number Operator $$[\\hat N,(a^\\dagger)^k]=k(a^\\dagger)^k,[\\hat N,a^k]=-ka^k.$$ Now if we take $\\varphi_1=a^\\dagger\\varphi_0,$ and generally, $\\varphi_i=(a^\\dagger)^i\\varphi_0,$ $$\\hat N\\varphi_{i}=[\\hat N,(a^\\dagger)^i]\\varphi_0=i(a^\\dagger)^i\\varphi_0.$$ Thus $\\varphi_i$ has energy $E=\\hbar\\omega(i+\\frac12).$ Thus $a^\\dagger$ is called the creation operator. Finally notice that $$a\\varphi_n=[a,(a^\\dagger)^n]\\varphi_0=n(a^\\dagger)^{n-1}\\varphi_0=n\\varphi_{n-1}.$$ Thus [!important] Creation and Destruction $\\hat a\\varphi_{i+1}=\\sqrt{i+1}\\cdot\\varphi_i$ and $\\hat a^\\dagger\\varphi_i=\\sqrt{n+1}\\varphi_{i+1},$ not considering scaling. $a\\varphi_0=\\bf 0.$","title":"Simple Harmonic Oscillator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.14%20Step%20Potential/","text":"The step potential is $0$ for $x<0,$ $V_0$ for $x>0$. Take a functional or whatever if you prefer. Once again we've got piecewise constant potentials, which is just linear combinations of exponentials. Specifically, with $k^2=\\frac{2mE}{\\hbar^2}, \\bar k^2=\\frac{2m(E-V_0)}{\\hbar^2},$ we have $\\Psi=Ae^{ikx}+Be^{-ikx}$ for $x<0,$ $C e^{i\\bar kx}$ for $x>0.$ Once again analyzing continuity of $\\psi$ and $\\psi'$ at $x=0$ we can solve for $A,B,C$ in terms of $k,\\bar k.$ With $E<V_0$ we get imaginary $\\bar k$ resulting in a decaying exponential in forbidden region, and some ghastly phase-shifted sine for $x<0.$ The time factor is $e^{iEt/\\hbar}.$ Then with the energy eigenstate we can compose these states into a normalizable wavepacket. The three terms corresponding to $A,B,C$ are like the incident, reflected, and transmitted wave. Using the stationary phase paradigm we can determine the speed of the wavepacket components. For $E>V_0$ the incoming and reflected both have speed $\\frac{\\hbar k_0}{m}$ and reflected has speed $\\frac{\\hbar \\bar k_0}m.$ For $E<V_0$ the observed phase shift in the sine because a time delay in the reflected component. Finally, a funny heuristic argument: Suppose we saw a particle in the forbidden region. Then because of the decaying exponential it's about $\\frac1k,$ where $\\kappa^2=\\frac{2m(V_0-E)}{\\hbar^2}$, of the way in. The position uncertainty must be at most $\\kappa,$ so $p\\geq\\frac{\\hbar}{\\nabla x}=\\hbar\\kappa.$ Then this new momentum contributes to an amount of energy $\\frac{p^2}{2m}=\\frac{\\hbar^2\\kappa^2}{2m}=V_0-E,$ so the final energy is in fact at least $V_0$ probably.","title":"8.04.14 Step Potential"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/","text":"Problem 1 Explore the double-well potential $V=-g\\delta(x-a)-g\\delta(x+a).$ Here $\\delta$ is Dirac delta, $a,g$ are positive constants. Even bound state is $\\cosh kx$ in the middle, $Ae^{-kx}$ on the right side. Then $A=1+e^{2ak}.$ Because we have Dirac delta justified in not matching derivatives, but $-\\frac{\\hbar^2}{2m}\\Delta \\psi'|_a=g\\psi(a).$ Then $\\psi'_L=k\\sinh ak, \\psi'_R=-kAe^{-ak}=-k\\cosh ak,$ so $\\Delta\\psi'|_a=-2ke^{ak}.$ Then $2ke^{ak}\\cdot\\frac{\\hbar^2}{2m}=g\\cosh ak,$ i.e. $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1+e^{-2ak}).$ Odd bound state is $\\sinh kx$ in the middle. Similar computation yields $\\psi'_L=k\\cosh ak,$ $\\psi'_R=-k\\sinh ak,$ so $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1-e^{-2ak}).$ Recall $k^2=\\frac{2mE}{\\hbar^2}.$ In the limit of large $a,$ the LHS goes to 0. We're solving $C=\\frac{1\\pm e^{-2z}}{z}$ for $z$ as $C\\to 0.$ Then $z\\approx \\frac1C,$ to one extra degree $z_1\\approx\\frac{1+e^{-2/C}}C,z_2\\approx\\frac{1-e^{-2/C}}C.$ Then $k_1-k_2=\\frac{2e^{-2/C}}{aC}=\\frac{2e^{-2amg/\\hbar^2}}{\\hbar^2/mg}=\\frac{2mge^{-2amg/\\hbar^2}}{\\hbar^2}.$ Problem 2 Skipped. Problem 3 Calculate probabilities that quantum SHO has position greater than amplitude in classical SHO for $\\mathcal E=1,3,5.$ We can work entirely in non-dimensional coordinates. We get the polynomials $1,x,x^2-\\frac12$ as solutions to $u''-2xu'+(\\mathcal E-1)u=0.$ The classical energy potential $V=x^2.$ Then, non-normalized we get - $\\psi=e^{-x^2/2}$ corresponding to $L=1.$ Get $\\approx 0.078649.$ - $\\psi=xe^{-x^2/2}$ corresponding to $L=\\sqrt 3.$ Get $\\approx 0.0558051.$ - $\\psi=(2x^2-1)e^{-x^2/2}$ corresponding to $L=\\sqrt 5.$ Get $\\approx 0.04753471774.$ Problem 4 Compute: (a) Expectation of $x^4$ on $n$th SHO eigenstate (b) $\\Delta x,\\Delta p,\\Delta x\\Delta p$ on $n$th SHO eigenstate Finally, (c) Check that $$e^{-s^2+2s\\xi}=\\sum_{n=0}H_n(\\xi)\\frac{s^n}{n!}$$ satisfies $H_n''-2\\xi H_n'+2nH_n=0.$ Recall that $\\hat x=(\\hat a+\\hat a^\\dagger)\\sqrt{\\frac{\\hbar}{2m\\omega}}.$ Then $$ \\begin{align } \\langle \\psi_n,\\hat x^4\\psi_n\\rangle &=\\langle \\hat x^2\\psi_n,\\hat x^2\\psi_n\\rangle\\ &=\\frac{\\hbar^2}{4m^2\\omega^2}\\langle(\\hat a^2+\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a+\\hat a^\\dagger\\hat a^\\dagger)\\psi_n,\\cdot \\rangle. \\end{align } $$ Note that $\\hat a$ is destruction, $\\hat a^\\dagger$ is construction so we can split by \"degree\". Reference L14-15, p7. $$\\langle\\hat a^2\\psi_n,\\cdot\\rangle=\\langle \\sqrt{n(n-1)}\\phi_{n-2},\\cdot\\rangle=n(n-1).$$ Similarly, $\\langle (\\hat a^\\dagger)^2\\psi_n,\\cdot\\rangle=(n+1)(n+2).$ Finally $(\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a)\\psi_n=(n+n+1)\\psi_n,$ so our final answer is $n(n-1)+(n+1)(n+2)+(2n+1)^2=6n^2+6n+3.$ Adding our units back in, we get an answer of $$(6n^2+6n+3)\\frac{\\hbar^2}{4m^2\\omega^2}.$$ (b) is similar. $(\\Delta x)^2=\\frac{\\hbar}{2m\\omega}(2n+1).$ Now $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Then clearly $\\langle \\psi_n,\\hat p\\psi_n=0$ (also true from potential symmetry). Finally $$ \\begin{align } \\langle \\psi_n,\\hat p^2\\psi_n\\rangle &=-\\frac{m\\omega\\hbar}2\\langle\\psi_n,C_1\\psi_{n+2}+C_2\\psi_{n-2}-(2n+1)\\psi_n\\rangle\\ &=(2n+1)\\frac{m\\omega\\hbar}2. \\end{align } $$ Thus $\\Delta x\\Delta p=(n+\\frac12)\\hbar.$ (c) Taking the $s$-derivative of the whole generating function yields $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=1}H_n \\frac{s^{n-1}}{(n-1)!}=\\sum_{n=0}H_{n+1}\\frac{s^n}{n!}.$$ Then $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=0}2H_{n}\\frac{s^{n+1}}{n!}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=1}2H_{n-1}n\\frac{s^n}{n!}.$$ Thus $$H_{n+1}=2\\xi H_n-2nH_{n-1}.$$ Taking the $\\xi$-derivative yields $$\\sum_{n=1}2nH_{n-1}\\frac{s^n}{n!}=2se^{-s^2+2s\\xi}=\\sum_{n=0}H_n'\\frac{s^n}{n!}.$$ Thus $2nH_{n-1}=H_n',$ so $$H_n''-2\\xi H_n'+2nH_n=4n(n-1)H_{n-2}-4\\xi nH_{n-1}+2nH_n=2n(H_n-2\\xi H_{n-1}+2(n-1)H_{n-2})=0.$$ Yay! Problem 5 Harmonic oscillator, except $V(x)=\\infty$ for $x<0$. What are the possible energy levels? Basically we need the polynomial solution to $H''-2xH+2nH=0$ to satisfy $H(0)=0.$ We established $H_{n+1}=2xH_n-2nH_{n-1},$ so $H_{n+1}(0)=-2nH_{n-1}(0).$ Since $H_0=1, H_1=x,$ we get $n$ must be odd so the possible energy levels are $\\mathcal E=4n+3.$ Re-dimensionalizing yields $E=(4n+3)\\hbar\\omega.$ Problem 6 Given $V=m\\omega^2x^2,$ $\\Psi(x,0)=\\frac1{\\sqrt2}\\left(\\varphi_0(x)+\\varphi_1(x)\\right)$: - Find $\\Psi(x,t),|\\Psi(x,t)|^2$ - Find $\\langle x\\rangle(t).$ - Find $\\langle p\\rangle(t).$ - Show that for any such $\\Psi,$ $|\\Psi(x,t)|^2=|\\Psi(x,t+T)|^2$ where $T=\\frac{2\\pi}\\omega.$ $E(\\varphi_0)=\\hbar\\omega, E(\\varphi_1)=3\\hbar\\omega,$ so $$\\Psi(x,t)=\\frac1{\\sqrt2}(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1).$$ Then $$\\Psi\\Psi^*=\\frac12\\left(\\varphi_0^2+\\varphi_1^2+2\\varphi_0\\varphi_1\\cos(2t\\omega)\\right).$$ Then $\\langle x\\rangle=\\cos(2t\\omega)\\int x\\varphi_0\\varphi_1.$ Turns out that $\\hat x\\varphi_0=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger)\\varphi_0=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\varphi_1.$ As a result $\\langle x\\rangle=\\cos(2t\\omega)\\sqrt{\\frac\\hbar{2m\\omega}}.$ Next, $$ \\begin{align } \\langle \\hat p\\rangle &=\\int \\Psi\\hat p\\Psi\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a)(e^{it\\omega}\\varphi_0+e^{3it\\omega}\\varphi_1)\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(-e^{3it\\omega}\\varphi_0+e^{it\\omega}\\varphi_1)\\ &=\\frac i2\\sqrt{\\frac{m\\omega\\hbar}2}\\int -e^{2it\\omega}\\varphi_0^2+e^{-2it\\omega}\\varphi_1^2\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\frac1{2i}(e^{2it\\omega}-e^{-2it\\omega})\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\sin(2t\\omega). \\end{align } $$ Finally, $E=N\\hbar\\omega,$ so all $e^{itE/\\hbar}$ terms are periodic in $t$ every $\\frac{2\\pi}{E/\\hbar}=\\frac{2\\pi}{N\\omega}.$ Thus if we increment $t$ by $\\frac{2\\pi}\\omega$ the value of $\\Psi$ is unchanged.","title":"8.04.15 Pset 7"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-1","text":"Explore the double-well potential $V=-g\\delta(x-a)-g\\delta(x+a).$ Here $\\delta$ is Dirac delta, $a,g$ are positive constants. Even bound state is $\\cosh kx$ in the middle, $Ae^{-kx}$ on the right side. Then $A=1+e^{2ak}.$ Because we have Dirac delta justified in not matching derivatives, but $-\\frac{\\hbar^2}{2m}\\Delta \\psi'|_a=g\\psi(a).$ Then $\\psi'_L=k\\sinh ak, \\psi'_R=-kAe^{-ak}=-k\\cosh ak,$ so $\\Delta\\psi'|_a=-2ke^{ak}.$ Then $2ke^{ak}\\cdot\\frac{\\hbar^2}{2m}=g\\cosh ak,$ i.e. $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1+e^{-2ak}).$ Odd bound state is $\\sinh kx$ in the middle. Similar computation yields $\\psi'_L=k\\cosh ak,$ $\\psi'_R=-k\\sinh ak,$ so $\\frac{\\hbar^2}{amg}=(ak)^{-1}(1-e^{-2ak}).$ Recall $k^2=\\frac{2mE}{\\hbar^2}.$ In the limit of large $a,$ the LHS goes to 0. We're solving $C=\\frac{1\\pm e^{-2z}}{z}$ for $z$ as $C\\to 0.$ Then $z\\approx \\frac1C,$ to one extra degree $z_1\\approx\\frac{1+e^{-2/C}}C,z_2\\approx\\frac{1-e^{-2/C}}C.$ Then $k_1-k_2=\\frac{2e^{-2/C}}{aC}=\\frac{2e^{-2amg/\\hbar^2}}{\\hbar^2/mg}=\\frac{2mge^{-2amg/\\hbar^2}}{\\hbar^2}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-2","text":"Skipped.","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-3","text":"Calculate probabilities that quantum SHO has position greater than amplitude in classical SHO for $\\mathcal E=1,3,5.$ We can work entirely in non-dimensional coordinates. We get the polynomials $1,x,x^2-\\frac12$ as solutions to $u''-2xu'+(\\mathcal E-1)u=0.$ The classical energy potential $V=x^2.$ Then, non-normalized we get - $\\psi=e^{-x^2/2}$ corresponding to $L=1.$ Get $\\approx 0.078649.$ - $\\psi=xe^{-x^2/2}$ corresponding to $L=\\sqrt 3.$ Get $\\approx 0.0558051.$ - $\\psi=(2x^2-1)e^{-x^2/2}$ corresponding to $L=\\sqrt 5.$ Get $\\approx 0.04753471774.$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-4","text":"Compute: (a) Expectation of $x^4$ on $n$th SHO eigenstate (b) $\\Delta x,\\Delta p,\\Delta x\\Delta p$ on $n$th SHO eigenstate Finally, (c) Check that $$e^{-s^2+2s\\xi}=\\sum_{n=0}H_n(\\xi)\\frac{s^n}{n!}$$ satisfies $H_n''-2\\xi H_n'+2nH_n=0.$ Recall that $\\hat x=(\\hat a+\\hat a^\\dagger)\\sqrt{\\frac{\\hbar}{2m\\omega}}.$ Then $$ \\begin{align } \\langle \\psi_n,\\hat x^4\\psi_n\\rangle &=\\langle \\hat x^2\\psi_n,\\hat x^2\\psi_n\\rangle\\ &=\\frac{\\hbar^2}{4m^2\\omega^2}\\langle(\\hat a^2+\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a+\\hat a^\\dagger\\hat a^\\dagger)\\psi_n,\\cdot \\rangle. \\end{align } $$ Note that $\\hat a$ is destruction, $\\hat a^\\dagger$ is construction so we can split by \"degree\". Reference L14-15, p7. $$\\langle\\hat a^2\\psi_n,\\cdot\\rangle=\\langle \\sqrt{n(n-1)}\\phi_{n-2},\\cdot\\rangle=n(n-1).$$ Similarly, $\\langle (\\hat a^\\dagger)^2\\psi_n,\\cdot\\rangle=(n+1)(n+2).$ Finally $(\\hat a\\hat a^\\dagger+\\hat a^\\dagger\\hat a)\\psi_n=(n+n+1)\\psi_n,$ so our final answer is $n(n-1)+(n+1)(n+2)+(2n+1)^2=6n^2+6n+3.$ Adding our units back in, we get an answer of $$(6n^2+6n+3)\\frac{\\hbar^2}{4m^2\\omega^2}.$$ (b) is similar. $(\\Delta x)^2=\\frac{\\hbar}{2m\\omega}(2n+1).$ Now $\\hat p=i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a).$ Then clearly $\\langle \\psi_n,\\hat p\\psi_n=0$ (also true from potential symmetry). Finally $$ \\begin{align } \\langle \\psi_n,\\hat p^2\\psi_n\\rangle &=-\\frac{m\\omega\\hbar}2\\langle\\psi_n,C_1\\psi_{n+2}+C_2\\psi_{n-2}-(2n+1)\\psi_n\\rangle\\ &=(2n+1)\\frac{m\\omega\\hbar}2. \\end{align } $$ Thus $\\Delta x\\Delta p=(n+\\frac12)\\hbar.$ (c) Taking the $s$-derivative of the whole generating function yields $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=1}H_n \\frac{s^{n-1}}{(n-1)!}=\\sum_{n=0}H_{n+1}\\frac{s^n}{n!}.$$ Then $$(2\\xi-2s)e^{-s^2+2s\\xi}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=0}2H_{n}\\frac{s^{n+1}}{n!}=\\sum_{n=0}2\\xi H_n\\frac{s^n}{n!}-\\sum_{n=1}2H_{n-1}n\\frac{s^n}{n!}.$$ Thus $$H_{n+1}=2\\xi H_n-2nH_{n-1}.$$ Taking the $\\xi$-derivative yields $$\\sum_{n=1}2nH_{n-1}\\frac{s^n}{n!}=2se^{-s^2+2s\\xi}=\\sum_{n=0}H_n'\\frac{s^n}{n!}.$$ Thus $2nH_{n-1}=H_n',$ so $$H_n''-2\\xi H_n'+2nH_n=4n(n-1)H_{n-2}-4\\xi nH_{n-1}+2nH_n=2n(H_n-2\\xi H_{n-1}+2(n-1)H_{n-2})=0.$$ Yay!","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-5","text":"Harmonic oscillator, except $V(x)=\\infty$ for $x<0$. What are the possible energy levels? Basically we need the polynomial solution to $H''-2xH+2nH=0$ to satisfy $H(0)=0.$ We established $H_{n+1}=2xH_n-2nH_{n-1},$ so $H_{n+1}(0)=-2nH_{n-1}(0).$ Since $H_0=1, H_1=x,$ we get $n$ must be odd so the possible energy levels are $\\mathcal E=4n+3.$ Re-dimensionalizing yields $E=(4n+3)\\hbar\\omega.$","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%207/#problem-6","text":"Given $V=m\\omega^2x^2,$ $\\Psi(x,0)=\\frac1{\\sqrt2}\\left(\\varphi_0(x)+\\varphi_1(x)\\right)$: - Find $\\Psi(x,t),|\\Psi(x,t)|^2$ - Find $\\langle x\\rangle(t).$ - Find $\\langle p\\rangle(t).$ - Show that for any such $\\Psi,$ $|\\Psi(x,t)|^2=|\\Psi(x,t+T)|^2$ where $T=\\frac{2\\pi}\\omega.$ $E(\\varphi_0)=\\hbar\\omega, E(\\varphi_1)=3\\hbar\\omega,$ so $$\\Psi(x,t)=\\frac1{\\sqrt2}(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1).$$ Then $$\\Psi\\Psi^*=\\frac12\\left(\\varphi_0^2+\\varphi_1^2+2\\varphi_0\\varphi_1\\cos(2t\\omega)\\right).$$ Then $\\langle x\\rangle=\\cos(2t\\omega)\\int x\\varphi_0\\varphi_1.$ Turns out that $\\hat x\\varphi_0=\\sqrt{\\frac\\hbar{2m\\omega}}(\\hat a+\\hat a^\\dagger)\\varphi_0=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\varphi_1.$ As a result $\\langle x\\rangle=\\cos(2t\\omega)\\sqrt{\\frac\\hbar{2m\\omega}}.$ Next, $$ \\begin{align } \\langle \\hat p\\rangle &=\\int \\Psi\\hat p\\Psi\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(\\hat a^\\dagger-\\hat a)(e^{it\\omega}\\varphi_0+e^{3it\\omega}\\varphi_1)\\ &=\\frac1{2}\\int(e^{-it\\omega}\\varphi_0+e^{-3it\\omega}\\varphi_1)i\\sqrt{\\frac{m\\omega\\hbar}2}(-e^{3it\\omega}\\varphi_0+e^{it\\omega}\\varphi_1)\\ &=\\frac i2\\sqrt{\\frac{m\\omega\\hbar}2}\\int -e^{2it\\omega}\\varphi_0^2+e^{-2it\\omega}\\varphi_1^2\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\frac1{2i}(e^{2it\\omega}-e^{-2it\\omega})\\ &=\\sqrt{\\frac{m\\omega\\hbar}2}\\sin(2t\\omega). \\end{align } $$ Finally, $E=N\\hbar\\omega,$ so all $e^{itE/\\hbar}$ terms are periodic in $t$ every $\\frac{2\\pi}{E/\\hbar}=\\frac{2\\pi}{N\\omega}.$ Thus if we increment $t$ by $\\frac{2\\pi}\\omega$ the value of $\\Psi$ is unchanged.","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/","text":"Problem 1 Explore the SHO state $\\psi_\\alpha\\equiv N\\text{exp}(\\alpha \\hat a^\\dagger)\\varphi_0.$ Equivalently, $$\\psi_\\alpha=N\\sum_{i=0}\\alpha^i (\\hat a^\\dagger)^i\\varphi_0\\frac1{i!}=N\\sum_{i=0}\\alpha^i \\varphi_i\\frac1{\\sqrt{i!}}.$$ Then $$1=|\\psi_\\alpha|^2=N^2\\sum_i \\alpha^{2i}\\frac1{i!}=Ne^{\\alpha^2/2}.$$ Thus $N=e^{-\\alpha^2/2}.$ Now $$\\hat a\\psi_\\alpha=N\\sum_{i=0}\\alpha^i \\varphi_{i-1}\\frac1{\\sqrt{(i-1)!}}=\\alpha\\psi_\\alpha.$$ Then $$ \\begin{align } \\hat H\\psi_\\alpha &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}E_i\\ &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}(i+\\frac12)\\hbar\\omega\\ &=e^{-\\alpha^2/2}\\left(\\sum_{i=1}\\alpha^i\\varphi_i\\frac{\\sqrt i}{\\sqrt{(i-1)!}}+\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{2\\sqrt{i!}}\\right), \\end{align } $$ so $$\\langle\\hat H\\rangle=e^{-\\alpha^2}\\left(\\alpha^{}e^{\\alpha^2}+\\frac12e^{\\alpha^2}\\right)=\\alpha+\\frac12.$$ Also, $$\\langle\\hat H^2\\rangle =e^{-\\alpha^2}\\sum_{i=0}\\alpha^i\\frac{(i+\\frac12)^2}{i!}.$$ Then $(i+\\frac12)^2=i^2+i+\\frac14=i(i-1)+2i+\\frac14,$ so we get $$\\langle\\hat H^2\\rangle=e^{-\\alpha^2}\\left(\\alpha^2e^{\\alpha^2}+2\\alpha e^{\\alpha^2}+\\frac14 e^{\\alpha^2}\\right)=\\alpha^2+2\\alpha+\\frac14.$$ Then $(\\Delta\\hat H)^2=\\alpha^2+2\\alpha+\\frac14-(\\alpha^2+\\alpha+\\frac14)=\\alpha.$ Finally, we can compute $\\psi_\\alpha$. Abbreviating as $f$ for ease, $$\\alpha f=\\hat af=(\\sqrt{\\frac{m\\omega}{2\\hbar}})(x+\\frac{\\hbar}{m\\omega}\\frac{\\partial}{\\partial x})f.$$ Nondimensionalizing with $L=\\sqrt{\\frac{\\hbar}{m\\omega}}$ yields $$\\alpha f=\\frac1{\\sqrt2}(x+\\frac{\\partial}{\\partial x})f.$$ Taking $f=e^{-x^2/2}h$ yields $\\alpha h=\\frac1{\\sqrt 2}h',$ so $h=e^{\\alpha x\\sqrt 2}.$ Thus before normalization $\\psi_\\alpha=e^{-x^2/2+\\alpha x\\sqrt 2}.$ Problem 2 Analyze double-delta potential, again. Use an extra $V_r(x)=\\frac{\\beta g}{x}$ for some small $\\beta>0$. Exercise in non-dimensionalization (sorta)! Use $E_0=\\frac{mg^2}{2\\hbar^2}.$ Recall that $\\xi=\\kappa a,$ where $E/E_0=-\\xi^2,$ $\\frac\\xi{1+e^{-2\\xi}}=\\lambda,$ $\\lambda\\equiv\\frac{mag}{\\hbar^2}$ (recall $g$ is Joules). See that $E=-E_0\\xi^2.$ Also, $V_r=\\frac{\\beta g}{2a}=\\frac{\\beta E_0}{\\lambda}.$ Then we can plot $E_{tot}=E+V_r=E_0(\\frac{\\beta}{\\lambda}-\\xi^2).$ For $\\beta=0.31,$ we can find local minima for $E_{tot}$ given $\\lambda.$ That yields the optimal value for $a,$ distance between two $H^+$ ions! Problem 3 Analyze the energy in the forbidden region of the finite square well as it limits towards the infinite square well. From a previous analysis we've determined that as $z_0^2=\\frac{2ma^2V}{\\hbar^2}$ limits to $\\infty$ equivalent to $V$ limit to $\\infty$. In this limit, $\\eta=ka\\sim\\frac\\pi2(1-\\frac1{z_0}),$ $\\xi=\\kappa a\\sim z_0,$ and $A$ the constant factor on the outer exponential satisfies $A\\sim \\frac\\pi{2z_0}e^{z_0}.$ What is $\\langle\\hat K\\rangle$ from the forbidden region $x>a,$ given $K=\\frac{\\hat p^2}{2m}=-\\frac{\\hbar^2\\partial^2}{2m\\partial x^2}?$ In this region we have $\\psi=\\frac A{\\sqrt a}e^{-\\kappa x}.$ Then we just get $$ \\begin{align } \\int_a^\\infty \\frac{A^2}{a}e^{-2\\kappa x}\\left(-\\frac{\\hbar^2\\kappa^2}{2m}\\right) &=\\frac{A^2\\hbar^2\\kappa}{4am}e^{-2\\kappa a}\\ &\\approx\\frac{A^2\\hbar^2z_0}{4a^2m}e^{-2z_0}\\ &\\approx\\frac{\\pi^2\\hbar^2}{16a^2mz_0}. \\end{align } $$ Units check out. As $z_0$ limits to $\\infty$ this contribution limits to $0$. yay! Problem 4 Consider step potential $V=0$ for $x<0,$ $V=V_0$ else. Analyze time delay, using a Gaussian wavepacket distribution $\\Phi$ with width $\\beta$. First let's analyze scattering time delay. As computed earlier for incoming $1$ we get $A=\\frac{\\kappa+ik}{\\kappa-ik},$ so letting $\\tan(\\delta)=\\frac k{\\kappa}$ means $e^{2i\\delta}=-A.$ Now we use $\\Phi(k)=e^{-\\beta^2a^2(k-k_0)^2},$ $a=\\frac{\\hbar}{\\sqrt{mV_0}}.$ The incoming wave is $$\\Psi_{inc}(x)=\\sqrt a\\int_0^{\\hat k} \\Phi(k)e^{ikx}e^{-iE(k)t/\\hbar},$$ so the outgoing wave is $$\\Psi_{out}(x)=-\\sqrt a\\int_0^\\hat k\\Phi(k)e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar},$$ where $\\delta(E)=\\arctan(\\frac k\\kappa)$ as derived above. After dividing by $2i$ we can rewrite the sum $-e^{2i\\delta(E(k))-ikx}+e^{ikx}$ as $e^{i\\delta(E)}\\sin(kx-\\delta).$ Introduce nondimensionalized $k=\\frac Ka,$ $x=au,$ and $t=\\frac\\hbar{V_0}\\tau.$ Naturally get $K_0$ and $kx=Ku.$ Let's look at the group velocity. For given $(u,\\tau)$ we need the phase derivative in $K$ at $K_0$ to be $0$. The phase is $$kx-E(k)t/\\hbar=Ku-\\frac{t\\hbar k^2}{2m}=Ku-\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau.$$ Then we need $u=\\frac{\\hbar^2}{V_0a^2m}\\tau K_0,$ i.e. $\\frac{du}{d\\tau}=\\frac{\\hbar^2}{V_0 a^2m}K_0=\\frac{\\hbar^2 k_0}{V_0am}.$ Recall now that $\\Delta x\\Delta k=\\Delta u\\Delta K\\geq \\frac12.$ Our distribution for $K$ is now $\\Phi(K)=e^{-\\beta^2(K-K_0)^2},$ so the uncertainty $\\Delta K$ is known to be $\\frac 1{2\\beta^2}$ (simple-ish integral by parts) so $\\Delta u\\geq \\beta^2.$ Continuing the non-dimensionalization, $E(k)=\\frac{\\hbar^2 k^2}{2m}=\\frac{\\hbar^2}{2a^2m}K^2=\\frac{\\hbar^2}{2a^2mV_0} V_0 K^2.$ First coeff is unitless. Furthermore, $e^{2i\\delta(E)}=\\frac{\\kappa+ik}{\\kappa-ik}=((\\kappa^2-k^2)+2k\\kappa i)\\div(\\kappa^2+k^2).$ Then $\\kappa=\\sqrt{\\frac{(V_0-E(K))}{\\hbar^2}},$ and $k=\\sqrt{\\frac{2mE}{\\hbar^2}}=\\frac Ka.$ Plugging all this in yields $$e^{2i\\delta(E)}=1-\\frac{\\hbar^2}{a^2mV_0}K^2+K\\sqrt{\\frac{2\\hbar^2}{a^2mV_0}-\\frac{2\\hbar^4}{a^4m^2V_0}K^2}\\equiv\\omega(K).$$ Then $\\Delta t=2\\hbar\\delta'(E)$ as a derivative in $k$ yields $\\Delta \\tau=\\frac{V_0}\\hbar \\Delta t=2V_0\\delta'(E).$ Then $\\delta=\\arctan(\\sqrt{\\frac{E}{V_0-E}}).$ Recall that $\\frac d{dx}\\arctan x=\\frac1{x^2+1},$ so $\\delta'(E)=\\frac1{2\\sqrt{E(V_0-E)}}.$ Then $$ \\Delta \\tau =\\frac{V_0}{\\sqrt{E(V_0-E)}} =\\frac{1}{\\sqrt{E/V_0\\cdot(1-E/V_0)}}. $$ Recall that $E/V_0=\\frac{\\hbar^2}{2a^2mV_0}K^2,$ and $\\Delta\\tau$ is evaluated at $K=K_0$ so we get $$\\Delta\\tau=\\frac1{K_0\\sqrt{\\frac{\\hbar^2}{2a^2mV_0}(1-\\frac{\\hbar^2}{2a^2mV_0}K_0^2)}}=\\frac{2a^2mV_0}{K_0\\sqrt{2a^2mV_0\\hbar^2-K_0^2}}.$$ We've computed several uncertainties and the time delay. Now let's look at the wavefunction again. $$ \\begin{align } a^{\\frac12}\\Psi(u,\\tau) &=a\\int_0^{\\hat k}\\Phi(k)\\left(e^{ikx}e^{-iE(k)t/\\hbar}-e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar}\\right)dk\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}\\Phi(K)\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}e^{-\\beta^2(K-K_0)^2}\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK. \\end{align } $$ yay!! we're done!! Problem 5 Consider same step potential. Compute reflection/transmission coefficients, and comment. For $E<V_0,$ as usual set $k^2=\\frac{2mE}{\\hbar^2},$ and $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2}.$ Then with transmission $1,$ reflection $A$ transmitted $B$ we get $A+1=B,$ and $\\frac{ik-ikA}{A+1}=-\\kappa.$ As a result $|A|=1$ so reflection coefficient is $0$. For $E>V_0$ set $k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ With the same setup we get $A+1=B$ and $\\frac{ik-ikA}{1+A}=ik_2$ so $A=\\frac{k-k_2}{k+k_2},$ and $B=\\frac{2k}{k+k_2}.$ But since the wavelength changed, we actually have $T={\\frac{k_2}k}\\frac{B^2}{1^2}=\\frac{4kk_2}{(k_1+k_2)^2}.$ Wondrously, $T+A^2=\\frac{4kk_2+k^2-2kk_2+k_2^2}{k^2+2kk_2+k_2^2}=1$. We only get perfect transmission in the limit $E\\to\\infty.$ Higher $E$, better $T$. O and BTW, for $E=V_0$ we get $k_2=0$ so $A=1$ and $B=2$ a constant flat thing. Perfect reflection.","title":"8.04.15 Pset 8"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-1","text":"Explore the SHO state $\\psi_\\alpha\\equiv N\\text{exp}(\\alpha \\hat a^\\dagger)\\varphi_0.$ Equivalently, $$\\psi_\\alpha=N\\sum_{i=0}\\alpha^i (\\hat a^\\dagger)^i\\varphi_0\\frac1{i!}=N\\sum_{i=0}\\alpha^i \\varphi_i\\frac1{\\sqrt{i!}}.$$ Then $$1=|\\psi_\\alpha|^2=N^2\\sum_i \\alpha^{2i}\\frac1{i!}=Ne^{\\alpha^2/2}.$$ Thus $N=e^{-\\alpha^2/2}.$ Now $$\\hat a\\psi_\\alpha=N\\sum_{i=0}\\alpha^i \\varphi_{i-1}\\frac1{\\sqrt{(i-1)!}}=\\alpha\\psi_\\alpha.$$ Then $$ \\begin{align } \\hat H\\psi_\\alpha &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}E_i\\ &=N\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{\\sqrt{i!}}(i+\\frac12)\\hbar\\omega\\ &=e^{-\\alpha^2/2}\\left(\\sum_{i=1}\\alpha^i\\varphi_i\\frac{\\sqrt i}{\\sqrt{(i-1)!}}+\\sum_{i=0}\\alpha^i\\varphi_i\\frac1{2\\sqrt{i!}}\\right), \\end{align } $$ so $$\\langle\\hat H\\rangle=e^{-\\alpha^2}\\left(\\alpha^{}e^{\\alpha^2}+\\frac12e^{\\alpha^2}\\right)=\\alpha+\\frac12.$$ Also, $$\\langle\\hat H^2\\rangle =e^{-\\alpha^2}\\sum_{i=0}\\alpha^i\\frac{(i+\\frac12)^2}{i!}.$$ Then $(i+\\frac12)^2=i^2+i+\\frac14=i(i-1)+2i+\\frac14,$ so we get $$\\langle\\hat H^2\\rangle=e^{-\\alpha^2}\\left(\\alpha^2e^{\\alpha^2}+2\\alpha e^{\\alpha^2}+\\frac14 e^{\\alpha^2}\\right)=\\alpha^2+2\\alpha+\\frac14.$$ Then $(\\Delta\\hat H)^2=\\alpha^2+2\\alpha+\\frac14-(\\alpha^2+\\alpha+\\frac14)=\\alpha.$ Finally, we can compute $\\psi_\\alpha$. Abbreviating as $f$ for ease, $$\\alpha f=\\hat af=(\\sqrt{\\frac{m\\omega}{2\\hbar}})(x+\\frac{\\hbar}{m\\omega}\\frac{\\partial}{\\partial x})f.$$ Nondimensionalizing with $L=\\sqrt{\\frac{\\hbar}{m\\omega}}$ yields $$\\alpha f=\\frac1{\\sqrt2}(x+\\frac{\\partial}{\\partial x})f.$$ Taking $f=e^{-x^2/2}h$ yields $\\alpha h=\\frac1{\\sqrt 2}h',$ so $h=e^{\\alpha x\\sqrt 2}.$ Thus before normalization $\\psi_\\alpha=e^{-x^2/2+\\alpha x\\sqrt 2}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-2","text":"Analyze double-delta potential, again. Use an extra $V_r(x)=\\frac{\\beta g}{x}$ for some small $\\beta>0$. Exercise in non-dimensionalization (sorta)! Use $E_0=\\frac{mg^2}{2\\hbar^2}.$ Recall that $\\xi=\\kappa a,$ where $E/E_0=-\\xi^2,$ $\\frac\\xi{1+e^{-2\\xi}}=\\lambda,$ $\\lambda\\equiv\\frac{mag}{\\hbar^2}$ (recall $g$ is Joules). See that $E=-E_0\\xi^2.$ Also, $V_r=\\frac{\\beta g}{2a}=\\frac{\\beta E_0}{\\lambda}.$ Then we can plot $E_{tot}=E+V_r=E_0(\\frac{\\beta}{\\lambda}-\\xi^2).$ For $\\beta=0.31,$ we can find local minima for $E_{tot}$ given $\\lambda.$ That yields the optimal value for $a,$ distance between two $H^+$ ions!","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-3","text":"Analyze the energy in the forbidden region of the finite square well as it limits towards the infinite square well. From a previous analysis we've determined that as $z_0^2=\\frac{2ma^2V}{\\hbar^2}$ limits to $\\infty$ equivalent to $V$ limit to $\\infty$. In this limit, $\\eta=ka\\sim\\frac\\pi2(1-\\frac1{z_0}),$ $\\xi=\\kappa a\\sim z_0,$ and $A$ the constant factor on the outer exponential satisfies $A\\sim \\frac\\pi{2z_0}e^{z_0}.$ What is $\\langle\\hat K\\rangle$ from the forbidden region $x>a,$ given $K=\\frac{\\hat p^2}{2m}=-\\frac{\\hbar^2\\partial^2}{2m\\partial x^2}?$ In this region we have $\\psi=\\frac A{\\sqrt a}e^{-\\kappa x}.$ Then we just get $$ \\begin{align } \\int_a^\\infty \\frac{A^2}{a}e^{-2\\kappa x}\\left(-\\frac{\\hbar^2\\kappa^2}{2m}\\right) &=\\frac{A^2\\hbar^2\\kappa}{4am}e^{-2\\kappa a}\\ &\\approx\\frac{A^2\\hbar^2z_0}{4a^2m}e^{-2z_0}\\ &\\approx\\frac{\\pi^2\\hbar^2}{16a^2mz_0}. \\end{align } $$ Units check out. As $z_0$ limits to $\\infty$ this contribution limits to $0$. yay!","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-4","text":"Consider step potential $V=0$ for $x<0,$ $V=V_0$ else. Analyze time delay, using a Gaussian wavepacket distribution $\\Phi$ with width $\\beta$. First let's analyze scattering time delay. As computed earlier for incoming $1$ we get $A=\\frac{\\kappa+ik}{\\kappa-ik},$ so letting $\\tan(\\delta)=\\frac k{\\kappa}$ means $e^{2i\\delta}=-A.$ Now we use $\\Phi(k)=e^{-\\beta^2a^2(k-k_0)^2},$ $a=\\frac{\\hbar}{\\sqrt{mV_0}}.$ The incoming wave is $$\\Psi_{inc}(x)=\\sqrt a\\int_0^{\\hat k} \\Phi(k)e^{ikx}e^{-iE(k)t/\\hbar},$$ so the outgoing wave is $$\\Psi_{out}(x)=-\\sqrt a\\int_0^\\hat k\\Phi(k)e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar},$$ where $\\delta(E)=\\arctan(\\frac k\\kappa)$ as derived above. After dividing by $2i$ we can rewrite the sum $-e^{2i\\delta(E(k))-ikx}+e^{ikx}$ as $e^{i\\delta(E)}\\sin(kx-\\delta).$ Introduce nondimensionalized $k=\\frac Ka,$ $x=au,$ and $t=\\frac\\hbar{V_0}\\tau.$ Naturally get $K_0$ and $kx=Ku.$ Let's look at the group velocity. For given $(u,\\tau)$ we need the phase derivative in $K$ at $K_0$ to be $0$. The phase is $$kx-E(k)t/\\hbar=Ku-\\frac{t\\hbar k^2}{2m}=Ku-\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau.$$ Then we need $u=\\frac{\\hbar^2}{V_0a^2m}\\tau K_0,$ i.e. $\\frac{du}{d\\tau}=\\frac{\\hbar^2}{V_0 a^2m}K_0=\\frac{\\hbar^2 k_0}{V_0am}.$ Recall now that $\\Delta x\\Delta k=\\Delta u\\Delta K\\geq \\frac12.$ Our distribution for $K$ is now $\\Phi(K)=e^{-\\beta^2(K-K_0)^2},$ so the uncertainty $\\Delta K$ is known to be $\\frac 1{2\\beta^2}$ (simple-ish integral by parts) so $\\Delta u\\geq \\beta^2.$ Continuing the non-dimensionalization, $E(k)=\\frac{\\hbar^2 k^2}{2m}=\\frac{\\hbar^2}{2a^2m}K^2=\\frac{\\hbar^2}{2a^2mV_0} V_0 K^2.$ First coeff is unitless. Furthermore, $e^{2i\\delta(E)}=\\frac{\\kappa+ik}{\\kappa-ik}=((\\kappa^2-k^2)+2k\\kappa i)\\div(\\kappa^2+k^2).$ Then $\\kappa=\\sqrt{\\frac{(V_0-E(K))}{\\hbar^2}},$ and $k=\\sqrt{\\frac{2mE}{\\hbar^2}}=\\frac Ka.$ Plugging all this in yields $$e^{2i\\delta(E)}=1-\\frac{\\hbar^2}{a^2mV_0}K^2+K\\sqrt{\\frac{2\\hbar^2}{a^2mV_0}-\\frac{2\\hbar^4}{a^4m^2V_0}K^2}\\equiv\\omega(K).$$ Then $\\Delta t=2\\hbar\\delta'(E)$ as a derivative in $k$ yields $\\Delta \\tau=\\frac{V_0}\\hbar \\Delta t=2V_0\\delta'(E).$ Then $\\delta=\\arctan(\\sqrt{\\frac{E}{V_0-E}}).$ Recall that $\\frac d{dx}\\arctan x=\\frac1{x^2+1},$ so $\\delta'(E)=\\frac1{2\\sqrt{E(V_0-E)}}.$ Then $$ \\Delta \\tau =\\frac{V_0}{\\sqrt{E(V_0-E)}} =\\frac{1}{\\sqrt{E/V_0\\cdot(1-E/V_0)}}. $$ Recall that $E/V_0=\\frac{\\hbar^2}{2a^2mV_0}K^2,$ and $\\Delta\\tau$ is evaluated at $K=K_0$ so we get $$\\Delta\\tau=\\frac1{K_0\\sqrt{\\frac{\\hbar^2}{2a^2mV_0}(1-\\frac{\\hbar^2}{2a^2mV_0}K_0^2)}}=\\frac{2a^2mV_0}{K_0\\sqrt{2a^2mV_0\\hbar^2-K_0^2}}.$$ We've computed several uncertainties and the time delay. Now let's look at the wavefunction again. $$ \\begin{align } a^{\\frac12}\\Psi(u,\\tau) &=a\\int_0^{\\hat k}\\Phi(k)\\left(e^{ikx}e^{-iE(k)t/\\hbar}-e^{2i\\delta(E(k))-ikx}e^{-iE(k)t/\\hbar}\\right)dk\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}\\Phi(K)\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK\\ &=\\int_0^{\\sqrt{\\frac{2a^2mV_0}{\\hbar^2}}}e^{-\\beta^2(K-K_0)^2}\\left(e^{-i\\frac{\\hbar^2}{2V_0a^2m}K^2\\tau}\\right)\\left(e^{iKu}-\\omega(K)e^{-iKu}\\right)dK. \\end{align } $$ yay!! we're done!!","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Pset%208/#problem-5","text":"Consider same step potential. Compute reflection/transmission coefficients, and comment. For $E<V_0,$ as usual set $k^2=\\frac{2mE}{\\hbar^2},$ and $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2}.$ Then with transmission $1,$ reflection $A$ transmitted $B$ we get $A+1=B,$ and $\\frac{ik-ikA}{A+1}=-\\kappa.$ As a result $|A|=1$ so reflection coefficient is $0$. For $E>V_0$ set $k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ With the same setup we get $A+1=B$ and $\\frac{ik-ikA}{1+A}=ik_2$ so $A=\\frac{k-k_2}{k+k_2},$ and $B=\\frac{2k}{k+k_2}.$ But since the wavelength changed, we actually have $T={\\frac{k_2}k}\\frac{B^2}{1^2}=\\frac{4kk_2}{(k_1+k_2)^2}.$ Wondrously, $T+A^2=\\frac{4kk_2+k^2-2kk_2+k_2^2}{k^2+2kk_2+k_2^2}=1$. We only get perfect transmission in the limit $E\\to\\infty.$ Higher $E$, better $T$. O and BTW, for $E=V_0$ we get $k_2=0$ so $A=1$ and $B=2$ a constant flat thing. Perfect reflection.","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/","text":"Resonance Scattering state: Finite square well with incoming wave from left side. Then we have inc., refl. on left side, two oscillating in the well, and a single transmitted from the right. Specifically, $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ Solving in terms of $A$ by matching $\\Psi,\\Psi'$ at $x=\\pm a.$ As usual we have $k^2=\\frac{2mE}{\\hbar^2},$ $k_2^2=\\frac{2m(E+V_0)}{\\hbar^2}.$ As a result we get $$\\frac1T=\\frac{|A|^2}{|F|^2}=1+\\frac14\\frac{V^2}{E^2+EV}\\sin^2(2k_2a).$$ Evidently for \"arbitrary\" $E,K_2$ we get $T\\in(0,1)$ and also $T\\to 0$ as $E\\to 0,$ $T\\to\\infty$ as $E\\to\\infty$. But if we want perfect $T=1$ we can fix $\\sin(2k_2a)=0,$ i.e. the frequency $k_2$ fits itself perfectly within the well width $2a$. More specifically, given that $2k_2a=n\\pi,$ we can compute $$E+V=\\frac{n^2\\pi^2\\hbar^2}{2m(2a)^2,}$$ the same as the energy for the $n$th bound state in the inf. square well! This was first seen in the real world in the Ramsauer-Townsend effect, where the number of reflections when electrons were fired at a wall of noble gases had unexpected troughs regularly. Half-Domain Scattering We consider scattering states with potentials with an infinite wall at $x=0$ and $V=0$ for $x>r$. These are instigated by an incoming wave from $x=\\infty,$ so $x=e^{-ikx}$ for the usual $k^2=\\frac{2mE}{\\hbar^2}.$ In the case that $V=0$ for all $x>0$ the only resolution to $\\Psi(0)=0$ is $\\sin(kx),$ so we'll slap $\\frac1{2i}$ on everything as convention. For general $V$ let the outgoing wave be $e^{ikx+i2\\delta}$ for some $\\delta(x).$ For $x>r$ we actually need $\\delta$ constant. Also $\\delta$ must be real because of how probability flux works. Then we can write $$\\Psi(x)=\\frac1{2i}\\left(e^{ikx+2i\\delta}-e^{-ikx}\\right)=e^{i\\delta}\\sin(kx+\\delta).$$ Time Delay If we examine a wavepacket constructed from $g(k)$ then $$\\Psi_{inc}=\\int g(k)(e^{-ikx})(e^{-iE(k)t/\\hbar}).$$ The reflected wavepacket is $$\\Psi_{ref}=\\int g(k)(e^{ikx+2i\\delta(k)})(e^{-iE(k)t/\\hbar}).$$ Since $\\frac{\\partial P}{\\partial k}=0$ at the crest, we get $$x+2\\frac{\\partial\\delta}{\\partial k}-\\frac1\\hbar\\frac{\\partial E}{\\partial k}t=0$$ evaluated at $k=k_0.$ Pulling out a $\\frac{\\partial E}{\\partial k}=\\frac{\\hbar^2k}{m}$ from $\\frac{\\partial\\delta}{\\partial k}$ we get $$x=\\frac{\\hbar k_0}{m}(t-2\\hbar\\delta'(E(k_0))).$$ As a result the time delay is $$\\Delta t=2\\hbar\\delta'(E(k_0))$$ (with $\\delta$ as a function of $E$ ofc). Example Consider constant potential for the nonzero region. Then ansatz is $A\\sin(k_2x),Be^{i\\delta}\\sin(kx+\\delta)$ piecewise. Notice $\\psi(0)=0$ is naturally handled. Solve this for $\\delta.$","title":"8.04.15 Resonance, Half Domain Scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#resonance","text":"Scattering state: Finite square well with incoming wave from left side. Then we have inc., refl. on left side, two oscillating in the well, and a single transmitted from the right. Specifically, $Ae^{ikx}+Be^{-ikx}$ for $x<-a,$ $Ce^{ik_2x}+De^{-ik_2x}$ for $|x| a.$ Solving in terms of $A$ by matching $\\Psi,\\Psi'$ at $x=\\pm a.$ As usual we have $k^2=\\frac{2mE}{\\hbar^2},$ $k_2^2=\\frac{2m(E+V_0)}{\\hbar^2}.$ As a result we get $$\\frac1T=\\frac{|A|^2}{|F|^2}=1+\\frac14\\frac{V^2}{E^2+EV}\\sin^2(2k_2a).$$ Evidently for \"arbitrary\" $E,K_2$ we get $T\\in(0,1)$ and also $T\\to 0$ as $E\\to 0,$ $T\\to\\infty$ as $E\\to\\infty$. But if we want perfect $T=1$ we can fix $\\sin(2k_2a)=0,$ i.e. the frequency $k_2$ fits itself perfectly within the well width $2a$. More specifically, given that $2k_2a=n\\pi,$ we can compute $$E+V=\\frac{n^2\\pi^2\\hbar^2}{2m(2a)^2,}$$ the same as the energy for the $n$th bound state in the inf. square well! This was first seen in the real world in the Ramsauer-Townsend effect, where the number of reflections when electrons were fired at a wall of noble gases had unexpected troughs regularly.","title":"Resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#half-domain-scattering","text":"We consider scattering states with potentials with an infinite wall at $x=0$ and $V=0$ for $x>r$. These are instigated by an incoming wave from $x=\\infty,$ so $x=e^{-ikx}$ for the usual $k^2=\\frac{2mE}{\\hbar^2}.$ In the case that $V=0$ for all $x>0$ the only resolution to $\\Psi(0)=0$ is $\\sin(kx),$ so we'll slap $\\frac1{2i}$ on everything as convention. For general $V$ let the outgoing wave be $e^{ikx+i2\\delta}$ for some $\\delta(x).$ For $x>r$ we actually need $\\delta$ constant. Also $\\delta$ must be real because of how probability flux works. Then we can write $$\\Psi(x)=\\frac1{2i}\\left(e^{ikx+2i\\delta}-e^{-ikx}\\right)=e^{i\\delta}\\sin(kx+\\delta).$$","title":"Half-Domain Scattering"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#time-delay","text":"If we examine a wavepacket constructed from $g(k)$ then $$\\Psi_{inc}=\\int g(k)(e^{-ikx})(e^{-iE(k)t/\\hbar}).$$ The reflected wavepacket is $$\\Psi_{ref}=\\int g(k)(e^{ikx+2i\\delta(k)})(e^{-iE(k)t/\\hbar}).$$ Since $\\frac{\\partial P}{\\partial k}=0$ at the crest, we get $$x+2\\frac{\\partial\\delta}{\\partial k}-\\frac1\\hbar\\frac{\\partial E}{\\partial k}t=0$$ evaluated at $k=k_0.$ Pulling out a $\\frac{\\partial E}{\\partial k}=\\frac{\\hbar^2k}{m}$ from $\\frac{\\partial\\delta}{\\partial k}$ we get $$x=\\frac{\\hbar k_0}{m}(t-2\\hbar\\delta'(E(k_0))).$$ As a result the time delay is $$\\Delta t=2\\hbar\\delta'(E(k_0))$$ (with $\\delta$ as a function of $E$ ofc).","title":"Time Delay"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.15%20Resonance%2C%20Half-Domain%20Scattering/#example","text":"Consider constant potential for the nonzero region. Then ansatz is $A\\sin(k_2x),Be^{i\\delta}\\sin(kx+\\delta)$ piecewise. Notice $\\psi(0)=0$ is naturally handled. Solve this for $\\delta.$","title":"Example"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/","text":"Context here: we're dealing just with the infinite wall, incoming wave, 1D potential thing. Levinson's What's the number of (countable) bound states for a potential? (Recall scattering states are infinite.) Let's take a second infinite wall whose distance $L$ limits to $\\infty$. We have $V$ which ends at $R$. Consider $V=0,$ then we got $\\sin(kx)$ coming in, the second wall creates $kL=n\\pi.$ Then for infinitesimal interval $dk$ we got $dn=\\frac L\\pi dk$ possibilities for $n$. For $V\\neq 0,$ we got $e^{i\\delta}\\sin(kx+\\delta)$ coming in, so BC yields $kL+\\delta=n'\\pi.$ Then same thing, $Ldk+\\frac{d\\delta}{d k}dk=\\pi dn'.$ Then $dn'=\\frac L\\pi dk + \\frac1\\pi \\frac{d\\delta}{dk}dk,$ so turning on $V$ costs us $dn-dn'=-\\frac1\\pi\\frac{d\\delta}{dk}dk$ bound states, over the interval $dk$. Then integrating over all $k$, we get a loss of $\\frac 1\\pi (\\delta(0)-\\delta(\\infty)).$ But as we slowly turn on $V$ we can't lose states, so all of these \"lost\" states become negative energy -> bound states! thonk. Modeling Resonance Clearly $E\\in (0,V_1).$ We want resonant, i.e. large amplitude in potential and large time delay. Recall $\\Delta t=\\frac{d\\delta}{dE},$ or something. We get bumps when $\\delta$ crosses $\\frac{n\\pi}2,$ cuz that's when stuff is \"resonant,\" I suppose. Waves matching with each other. Let's have a double-step potential, $V=-V_0$ for $x<a,$ $V=V_1$ for $x<2a,$ $V=0$ otherwise. Solve with a cute trick using $\\cosh(x-a),\\sinh(x-a)$ in the middle interval. ![[Pasted image 20240107222205.png]] We want large resonance, so we want $\\delta$ to quickly cross from $0$ to $\\pi$, so large derivative at $\\delta=\\frac\\pi 2$. We can achieve this with $$\\delta=\\tan^{-1}\\left(\\frac{\\alpha\\varepsilon}{\\alpha-k}\\right).$$ Using $\\beta=\\alpha\\varepsilon,$ ![[Pasted image 20240107222729.png]] We then calculate $\\Delta t\\sim \\frac1\\beta$ (using $\\frac{d\\delta}{dk}$ as heuristic) and our amplitude heuristic $|\\psi_s|^2=\\sin\\delta^2=\\frac{\\beta^2}{\\beta^2+(\\alpha-k)^2}.$ At $k\\sim \\alpha,$ we get $|\\psi_s|^2=\\frac{\\frac14\\Gamma^2}{(E-E_\\alpha)^2+\\frac14\\Gamma^2}$ where $\\Gamma=\\frac{2\\alpha\\beta\\hbar^2}{m}.$ Here $\\psi_s$ is basically a measure of scattering resonance sm. $\\Gamma$ is a measure of the width of the distribution of the wavefunction amplitude in the potential as a function of $k$. Taking the time $\\tau=\\frac\\hbar\\Gamma,$ we can evaluate $\\Delta t=4\\tau.$ Let's go a little wild. Take the $x>R$ wavefunction $e^{i\\delta}\\sin(kx+\\delta),$ so we care about $A_s=e^{i\\delta}\\sin\\delta.$ $A_s$ here is scattering amplitude. Then $A_s=\\frac{\\tan\\delta}{1-i\\tan\\delta}.$ So weirdly enough we'd like $\\tan\\delta=-i.$ Notice that our $|A_s|=1$ from earlier is just the best we can do with real $\\delta$, $\\delta\\to\\frac\\pi 2,$ i.e. $\\tan\\delta\\to\\infty.$ Now, continuing with our weird example... If we sub stuff back in we get $A_s=\\frac{\\frac{\\beta}{\\alpha-k}}{1-i\\frac{\\beta}{\\alpha-k}}=\\frac{\\beta}{(a-i\\beta)-k}.$ Then we want $k$ to be as close to $a-i\\beta$ as possible. If we allow complex $k$... we get arbitrarily big resonance at $\\tan(\\delta(k))=-i.$ Here, the imaginary part $\\beta$ refers us to the half-life (somehow, didnt get to that,) and the positive-imaginary-part poles refer us to the bound states! (somehow?? $E\\sim k^2<0,$ I suppose) so goes physics ![[Pasted image 20240107223209.png]]","title":"8.04.16 Levinson's, Resonance, Complex k plane"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/#levinsons","text":"What's the number of (countable) bound states for a potential? (Recall scattering states are infinite.) Let's take a second infinite wall whose distance $L$ limits to $\\infty$. We have $V$ which ends at $R$. Consider $V=0,$ then we got $\\sin(kx)$ coming in, the second wall creates $kL=n\\pi.$ Then for infinitesimal interval $dk$ we got $dn=\\frac L\\pi dk$ possibilities for $n$. For $V\\neq 0,$ we got $e^{i\\delta}\\sin(kx+\\delta)$ coming in, so BC yields $kL+\\delta=n'\\pi.$ Then same thing, $Ldk+\\frac{d\\delta}{d k}dk=\\pi dn'.$ Then $dn'=\\frac L\\pi dk + \\frac1\\pi \\frac{d\\delta}{dk}dk,$ so turning on $V$ costs us $dn-dn'=-\\frac1\\pi\\frac{d\\delta}{dk}dk$ bound states, over the interval $dk$. Then integrating over all $k$, we get a loss of $\\frac 1\\pi (\\delta(0)-\\delta(\\infty)).$ But as we slowly turn on $V$ we can't lose states, so all of these \"lost\" states become negative energy -> bound states! thonk.","title":"Levinson's"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.16%20Levinson%27s%2C%20Resonance%2C%20Complex%20k-plane/#modeling-resonance","text":"Clearly $E\\in (0,V_1).$ We want resonant, i.e. large amplitude in potential and large time delay. Recall $\\Delta t=\\frac{d\\delta}{dE},$ or something. We get bumps when $\\delta$ crosses $\\frac{n\\pi}2,$ cuz that's when stuff is \"resonant,\" I suppose. Waves matching with each other. Let's have a double-step potential, $V=-V_0$ for $x<a,$ $V=V_1$ for $x<2a,$ $V=0$ otherwise. Solve with a cute trick using $\\cosh(x-a),\\sinh(x-a)$ in the middle interval. ![[Pasted image 20240107222205.png]] We want large resonance, so we want $\\delta$ to quickly cross from $0$ to $\\pi$, so large derivative at $\\delta=\\frac\\pi 2$. We can achieve this with $$\\delta=\\tan^{-1}\\left(\\frac{\\alpha\\varepsilon}{\\alpha-k}\\right).$$ Using $\\beta=\\alpha\\varepsilon,$ ![[Pasted image 20240107222729.png]] We then calculate $\\Delta t\\sim \\frac1\\beta$ (using $\\frac{d\\delta}{dk}$ as heuristic) and our amplitude heuristic $|\\psi_s|^2=\\sin\\delta^2=\\frac{\\beta^2}{\\beta^2+(\\alpha-k)^2}.$ At $k\\sim \\alpha,$ we get $|\\psi_s|^2=\\frac{\\frac14\\Gamma^2}{(E-E_\\alpha)^2+\\frac14\\Gamma^2}$ where $\\Gamma=\\frac{2\\alpha\\beta\\hbar^2}{m}.$ Here $\\psi_s$ is basically a measure of scattering resonance sm. $\\Gamma$ is a measure of the width of the distribution of the wavefunction amplitude in the potential as a function of $k$. Taking the time $\\tau=\\frac\\hbar\\Gamma,$ we can evaluate $\\Delta t=4\\tau.$ Let's go a little wild. Take the $x>R$ wavefunction $e^{i\\delta}\\sin(kx+\\delta),$ so we care about $A_s=e^{i\\delta}\\sin\\delta.$ $A_s$ here is scattering amplitude. Then $A_s=\\frac{\\tan\\delta}{1-i\\tan\\delta}.$ So weirdly enough we'd like $\\tan\\delta=-i.$ Notice that our $|A_s|=1$ from earlier is just the best we can do with real $\\delta$, $\\delta\\to\\frac\\pi 2,$ i.e. $\\tan\\delta\\to\\infty.$ Now, continuing with our weird example... If we sub stuff back in we get $A_s=\\frac{\\frac{\\beta}{\\alpha-k}}{1-i\\frac{\\beta}{\\alpha-k}}=\\frac{\\beta}{(a-i\\beta)-k}.$ Then we want $k$ to be as close to $a-i\\beta$ as possible. If we allow complex $k$... we get arbitrarily big resonance at $\\tan(\\delta(k))=-i.$ Here, the imaginary part $\\beta$ refers us to the half-life (somehow, didnt get to that,) and the positive-imaginary-part poles refer us to the bound states! (somehow?? $E\\sim k^2<0,$ I suppose) so goes physics ![[Pasted image 20240107223209.png]]","title":"Modeling Resonance"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/","text":"Non-dimensionalized momentum $\\frac{i\\hat pa}{\\hbar}$ represents location shift; $e^{\\frac{i\\hat pa}\\hbar}=e^{a\\frac{\\partial}{\\partial x}},$ after expansion, $e^{a\\frac\\partial{\\partial x}}\\psi(x)=\\psi(x+a)$ representing shift left $-a$. Angular momentum operator Typically $\\bf L=\\bf r\\times\\bf p.$ We're using central angular momentum with spherical potential, so $\\bf r=\\bf x, \\bf p=\\hat p.$ - $\\bf\\hat L\\equiv\\bf\\hat r\\times\\bf\\hat p$ - $\\hat L_x=\\hat y\\hat p_z-\\hat z\\hat p_y$ - $\\hat L_i$ are Hermitian - Computing the commutator, $$ \\begin{align } [\\hat L_x,\\hat L_y] &=[yp_z-zp_y,zp_x-xp_z]\\ &=[yp_z,zp_x]+[zp_y,xp_z]\\ &=y[p_z,z]p_x+x[z,p_z]p_y\\ &=i\\hbar(xp_y-yp_x)\\ &=i\\hbar L_z. \\end{align } $$ - Commutators are ==algebraically cyclic==. Spin operators are similarly algebraically cyclic with commutator. - Thus no nontrivial eigenstate of $L_x,L_y$ simultaneously exists Expanding the original expansion yields $$\\hat L_x=\\frac\\hbar i(y\\frac d{dz}-z\\frac d{dy})=\\frac\\hbar i\\frac{\\partial}{\\partial \\phi_x}.$$ Here $\\phi_x=\\arctan(\\frac zy),$ the angle formed by the projection onto the 2D $yz$ plane, with $y$ as base. Expressing $L_x,L_y$ using standard spherical $\\phi,\\theta$ is trickier. Radial Ansatz Working with spherical potential, Schrodinger's becomes an equation in $\\bf L^2$ and $\\frac d{dr}.$ Now we care about $\\psi$ the eigenstate of $\\hat L_z,\\bf L^2.$ We specify $L_z\\psi=\\hbar m\\psi,{\\bf L^2}\\psi=\\hbar^2l(l+1)\\psi.$ Recall $\\psi(\\theta,\\phi)$ is $2\\pi$ periodic in both arguments. Solving first directly yields $\\frac d{d\\phi}\\psi=im\\psi,$ so $\\psi\\sim e^{im\\phi}.$ by $2\\pi$ periodicity $m\\in \\mathbb Z.$ Solving second and doing algebra yields $x=\\cos\\theta,$ $\\psi\\sim P,$ $$\\frac d{dx}\\left[(1-x)^2\\frac{dP}{dx}\\right]+\\left[l(l+1)-\\frac{m^2}{1-x^2}\\right]P=0.$$Here $P$ is Legendre polynomial $l,m$. Taking $m=0$, we can solve for coefficients of the series expansion, we get the Legendre polynomials $P_l=\\frac1{2^l l!}\\left(\\frac d{dx}\\right)^l(x^2-1)^l.$ Normalized form is $$Y_{l,m}(\\theta,\\phi)=\\sqrt{\\frac{2l+1}{4\\pi}\\frac{(l-m)!}{(l+m)!}}(-1)^me^{im\\phi}P_l^m(\\cos\\theta),$$with $P_l^m$ above Legendre polynomial. Resuming with radial ansatz, - Plugging in $\\psi(r,\\theta,\\phi)=R(r)Y(\\theta,\\phi)$ yields $u\\equiv rR,$ $$-\\frac{\\hbar^2}{2m}\\frac{d^2 u}{dr^2}+\\left(V(r)+\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u=Eu.$$ - We can match singular terms (the $r^{-2}$) as $r\\to 0,$ and determine $u\\sim r^{l+1}$ in that region.","title":"8.04.17 Angular Momentum, Radial Ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/#angular-momentum-operator","text":"Typically $\\bf L=\\bf r\\times\\bf p.$ We're using central angular momentum with spherical potential, so $\\bf r=\\bf x, \\bf p=\\hat p.$ - $\\bf\\hat L\\equiv\\bf\\hat r\\times\\bf\\hat p$ - $\\hat L_x=\\hat y\\hat p_z-\\hat z\\hat p_y$ - $\\hat L_i$ are Hermitian - Computing the commutator, $$ \\begin{align } [\\hat L_x,\\hat L_y] &=[yp_z-zp_y,zp_x-xp_z]\\ &=[yp_z,zp_x]+[zp_y,xp_z]\\ &=y[p_z,z]p_x+x[z,p_z]p_y\\ &=i\\hbar(xp_y-yp_x)\\ &=i\\hbar L_z. \\end{align } $$ - Commutators are ==algebraically cyclic==. Spin operators are similarly algebraically cyclic with commutator. - Thus no nontrivial eigenstate of $L_x,L_y$ simultaneously exists Expanding the original expansion yields $$\\hat L_x=\\frac\\hbar i(y\\frac d{dz}-z\\frac d{dy})=\\frac\\hbar i\\frac{\\partial}{\\partial \\phi_x}.$$ Here $\\phi_x=\\arctan(\\frac zy),$ the angle formed by the projection onto the 2D $yz$ plane, with $y$ as base. Expressing $L_x,L_y$ using standard spherical $\\phi,\\theta$ is trickier.","title":"Angular momentum operator"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Angular%20Momentum%2C%20Radial%20Ansatz/#radial-ansatz","text":"Working with spherical potential, Schrodinger's becomes an equation in $\\bf L^2$ and $\\frac d{dr}.$ Now we care about $\\psi$ the eigenstate of $\\hat L_z,\\bf L^2.$ We specify $L_z\\psi=\\hbar m\\psi,{\\bf L^2}\\psi=\\hbar^2l(l+1)\\psi.$ Recall $\\psi(\\theta,\\phi)$ is $2\\pi$ periodic in both arguments. Solving first directly yields $\\frac d{d\\phi}\\psi=im\\psi,$ so $\\psi\\sim e^{im\\phi}.$ by $2\\pi$ periodicity $m\\in \\mathbb Z.$ Solving second and doing algebra yields $x=\\cos\\theta,$ $\\psi\\sim P,$ $$\\frac d{dx}\\left[(1-x)^2\\frac{dP}{dx}\\right]+\\left[l(l+1)-\\frac{m^2}{1-x^2}\\right]P=0.$$Here $P$ is Legendre polynomial $l,m$. Taking $m=0$, we can solve for coefficients of the series expansion, we get the Legendre polynomials $P_l=\\frac1{2^l l!}\\left(\\frac d{dx}\\right)^l(x^2-1)^l.$ Normalized form is $$Y_{l,m}(\\theta,\\phi)=\\sqrt{\\frac{2l+1}{4\\pi}\\frac{(l-m)!}{(l+m)!}}(-1)^me^{im\\phi}P_l^m(\\cos\\theta),$$with $P_l^m$ above Legendre polynomial. Resuming with radial ansatz, - Plugging in $\\psi(r,\\theta,\\phi)=R(r)Y(\\theta,\\phi)$ yields $u\\equiv rR,$ $$-\\frac{\\hbar^2}{2m}\\frac{d^2 u}{dr^2}+\\left(V(r)+\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u=Eu.$$ - We can match singular terms (the $r^{-2}$) as $r\\to 0,$ and determine $u\\sim r^{l+1}$ in that region.","title":"Radial Ansatz"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/","text":"Problem 1 Explore $f(\\lambda)=\\int_{-\\infty}^{\\infty}dx\\,e^{-100(x-2)^2}e^{i\\phi(\\lambda,x)}$ where $\\phi(\\lambda,x)=50(x-\\lambda\\frac1{32}x^4).$ Half-max width is at $e^{-100(x-2)^2}=\\frac12,$ i.e. $100(x-2)^2=\\ln 2,$ so $\\Delta=\\frac{\\sqrt {\\ln 2}}{5}.$ Integrating from 1 to 3 is thus completely fine, at 1 or 3 we've reached $e^{-100},$ wow. Stationary phase specifies $\\left.\\frac d{dx}\\phi\\right|_2=0,$ i.e. $1-\\frac18\\lambda 2^3=0,$ so $\\lambda=1.$ Then $50(x-\\frac1{32} x^4)$ expanded degree 2 round $x=2$ is $50(1.5-\\frac3{16}(x-2)^2).$ Using that approximation, within the half-max width range, the excursion of $\\phi$ is just $50\\cdot\\frac 3{16}\\frac{ln 2}{100}=\\left(\\frac{3\\ln 2}{32\\pi}\\right)\\pi$ radians. Not all that much! Integrating with constant $\\phi$ yields $\\frac{\\sqrt\\pi}{10}\\cdot e^{75i}.$ Integrating with quadratic approx. $\\phi$ yields $\\frac{\\sqrt\\pi}{\\sqrt{100+\\frac3{16}i}}\\cdot e^{75i}.$ Problem 2 Analyze limits for $\\tan(\\delta)=\\frac{1-\\frac{k'}{k}\\cot k'a\\tan ka}{\\tan ka+\\frac{k'}k\\cot k'a},$ with $k^2=\\frac{2mE}{\\hbar^2}, k'^2=\\frac{2m(V+E)}{\\hbar^2},$ and $z_0=\\frac{2mV_0a^2}{\\hbar^2}.$ As $E\\to 0,$ $k\\to 0$ and $k'\\to \\frac{\\sqrt{2mV}}{\\hbar}.$ Then we can rewrite as $$\\tan(\\delta)=\\frac{k-k'\\cot k'a\\tan ka}{k\\tan ka+k'\\cot k'a}.$$ From this it is clear top $\\to 0$ and bottom $\\to k'\\cot k'a\\neq 0,$ so $\\tan(\\delta)\\to 0.$ As $E\\to\\infty,$ $k'-k\\to 0$$ so $\\cot k'a\\tan ka\\to 1.$ Top $\\to 0$ while bottom $\\to 2k(\\tan ka+\\cot k'a)$ which has norm at least $\\frac14$. Thus $\\tan(\\delta)\\to 0.$ Finally, letting $u=ka$ we have $k'a=\\sqrt{u^2+z_0^2},$ so we get $$\\delta=\\arctan\\left(\\frac{u-\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}\\tan u}{u\\tan u+\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}}\\right).$$ Problem 3 Solve for phase shift in the potential $V$ equalling $V_0>0$ for $x\\in (0,a),$ $0$ for $x>a,$ and $\\infty$ otherwise. If $E(k)>V_0$ then we get $k^2=\\frac{2mE}{\\hbar^2}, k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ On the right we have $\\sin(kx+\\delta)$ and on the left $\\sin(k_2x)$ so the old $\\psi/\\psi'$ strat yields $$ \\begin{align } k_2\\tan(ak_2) &=k\\tan(ak+\\delta)\\ &=\\frac{k\\tan(ak)+k\\tan(\\delta)}{1-\\tan(ak)\\tan(\\delta)}. \\end{align } $$ Then it's plain to see (from sight expansion) that $$\\tan(\\delta)=\\frac{k_2\\tan(ak_2)-k\\tan(ak)}{k+k_2\\tan(ak_2)\\tan(ak)}.$$ If $E(k)<V_0$ then with $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2},$ left is $\\sinh(\\kappa x)$ and right is $\\sin(kx+\\delta)$ so same thing $$ \\begin{align } \\kappa\\tanh(a\\kappa) &=k\\tan(ak+\\delta).\\ \\end{align } $$ By the same manipulation, $$\\tan(\\delta)=\\frac{\\kappa\\tanh(a\\kappa)-k\\tan(ak)}{k+\\kappa\\tanh(a\\kappa)\\tan(ak)}.$$ Problem 4 Examine phase shift with $V(x)=g\\delta(x-a),$ $g$ of units $J\\cdot m$. On the left $\\sin(kx)$ on the right $\\sin(kx+\\delta),$ as usual. Integrating the Hamiltonian yields $$g\\Psi(a)=\\frac{\\hbar^2}{2m}\\Delta\\left.\\frac d{dx}\\Psi\\right|_a.$$ Matching via $\\sin(ak+\\delta)\\sin(kx)$ and $\\sin(ak)\\sin(kx+\\delta),$ we get $$ \\begin{align } g\\sin(ak)\\sin(ak+\\delta) &=\\frac{\\hbar^2}{2m}\\left(k\\cos(ak+\\delta)\\sin(ak)-k\\cos(ak)\\sin(ak+\\delta)\\right)\\ &=\\frac{k\\hbar^2}{2m}\\sin(ak-(ak+\\delta))\\ &=-\\frac{k\\hbar^2}{2m}\\sin\\delta. \\end{align } $$ Then $$g\\sin^2(ak)\\cos(\\delta)+g\\cos(ak)\\sin(ak)\\sin(\\delta)+\\frac{\\hbar^2}{2m}k\\sin(\\delta)=0.$$ Consequently $$\\tan(\\delta)=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\frac{\\hbar^2}{2amg}ak}=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\lambda ak},$$ using denote $\\lambda=\\frac{\\hbar^2}{2amg}.$ As $ak\\to 0$ we get $\\sin(ak)\\to ak$ so $\\tan(\\delta)\\to -\\frac{ak}{1+\\lambda},$ and as $\\lambda\\to\\infty$ we get $\\tan(\\delta)=0$ for any $ak$. Problem 5 Show that $x,y,p_x,p_y$ have zero expectation on a $L_z$ eigenstate. First we compute a bazillion commutators. $L_z=\\frac\\hbar i\\left(x\\frac d{dy}-y\\frac d{dz}\\right).$ Then $$ \\begin{align } [L_z,x]&=\\frac\\hbar i(0)-\\frac\\hbar i(y)=-\\frac\\hbar i y,\\ [L_z,y]&=\\frac\\hbar i(x)-\\frac\\hbar i(0)=\\frac\\hbar i x,\\ [L_z,z]&=0,\\ [L_z,p_x]&=\\frac\\hbar i\\left(-\\frac\\hbar i \\frac d{dy}\\right) - \\frac\\hbar i(0)=-\\frac\\hbar i p_y,\\ [L_z,p_y]&=\\frac\\hbar i(0) - \\frac\\hbar i\\left(-\\frac\\hbar i\\frac d{dx}\\right)=\\frac\\hbar i p_x,\\ [L_z,p_z]&=0. \\end{align } $$ Finally, if $[L_z,\\hat A]=\\hat B,$ then for an eigenstate $L_z\\psi=\\lambda\\psi$ we get $$\\begin{align } \\langle\\psi,\\hat A\\psi\\rangle &=\\langle\\psi,\\hat A\\lambda\\psi\\rangle\\frac1{\\lambda}\\ &=\\langle\\psi,\\hat AL_z\\psi\\rangle\\frac1{\\lambda}\\ &=\\left(\\langle\\psi,L_z\\hat A\\psi\\rangle+\\langle\\psi,[\\hat A,L_z]\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\left(\\langle L_z\\psi,\\hat A\\psi\\rangle+\\langle\\psi,-\\hat B\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\frac1{\\lambda}\\langle L_z\\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle\\ &=\\langle \\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle.\\ \\end{align }$$ Thus $\\langle\\psi,\\hat B\\psi\\rangle=0.$ Thus by the previous commutator computations, the claim is shown. Problem 6 Compute $\\bf L^2$ in spherical coordinates. Skipped","title":"8.04.17 Pset 9"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-1","text":"Explore $f(\\lambda)=\\int_{-\\infty}^{\\infty}dx\\,e^{-100(x-2)^2}e^{i\\phi(\\lambda,x)}$ where $\\phi(\\lambda,x)=50(x-\\lambda\\frac1{32}x^4).$ Half-max width is at $e^{-100(x-2)^2}=\\frac12,$ i.e. $100(x-2)^2=\\ln 2,$ so $\\Delta=\\frac{\\sqrt {\\ln 2}}{5}.$ Integrating from 1 to 3 is thus completely fine, at 1 or 3 we've reached $e^{-100},$ wow. Stationary phase specifies $\\left.\\frac d{dx}\\phi\\right|_2=0,$ i.e. $1-\\frac18\\lambda 2^3=0,$ so $\\lambda=1.$ Then $50(x-\\frac1{32} x^4)$ expanded degree 2 round $x=2$ is $50(1.5-\\frac3{16}(x-2)^2).$ Using that approximation, within the half-max width range, the excursion of $\\phi$ is just $50\\cdot\\frac 3{16}\\frac{ln 2}{100}=\\left(\\frac{3\\ln 2}{32\\pi}\\right)\\pi$ radians. Not all that much! Integrating with constant $\\phi$ yields $\\frac{\\sqrt\\pi}{10}\\cdot e^{75i}.$ Integrating with quadratic approx. $\\phi$ yields $\\frac{\\sqrt\\pi}{\\sqrt{100+\\frac3{16}i}}\\cdot e^{75i}.$","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-2","text":"Analyze limits for $\\tan(\\delta)=\\frac{1-\\frac{k'}{k}\\cot k'a\\tan ka}{\\tan ka+\\frac{k'}k\\cot k'a},$ with $k^2=\\frac{2mE}{\\hbar^2}, k'^2=\\frac{2m(V+E)}{\\hbar^2},$ and $z_0=\\frac{2mV_0a^2}{\\hbar^2}.$ As $E\\to 0,$ $k\\to 0$ and $k'\\to \\frac{\\sqrt{2mV}}{\\hbar}.$ Then we can rewrite as $$\\tan(\\delta)=\\frac{k-k'\\cot k'a\\tan ka}{k\\tan ka+k'\\cot k'a}.$$ From this it is clear top $\\to 0$ and bottom $\\to k'\\cot k'a\\neq 0,$ so $\\tan(\\delta)\\to 0.$ As $E\\to\\infty,$ $k'-k\\to 0$$ so $\\cot k'a\\tan ka\\to 1.$ Top $\\to 0$ while bottom $\\to 2k(\\tan ka+\\cot k'a)$ which has norm at least $\\frac14$. Thus $\\tan(\\delta)\\to 0.$ Finally, letting $u=ka$ we have $k'a=\\sqrt{u^2+z_0^2},$ so we get $$\\delta=\\arctan\\left(\\frac{u-\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}\\tan u}{u\\tan u+\\sqrt{u^2+z_0^2}\\cot\\sqrt{u^2+z_0^2}}\\right).$$","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-3","text":"Solve for phase shift in the potential $V$ equalling $V_0>0$ for $x\\in (0,a),$ $0$ for $x>a,$ and $\\infty$ otherwise. If $E(k)>V_0$ then we get $k^2=\\frac{2mE}{\\hbar^2}, k_2^2=\\frac{2m(E-V)}{\\hbar^2}.$ On the right we have $\\sin(kx+\\delta)$ and on the left $\\sin(k_2x)$ so the old $\\psi/\\psi'$ strat yields $$ \\begin{align } k_2\\tan(ak_2) &=k\\tan(ak+\\delta)\\ &=\\frac{k\\tan(ak)+k\\tan(\\delta)}{1-\\tan(ak)\\tan(\\delta)}. \\end{align } $$ Then it's plain to see (from sight expansion) that $$\\tan(\\delta)=\\frac{k_2\\tan(ak_2)-k\\tan(ak)}{k+k_2\\tan(ak_2)\\tan(ak)}.$$ If $E(k)<V_0$ then with $\\kappa^2=\\frac{2m(V-E)}{\\hbar^2},$ left is $\\sinh(\\kappa x)$ and right is $\\sin(kx+\\delta)$ so same thing $$ \\begin{align } \\kappa\\tanh(a\\kappa) &=k\\tan(ak+\\delta).\\ \\end{align } $$ By the same manipulation, $$\\tan(\\delta)=\\frac{\\kappa\\tanh(a\\kappa)-k\\tan(ak)}{k+\\kappa\\tanh(a\\kappa)\\tan(ak)}.$$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-4","text":"Examine phase shift with $V(x)=g\\delta(x-a),$ $g$ of units $J\\cdot m$. On the left $\\sin(kx)$ on the right $\\sin(kx+\\delta),$ as usual. Integrating the Hamiltonian yields $$g\\Psi(a)=\\frac{\\hbar^2}{2m}\\Delta\\left.\\frac d{dx}\\Psi\\right|_a.$$ Matching via $\\sin(ak+\\delta)\\sin(kx)$ and $\\sin(ak)\\sin(kx+\\delta),$ we get $$ \\begin{align } g\\sin(ak)\\sin(ak+\\delta) &=\\frac{\\hbar^2}{2m}\\left(k\\cos(ak+\\delta)\\sin(ak)-k\\cos(ak)\\sin(ak+\\delta)\\right)\\ &=\\frac{k\\hbar^2}{2m}\\sin(ak-(ak+\\delta))\\ &=-\\frac{k\\hbar^2}{2m}\\sin\\delta. \\end{align } $$ Then $$g\\sin^2(ak)\\cos(\\delta)+g\\cos(ak)\\sin(ak)\\sin(\\delta)+\\frac{\\hbar^2}{2m}k\\sin(\\delta)=0.$$ Consequently $$\\tan(\\delta)=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\frac{\\hbar^2}{2amg}ak}=-\\frac{\\sin^2(ak)}{\\cos(ak)\\sin(ak)+\\lambda ak},$$ using denote $\\lambda=\\frac{\\hbar^2}{2amg}.$ As $ak\\to 0$ we get $\\sin(ak)\\to ak$ so $\\tan(\\delta)\\to -\\frac{ak}{1+\\lambda},$ and as $\\lambda\\to\\infty$ we get $\\tan(\\delta)=0$ for any $ak$.","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-5","text":"Show that $x,y,p_x,p_y$ have zero expectation on a $L_z$ eigenstate. First we compute a bazillion commutators. $L_z=\\frac\\hbar i\\left(x\\frac d{dy}-y\\frac d{dz}\\right).$ Then $$ \\begin{align } [L_z,x]&=\\frac\\hbar i(0)-\\frac\\hbar i(y)=-\\frac\\hbar i y,\\ [L_z,y]&=\\frac\\hbar i(x)-\\frac\\hbar i(0)=\\frac\\hbar i x,\\ [L_z,z]&=0,\\ [L_z,p_x]&=\\frac\\hbar i\\left(-\\frac\\hbar i \\frac d{dy}\\right) - \\frac\\hbar i(0)=-\\frac\\hbar i p_y,\\ [L_z,p_y]&=\\frac\\hbar i(0) - \\frac\\hbar i\\left(-\\frac\\hbar i\\frac d{dx}\\right)=\\frac\\hbar i p_x,\\ [L_z,p_z]&=0. \\end{align } $$ Finally, if $[L_z,\\hat A]=\\hat B,$ then for an eigenstate $L_z\\psi=\\lambda\\psi$ we get $$\\begin{align } \\langle\\psi,\\hat A\\psi\\rangle &=\\langle\\psi,\\hat A\\lambda\\psi\\rangle\\frac1{\\lambda}\\ &=\\langle\\psi,\\hat AL_z\\psi\\rangle\\frac1{\\lambda}\\ &=\\left(\\langle\\psi,L_z\\hat A\\psi\\rangle+\\langle\\psi,[\\hat A,L_z]\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\left(\\langle L_z\\psi,\\hat A\\psi\\rangle+\\langle\\psi,-\\hat B\\psi\\rangle\\right)\\frac1{\\lambda}\\ &=\\frac1{\\lambda}\\langle L_z\\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle\\ &=\\langle \\psi,\\hat A\\psi\\rangle+\\frac1{\\lambda}\\langle\\psi,-\\hat B\\psi\\rangle.\\ \\end{align }$$ Thus $\\langle\\psi,\\hat B\\psi\\rangle=0.$ Thus by the previous commutator computations, the claim is shown.","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.17%20Pset%209/#problem-6","text":"Compute $\\bf L^2$ in spherical coordinates. Skipped","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/","text":"Problem 1 Show that $\\psi(x)=e^{i\\delta(k)}\\sin(kx+\\delta(k))$ having a bound-state solution means $A_s=e^{i\\delta}\\sin\\delta$ has a pole for some imaginary value $k=i\\kappa.$ Expand to get $e^{-ikx}+e^{i(kx+2\\delta(k))},$ signflip for convenience to get $e^{ikx}+e^{-i(kx+2\\delta(k))}.$ Then for bound state asymptotically becoming $e^{-\\kappa x}$ we get $k=i\\kappa$ so $e^{-\\kappa x}+e^{\\kappa x-2i\\delta(k)}.$ Then evidently $|e^{2i\\delta(k)}|>|e^{\\kappa x}$ as $x\\to\\infty$ so $e^{i\\delta(k)}$ has a pole there, done. Problem 2 Suppose $\\hat A,\\hat B$ are commuting operators, and spectrum of $\\hat A$ is nondegenerate. Then all eigenstates of $\\hat A$ are the same for $\\hat B.$ Simply, $\\lambda B\\psi=BA\\psi=AB\\psi,$ and $A\\varphi=\\lambda\\varphi$ if and only if $\\varphi=k\\psi,$ so $B\\psi=k\\psi$ and we're done. Problem 3 Determine expectation, uncertainty of $L_z, \\bf{L^2}$ for $$\\psi(r,\\theta,\\phi)=\\frac14\\sqrt{\\frac 5\\pi}\\sin^2\\theta(1+\\sqrt{14}\\cos\\theta)\\cos2\\phi f(r).$$ Well, using Wikipedia's handy spherical harmonic lookup table we get $$\\psi=\\frac1{\\sqrt 6}(Y_2^{-2}+Y_2^2)+\\frac1{\\sqrt 3}(Y_3^{-2}+Y_3^2).$$ Eigenvalue of $L_z$ is $m,$ $\\bf L^2$ is $l(l+1).$ Then $L_z$ is even chance $-2$ or $2$, and $\\bf L^2$ is $\\frac13$ chance $6,$ $\\frac23$ chance $12.$ Expectations are $0,10;$ uncertainties are $2,2\\sqrt 2.$ Problem 4 Consider $l=0$ states of an infinite/finite spherical well $V(r)$, $0$ for $r<a$ and $\\infty$ otherwise. Resulting equation for $u=r\\psi(r)$ is $$(-\\frac{\\hbar^2}{2m}\\nabla^2+V)u=Eu.$$ This is indistinguishable from time-independent Schrodinger's. Usual solving yields $k^2=\\frac{2mE}{\\hbar^2},$ and $u$ vanishes at $r=0$ so $u=\\sin(k), u(a)=0$ so $ak=n\\pi,$ etc.etc. Yields same quantized energies as infinite well potential. For a finite spherical well $V$ equalling $-V_0$ for small $r$ and $0$ for large $r$, stipulation that $u(a)=0$ yields $k^2=\\frac{2m(V-E)}{\\hbar^2},$ $\\kappa^2=\\frac{2mE}{\\hbar^2},$ and by usual derivative/value matching $ak\\cot(ak)=-a\\kappa.$ Note also that $(ak)^2+(a\\kappa)^2=\\frac{2V_0ma^2}{\\hbar^2}.$ By Desmos inspection/some weird casework it's clear $(ak)^2+(a\\kappa)^2\\geq\\frac{\\pi^2}4$ at $\\kappa=0,ak=\\frac\\pi 2,$ so $\\frac{2V_0ma^2}{\\hbar^2}\\geq\\frac{\\pi^2}4,$ the result follows. Problem 5 SKIPPED Problem 6 Examine the Virial Theorem $\\langle T\\rangle=-\\frac12\\langle V\\rangle.$ IDK bro just do shit lol. Problem 7 .","title":"8.04.18 Pset 10"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-1","text":"Show that $\\psi(x)=e^{i\\delta(k)}\\sin(kx+\\delta(k))$ having a bound-state solution means $A_s=e^{i\\delta}\\sin\\delta$ has a pole for some imaginary value $k=i\\kappa.$ Expand to get $e^{-ikx}+e^{i(kx+2\\delta(k))},$ signflip for convenience to get $e^{ikx}+e^{-i(kx+2\\delta(k))}.$ Then for bound state asymptotically becoming $e^{-\\kappa x}$ we get $k=i\\kappa$ so $e^{-\\kappa x}+e^{\\kappa x-2i\\delta(k)}.$ Then evidently $|e^{2i\\delta(k)}|>|e^{\\kappa x}$ as $x\\to\\infty$ so $e^{i\\delta(k)}$ has a pole there, done.","title":"Problem 1"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-2","text":"Suppose $\\hat A,\\hat B$ are commuting operators, and spectrum of $\\hat A$ is nondegenerate. Then all eigenstates of $\\hat A$ are the same for $\\hat B.$ Simply, $\\lambda B\\psi=BA\\psi=AB\\psi,$ and $A\\varphi=\\lambda\\varphi$ if and only if $\\varphi=k\\psi,$ so $B\\psi=k\\psi$ and we're done.","title":"Problem 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-3","text":"Determine expectation, uncertainty of $L_z, \\bf{L^2}$ for $$\\psi(r,\\theta,\\phi)=\\frac14\\sqrt{\\frac 5\\pi}\\sin^2\\theta(1+\\sqrt{14}\\cos\\theta)\\cos2\\phi f(r).$$ Well, using Wikipedia's handy spherical harmonic lookup table we get $$\\psi=\\frac1{\\sqrt 6}(Y_2^{-2}+Y_2^2)+\\frac1{\\sqrt 3}(Y_3^{-2}+Y_3^2).$$ Eigenvalue of $L_z$ is $m,$ $\\bf L^2$ is $l(l+1).$ Then $L_z$ is even chance $-2$ or $2$, and $\\bf L^2$ is $\\frac13$ chance $6,$ $\\frac23$ chance $12.$ Expectations are $0,10;$ uncertainties are $2,2\\sqrt 2.$","title":"Problem 3"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-4","text":"Consider $l=0$ states of an infinite/finite spherical well $V(r)$, $0$ for $r<a$ and $\\infty$ otherwise. Resulting equation for $u=r\\psi(r)$ is $$(-\\frac{\\hbar^2}{2m}\\nabla^2+V)u=Eu.$$ This is indistinguishable from time-independent Schrodinger's. Usual solving yields $k^2=\\frac{2mE}{\\hbar^2},$ and $u$ vanishes at $r=0$ so $u=\\sin(k), u(a)=0$ so $ak=n\\pi,$ etc.etc. Yields same quantized energies as infinite well potential. For a finite spherical well $V$ equalling $-V_0$ for small $r$ and $0$ for large $r$, stipulation that $u(a)=0$ yields $k^2=\\frac{2m(V-E)}{\\hbar^2},$ $\\kappa^2=\\frac{2mE}{\\hbar^2},$ and by usual derivative/value matching $ak\\cot(ak)=-a\\kappa.$ Note also that $(ak)^2+(a\\kappa)^2=\\frac{2V_0ma^2}{\\hbar^2}.$ By Desmos inspection/some weird casework it's clear $(ak)^2+(a\\kappa)^2\\geq\\frac{\\pi^2}4$ at $\\kappa=0,ak=\\frac\\pi 2,$ so $\\frac{2V_0ma^2}{\\hbar^2}\\geq\\frac{\\pi^2}4,$ the result follows.","title":"Problem 4"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-5","text":"SKIPPED","title":"Problem 5"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-6","text":"Examine the Virial Theorem $\\langle T\\rangle=-\\frac12\\langle V\\rangle.$ IDK bro just do shit lol.","title":"Problem 6"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Pset%2010/#problem-7","text":".","title":"Problem 7"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/","text":"We solve Hydrogen! LFG Setup Wavefunction $\\Psi(x_p,x_e),$ probability of finding in neighborhood. - Usual canonical operators $\\hat x_p,\\hat p_p, \\hat x_e,\\hat p_e;$ pairwise commutator $i\\hbar,$ $p,e$ are independent from each other - Squared norm integral in $\\mathbb R^3\\otimes\\mathbb R^3$ is 1 The Hamiltonian is $\\frac{p_p^2}{2m_p}+\\frac{p_e^2}{2m_e}+V(|x_e-x_p|),$ which is rather unwieldy because of the terrible $V$ making it not separable. Let's separate degrees of freedom, having COM (upper) and relative (lower). Using $M=m_e+m_p,$ canonical commutator restrictions yield $$\\begin{align }X&=\\frac{m_ex_e+m_px_p}{M},\\x&=x_e-x_p,\\P&=p_e+p_p,\\p&=\\frac{m_e}Mp_e-\\frac{m_p}Mp_p.\\end{align }$$ Rewrite in $\\Psi(X,x).$ Now $p_p=\\frac{m_p}MP-p, p_e=\\frac{m_e}MP+p,$ Hamiltonian becomes $\\frac{P^2}{2M}+\\frac{Mp^2}{2m_pm_e}.$ Using $\\mu=\\frac{m_pm_e}{M},$ we get $$\\hat H=\\frac{P^2}{2M}+\\frac{p^2}{2\\mu}+V(x).$$ Letting $\\Psi(X,x)=\\Psi_{CM}(X)\\Psi_{rel}(x),$ we get $$\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}+\\frac1{\\Psi_{rel}}\\left(\\frac{p^2}{2\\mu}+V(x)\\right)\\Psi_{rel}=E.$$ Now we can separate into $E_{CM}$ and $E_{rel},$ which must sum to $E$. Analysis We use potential $V(x)=\\frac{Zke^2}{r},$ where $Z$ is # protons, $ke^2=K$ where $e$ electron charge, $k$ is Coulomb's const. To be safe, nondim. with $a_0=\\frac{\\hbar^2}{mke^2}\\approx 53$pm. Here $m=m_e\\approx\\mu,$ since $m_e\\ll m_p$. First equation $\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}=E_{CM}$ with $P=\\frac\\hbar i\\nabla_X$ resolves to $\\Psi_{CM}=e^{i(X\\cdot v)}$ for some vector $v\\cdot v=\\frac{2ME_{CM}}{\\hbar^2},$ as usual. Second equation is harder. Nondim. with $r=\\frac{a_0}{2Z}x.$ Then recall our radial ansatz $u=r\\Psi,$ $$\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+\\frac{\\hbar^2l(l+1)}{2mr^2}+\\frac{Zke^2}r\\right)u=Eu.$$ Nondim. and cancelling yields $$\\left(-\\frac{2Z^2\\hbar^2}{ma_0^2}\\nabla^2_x+\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac{l(l+1)}{x^2}-\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac1x\\right)u=Eu.$$ Then let $\\kappa^2=-\\frac{E}{2Z^2\\hbar^2/ma_0^2}$ so $$\\left(-\\nabla^2+\\frac{l(l+1)}{x^2}-\\frac1x\\right)u=-\\kappa^2 u.$$ In the limit $x\\to\\infty$ we get $u\\sim e^{\\pm \\kappa x}$. For $x\\to 0$ we must have $\\nabla^2=\\frac{l(l+1)}{x^2}$ so $u\\sim x^{l+1}$ there. Then $\\rho=\\kappa x$ and $u=e^{-\\rho}\\rho^{l+1}W$ yields $$\\rho W''+2(l+1-\\rho)W'+\\left(\\frac1\\kappa-2(l+1)\\right)W=0.$$ Series expansion $W=\\sum_i a_ix^i$ yields at degree $k$ $$(k+1)ka_{k+1}+2(l+1)(k+1)a_{k+1}-2ka_{k}+\\left(\\frac1\\kappa-2(l+1)\\right)a_k=0,$$ $$\\frac{a_{k+1}}{a_k}=\\frac{2(l+k+1)-\\frac1\\kappa}{2l+2+k}.$$ If this never terminates then $u$ is not tight enough, so we need $\\kappa=\\frac1{2(l+k+1)}.$ Set $n=l+k+1$ for some $k$. Then $E=-\\frac{Z^2\\hbar^2}{2ma_0^2n^2}.$ Finally our ansatz is $$ \\begin{align } \\Psi_{rel}=R(r)Y_{lm}&=\\mathcal N\\rho^le^{-\\rho}W_{nl}(\\rho)Y_{lm}(\\theta,\\phi)\\ &=\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi), \\end{align } $$ $$\\Psi=\\Psi_{CM}\\Psi_{rel}=e^{i(X\\cdot v)}\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi).$$","title":"8.04.18 Solving Hydrogen"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/#setup","text":"Wavefunction $\\Psi(x_p,x_e),$ probability of finding in neighborhood. - Usual canonical operators $\\hat x_p,\\hat p_p, \\hat x_e,\\hat p_e;$ pairwise commutator $i\\hbar,$ $p,e$ are independent from each other - Squared norm integral in $\\mathbb R^3\\otimes\\mathbb R^3$ is 1 The Hamiltonian is $\\frac{p_p^2}{2m_p}+\\frac{p_e^2}{2m_e}+V(|x_e-x_p|),$ which is rather unwieldy because of the terrible $V$ making it not separable. Let's separate degrees of freedom, having COM (upper) and relative (lower). Using $M=m_e+m_p,$ canonical commutator restrictions yield $$\\begin{align }X&=\\frac{m_ex_e+m_px_p}{M},\\x&=x_e-x_p,\\P&=p_e+p_p,\\p&=\\frac{m_e}Mp_e-\\frac{m_p}Mp_p.\\end{align }$$ Rewrite in $\\Psi(X,x).$ Now $p_p=\\frac{m_p}MP-p, p_e=\\frac{m_e}MP+p,$ Hamiltonian becomes $\\frac{P^2}{2M}+\\frac{Mp^2}{2m_pm_e}.$ Using $\\mu=\\frac{m_pm_e}{M},$ we get $$\\hat H=\\frac{P^2}{2M}+\\frac{p^2}{2\\mu}+V(x).$$ Letting $\\Psi(X,x)=\\Psi_{CM}(X)\\Psi_{rel}(x),$ we get $$\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}+\\frac1{\\Psi_{rel}}\\left(\\frac{p^2}{2\\mu}+V(x)\\right)\\Psi_{rel}=E.$$ Now we can separate into $E_{CM}$ and $E_{rel},$ which must sum to $E$.","title":"Setup"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.18%20Solving%20Hydrogen/#analysis","text":"We use potential $V(x)=\\frac{Zke^2}{r},$ where $Z$ is # protons, $ke^2=K$ where $e$ electron charge, $k$ is Coulomb's const. To be safe, nondim. with $a_0=\\frac{\\hbar^2}{mke^2}\\approx 53$pm. Here $m=m_e\\approx\\mu,$ since $m_e\\ll m_p$. First equation $\\frac1{\\Psi_{CM}}\\left(\\frac{P^2}{2M}\\right)\\Psi_{CM}=E_{CM}$ with $P=\\frac\\hbar i\\nabla_X$ resolves to $\\Psi_{CM}=e^{i(X\\cdot v)}$ for some vector $v\\cdot v=\\frac{2ME_{CM}}{\\hbar^2},$ as usual. Second equation is harder. Nondim. with $r=\\frac{a_0}{2Z}x.$ Then recall our radial ansatz $u=r\\Psi,$ $$\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+\\frac{\\hbar^2l(l+1)}{2mr^2}+\\frac{Zke^2}r\\right)u=Eu.$$ Nondim. and cancelling yields $$\\left(-\\frac{2Z^2\\hbar^2}{ma_0^2}\\nabla^2_x+\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac{l(l+1)}{x^2}-\\frac{2Z^2\\hbar^2}{ma_0^2}\\frac1x\\right)u=Eu.$$ Then let $\\kappa^2=-\\frac{E}{2Z^2\\hbar^2/ma_0^2}$ so $$\\left(-\\nabla^2+\\frac{l(l+1)}{x^2}-\\frac1x\\right)u=-\\kappa^2 u.$$ In the limit $x\\to\\infty$ we get $u\\sim e^{\\pm \\kappa x}$. For $x\\to 0$ we must have $\\nabla^2=\\frac{l(l+1)}{x^2}$ so $u\\sim x^{l+1}$ there. Then $\\rho=\\kappa x$ and $u=e^{-\\rho}\\rho^{l+1}W$ yields $$\\rho W''+2(l+1-\\rho)W'+\\left(\\frac1\\kappa-2(l+1)\\right)W=0.$$ Series expansion $W=\\sum_i a_ix^i$ yields at degree $k$ $$(k+1)ka_{k+1}+2(l+1)(k+1)a_{k+1}-2ka_{k}+\\left(\\frac1\\kappa-2(l+1)\\right)a_k=0,$$ $$\\frac{a_{k+1}}{a_k}=\\frac{2(l+k+1)-\\frac1\\kappa}{2l+2+k}.$$ If this never terminates then $u$ is not tight enough, so we need $\\kappa=\\frac1{2(l+k+1)}.$ Set $n=l+k+1$ for some $k$. Then $E=-\\frac{Z^2\\hbar^2}{2ma_0^2n^2}.$ Finally our ansatz is $$ \\begin{align } \\Psi_{rel}=R(r)Y_{lm}&=\\mathcal N\\rho^le^{-\\rho}W_{nl}(\\rho)Y_{lm}(\\theta,\\phi)\\ &=\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi), \\end{align } $$ $$\\Psi=\\Psi_{CM}\\Psi_{rel}=e^{i(X\\cdot v)}\\mathcal N\\left(\\frac r{a_0}\\right)^le^{-\\frac{2kZ}{a_0}r}W_{nl}(\\frac{2kZ}{a_0}r)Y_{lm}(\\theta,\\phi).$$","title":"Analysis"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.2%20Dualities%20of%20Light%20and%20Matter/","text":"Effectively, everything is both a wave and a particle. Lots of fun. Formulas: - Photon energy $E=h\\nu$ - ==Compton wavelength is the wavelength for a photon whose energy equals the particle's rest energy ==: $mc^2=h\\nu=h\\frac c\\lambda,$ so $\\lambda_C=\\frac h{mc}$ - As Compton wavelength is wavelength of photon with the same rest energy, ==de Broglie wavelength is that of photon with the same momentum .== - A matter particle has an approximate de Broglie wavelength $\\lambda=\\frac hp.$ - Relativity momentum: $E^2-p^2c^2=m^2c^4.$ Thus $E=\\sqrt{m^2c^4+p^2c^2,}$ and $\\Delta E=E-mc^2\\approx \\frac12{p^2}m$ as expected. - As a result, momentum and energy both have a $\\frac1{\\sqrt{1-\\frac{v^2}{c^2}}}$ factor on top of the classical $mc^2$ and $mv$. Photoelectron scattering is what happens when a photon hits an atom and ionizes it. By classical EM intuition it's just an electron that begins oscillating in a field, but that don't really work for photons at energies higher than potential holding electrons. Using relativistic momentum and energy conservation, Compton scattering yields new photon wavelength $\\lambda_f=\\lambda_i+\\lambda_C(1-\\cos\\theta),$ where $\\theta$ is directional angle change and $\\lambda_C$ is Compton wavelength for the electron. $\\require{mhchem}\\newcommand{\\CC}{\\mathbb C}$-latex","title":"8.04.2 Dualities of Light and Matter"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/","text":"de Broglie wavelengths We can take a normal Galilean transformation $$x'=x-vt, t'=t.$$ Notice that since $p=mv,$ and $\\lambda=\\frac hp,$ so the de Broglie wavelength actually changes with an inertial frame of reference change. Understandably, the observable, time-independently measurable phase $\\phi=kx-\\omega t$ is a Galilean invariant. [!TIP] Harmonic equations and relations The primitives are $v$ velocity and $\\lambda$ complete wavelength. $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda$$ ==> Here $\\nu$ is complete cycle frequency while $\\omega$ is the angular frequency, and $k$ is the wavenumber (inverse distance units), meaning one would write $\\sin(kx).$== Recall also the Planck constant $h$ and the reduced $\\hbar$. [!info] de Broglie relations $$p=\\hbar k,\\quad\\lambda=\\frac{2\\pi}k=\\frac hp.$$ Group Velocities We have a packet of waves traveling together. For each wavenumber $k$ there is an associated frequency $\\omega(k)$ and a magnitude $\\Phi(k).$ At each point in time $t,$ for each location $x$ we stack all the waves and their amplitudes at that point by computing $$\\psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ We can try to figure out where this is nonnegligible, determining the location of the \"body\" of the wave packet. Assume that $\\Phi(k)$ has a large peak at $k_0.$ We need $i(kx-\\omega(k)t)$ to be stationary at $k_0$, which yields $\\frac xt=\\frac{\\partial \\omega}{\\partial k}.$ Thus there is a group velocity of $\\frac{\\partial \\omega}{\\partial k},$ the speed at which the apparent bulk of the wave moves. Wavefunctions We can have candidate wavefunctions. Given $E,p$ we can determine $k,\\omega,$ sure. We could try $\\cos(kx\\pm\\omega t,)$ but we have the constraint that [!warning] Matter wavefunction constraint Under any superposition, $\\nexists t\\colon \\forall x, \\psi(x,t)=0.$ Turns out that $\\cos(kx-\\omega t)+\\cos(kx+\\omega t)=2\\cos(kx)\\cos(\\omega t),$ clearly fails. Therefore it has to be $e^{i(\\pm kx\\pm\\omega t)};$ sticking to the above convention, we have [!important] Matter wavefunction formula $$\\Psi(x,t)=e^{i(kx-\\omega t)}.$$ This allows the wave to propogate forward ; $kx'-\\omega t'=kx-\\omega t\\implies k\\Delta x=\\omega\\Delta t.$","title":"8.04.3 Matter Waves"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#de-broglie-wavelengths","text":"We can take a normal Galilean transformation $$x'=x-vt, t'=t.$$ Notice that since $p=mv,$ and $\\lambda=\\frac hp,$ so the de Broglie wavelength actually changes with an inertial frame of reference change. Understandably, the observable, time-independently measurable phase $\\phi=kx-\\omega t$ is a Galilean invariant. [!TIP] Harmonic equations and relations The primitives are $v$ velocity and $\\lambda$ complete wavelength. $$\\lambda\\nu=v,\\quad T=\\frac1\\nu=\\frac\\lambda v, \\quad \\omega=\\frac{2\\pi}T=2\\pi\\nu,\\quad k=\\frac{2\\pi}\\lambda$$ ==> Here $\\nu$ is complete cycle frequency while $\\omega$ is the angular frequency, and $k$ is the wavenumber (inverse distance units), meaning one would write $\\sin(kx).$== Recall also the Planck constant $h$ and the reduced $\\hbar$. [!info] de Broglie relations $$p=\\hbar k,\\quad\\lambda=\\frac{2\\pi}k=\\frac hp.$$","title":"de Broglie wavelengths"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#group-velocities","text":"We have a packet of waves traveling together. For each wavenumber $k$ there is an associated frequency $\\omega(k)$ and a magnitude $\\Phi(k).$ At each point in time $t,$ for each location $x$ we stack all the waves and their amplitudes at that point by computing $$\\psi(x,t)=\\int\\Phi(k)e^{i(kx-\\omega(k)t)}\\,dk.$$ We can try to figure out where this is nonnegligible, determining the location of the \"body\" of the wave packet. Assume that $\\Phi(k)$ has a large peak at $k_0.$ We need $i(kx-\\omega(k)t)$ to be stationary at $k_0$, which yields $\\frac xt=\\frac{\\partial \\omega}{\\partial k}.$ Thus there is a group velocity of $\\frac{\\partial \\omega}{\\partial k},$ the speed at which the apparent bulk of the wave moves.","title":"Group Velocities"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.3%20Matter%20Waves/#wavefunctions","text":"We can have candidate wavefunctions. Given $E,p$ we can determine $k,\\omega,$ sure. We could try $\\cos(kx\\pm\\omega t,)$ but we have the constraint that [!warning] Matter wavefunction constraint Under any superposition, $\\nexists t\\colon \\forall x, \\psi(x,t)=0.$ Turns out that $\\cos(kx-\\omega t)+\\cos(kx+\\omega t)=2\\cos(kx)\\cos(\\omega t),$ clearly fails. Therefore it has to be $e^{i(\\pm kx\\pm\\omega t)};$ sticking to the above convention, we have [!important] Matter wavefunction formula $$\\Psi(x,t)=e^{i(kx-\\omega t)}.$$ This allows the wave to propogate forward ; $kx'-\\omega t'=kx-\\omega t\\implies k\\Delta x=\\omega\\Delta t.$","title":"Wavefunctions"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.4%20Schrodinger%27s%20and%20Wavefunctions/","text":"[!FAQ] What is the wavefunction? It's a function returning the likelihood of finding a particle in the neighborhood . Specifically, $P(\\bf x)=|\\Psi(x)|^2.$ The Schrodinger equation will, in fact, preserve $\\int |\\Psi(\\bf x)|^2\\,d\\bf x^3=1.$ In one dimension, at non-relativistic speeds, since the particle is of the form $$e^{i(kx-\\omega t)}$$ and the momentum, energy are $p=\\hbar k$ and $E=\\hbar\\omega=\\frac{p^2}{2m},$ we get the operator $$\\hat p=\\frac{\\hbar}i\\frac{\\partial}{\\partial x}.$$ Then $$\\hat E\\Psi=\\hbar\\omega=i\\hbar\\frac{\\partial}{\\partial t}\\Psi,$$ but also $$\\hat E\\Psi=\\frac{p^2}{2m}\\Psi=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi.$$ Then we get the [!important] Free Particle Schrodinger's Equation $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi.$$ This holds for all free particles of mass m regardless of speed or frequency. Now if we add a potential energy, we now have $E=\\frac{p^2}{2m}+V$ (the first term is kinetic energy). Furthermore we can extend to three dimensions using the Laplacian and del , yielding [!important] Schrodinger's Equation Where both $V$ and $\\Psi$ are functions of $\\textbf{x}$ and $t,$ $$i\\hbar\\frac{\\partial}{\\partial t}\\Psi=\\left(-\\frac{\\hbar^2}{2m}\\nabla^2+V\\right)\\Psi.$$ Note that this is linear in $\\Psi$ and first order in $t$. We also have the $\\hat x=x$ operator (basically just an abstract formalization). Then we can compute commutators $[\\hat A,\\hat B]=\\hat A\\hat B-\\hat B\\hat A.$ It turns out that $[\\hat x,\\hat p]=i\\hbar.$ Quantum Linear Algebra Operators Matrices Wavefunctions/States Vectors Eigenstates Eigenvectors","title":"8.04.4 Schrodinger's and Wavefunctions"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/","text":"$\\require{mhchem}\\newcommand{\\CC}{\\mathbb C} \\newcommand{\\SN}{\\mathcal N} \\newcommand{\\Ham}{\\hat H}\\DeclareMathOperator{\\IM}{Im}$ As $\\Psi$ is a probability we can say at any time $t,$ the whole space $\\int |\\Psi|^2\\,dx=1.$ - This is a nice convention to have, but not necessary; at any time we can just scale by $\\mathcal N=\\int |\\Psi|^2\\,dx,\\Psi'=\\frac1{\\sqrt{\\mathcal N}}\\Psi.$ - In fact our previous wavefunction for the free particle doesn't work for this reason - We can superimpose non-square-integrable functions and arrive at a square-integrable function - In general we would like $\\int|\\Psi|^2\\,dx$ to exist. Total Amplitude Conservation We need two conditions: [!FAQ] Two necessary conditions It is sufficient for $$\\lim_{x\\to\\infty}\\frac{\\partial\\Psi}{\\partial x}<\\infty,\\qquad \\lim_{x\\to\\infty}\\Psi=0.$$ Let $\\rho(x,t)=\\Psi(x,t)\\Psi(x,t)^ ,$ and let $\\SN(t)\\equiv\\int\\rho(x,t)\\,dx.$ We want to show $\\frac{\\partial}{\\partial t}\\SN=0.$ Equivalently, $$\\int \\frac{\\partial\\Psi}{\\partial t}\\Psi^ +\\frac{\\partial\\Psi^ }{\\partial t}\\Psi\\,dx=0,$$ but we can use the Schrodinger equation and get $$\\int \\Psi^ \\cdot-\\frac i{\\hbar}\\Ham\\Psi+\\Psi\\cdot\\frac i{\\hbar}\\Ham\\Psi^ \\,dx=0,$$ equivalently $$\\int \\Psi^ \\Ham\\Psi\\,dx=\\int \\Psi\\Ham\\Psi^*\\,dx.$$ [!important] Hermitian operator In the case that $$\\int (\\Ham\\Psi_1)^ \\Psi_2\\,dx=\\int\\Psi_1^ (\\Ham\\Psi_2)\\,dx,$$ we call $\\Ham$ a Hermitian operator. In general a linear operator $T$ will have a conjugate $T^\\dagger$ that satisfies the above property, and $T$ is Hermitian if $T=T^\\dagger.$ Hamiltonian is Hermitian Expanding the expression $$\\frac i\\hbar\\left(\\Psi\\Ham\\Psi^ -\\Psi^ \\Ham\\Psi\\right)=-\\frac {i\\hbar}{2m}\\left(\\Psi\\nabla^2\\Psi^ -\\Psi^ \\nabla^2\\Psi\\right)=-\\frac{\\hbar}m\\nabla\\cdot\\left[\\frac1{2i}\\left(\\Psi^ (\\nabla\\Psi)-\\Psi(\\nabla \\Psi^ )\\right)\\right].$$ The conclusion is then $$\\frac{\\partial\\rho}{\\partial t}+\\frac\\hbar m\\nabla\\cdot\\text{Im}\\left(\\Psi^ (\\nabla\\Psi)\\right)=0.$$ Then letting $J=\\frac\\hbar m \\IM(\\Psi^ (\\nabla\\Psi))$ we get $\\frac{\\partial\\rho}{\\partial t}+\\frac{\\partial J}{\\partial x}=0,$ and so $$\\frac{\\partial \\SN}{\\partial t}=\\int\\frac{\\partial\\rho}{\\partial t}\\,dx=\\int\\frac{\\partial J}{\\partial x}\\,dx=\\left. \\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))\\right|_{-\\infty}^{\\infty}=0,$$ on the earlier assumption. [!important] Probability current $J=\\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))$ is the \"probability current,\" meaning that (1) in 1 dimension, $J$ is the ROC of the cumulative probability distribution (2) in 3 dimensions, flux integral of $J$ represents ROC in that region Item EM Quantum $\\rho$ charge density probability density $Q_V$ charge in volume probability in volume $\\bf J$ current flow density probability flow density","title":"8.04.5 Wavefunction as Probability"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/#total-amplitude-conservation","text":"We need two conditions: [!FAQ] Two necessary conditions It is sufficient for $$\\lim_{x\\to\\infty}\\frac{\\partial\\Psi}{\\partial x}<\\infty,\\qquad \\lim_{x\\to\\infty}\\Psi=0.$$ Let $\\rho(x,t)=\\Psi(x,t)\\Psi(x,t)^ ,$ and let $\\SN(t)\\equiv\\int\\rho(x,t)\\,dx.$ We want to show $\\frac{\\partial}{\\partial t}\\SN=0.$ Equivalently, $$\\int \\frac{\\partial\\Psi}{\\partial t}\\Psi^ +\\frac{\\partial\\Psi^ }{\\partial t}\\Psi\\,dx=0,$$ but we can use the Schrodinger equation and get $$\\int \\Psi^ \\cdot-\\frac i{\\hbar}\\Ham\\Psi+\\Psi\\cdot\\frac i{\\hbar}\\Ham\\Psi^ \\,dx=0,$$ equivalently $$\\int \\Psi^ \\Ham\\Psi\\,dx=\\int \\Psi\\Ham\\Psi^*\\,dx.$$ [!important] Hermitian operator In the case that $$\\int (\\Ham\\Psi_1)^ \\Psi_2\\,dx=\\int\\Psi_1^ (\\Ham\\Psi_2)\\,dx,$$ we call $\\Ham$ a Hermitian operator. In general a linear operator $T$ will have a conjugate $T^\\dagger$ that satisfies the above property, and $T$ is Hermitian if $T=T^\\dagger.$","title":"Total Amplitude Conservation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.5%20Wavefunction%20as%20Probability/#hamiltonian-is-hermitian","text":"Expanding the expression $$\\frac i\\hbar\\left(\\Psi\\Ham\\Psi^ -\\Psi^ \\Ham\\Psi\\right)=-\\frac {i\\hbar}{2m}\\left(\\Psi\\nabla^2\\Psi^ -\\Psi^ \\nabla^2\\Psi\\right)=-\\frac{\\hbar}m\\nabla\\cdot\\left[\\frac1{2i}\\left(\\Psi^ (\\nabla\\Psi)-\\Psi(\\nabla \\Psi^ )\\right)\\right].$$ The conclusion is then $$\\frac{\\partial\\rho}{\\partial t}+\\frac\\hbar m\\nabla\\cdot\\text{Im}\\left(\\Psi^ (\\nabla\\Psi)\\right)=0.$$ Then letting $J=\\frac\\hbar m \\IM(\\Psi^ (\\nabla\\Psi))$ we get $\\frac{\\partial\\rho}{\\partial t}+\\frac{\\partial J}{\\partial x}=0,$ and so $$\\frac{\\partial \\SN}{\\partial t}=\\int\\frac{\\partial\\rho}{\\partial t}\\,dx=\\int\\frac{\\partial J}{\\partial x}\\,dx=\\left. \\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))\\right|_{-\\infty}^{\\infty}=0,$$ on the earlier assumption. [!important] Probability current $J=\\frac\\hbar m \\IM(\\Psi^*(\\nabla\\Psi))$ is the \"probability current,\" meaning that (1) in 1 dimension, $J$ is the ROC of the cumulative probability distribution (2) in 3 dimensions, flux integral of $J$ represents ROC in that region Item EM Quantum $\\rho$ charge density probability density $Q_V$ charge in volume probability in volume $\\bf J$ current flow density probability flow density","title":"Hamiltonian is Hermitian"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.6%20Uncertainty%20and%20Fourier%20Inverse/","text":"When we have a wavepacket at fixed time $t=0$ $$\\Psi(x,0)=\\int\\Phi(k)e^{ikx}\\,dk$$ we get the relation [!important] Heisenberg Uncertainty $$\\Delta x\\Delta k\\geq\\frac12.$$ More specifically, $p=\\hbar k$ so $\\Delta x\\Delta p\\geq\\frac\\hbar 2.$ Recall the earlier exercise where we expanded $\\omega(k)$ to first order; now expanding to second order (suspicious) $$\\frac{d\\omega}{dk}=\\frac{dE}{dp}=\\frac pm=\\frac{\\hbar k}m.$$ This seems to interchange between de Broglie and Compton, but we won't worry about that. Then $\\frac{d^2\\omega}{d k^2}=\\frac\\hbar m$. We want to see how far we can go without destroying the linear $\\omega$ estimate; i.e. how far for $\\omega(k)t$ to be perturbed $O(1),$ i.e. $(\\nabla k)^2\\frac\\hbar m t\\ll1,$ or $t\\ll\\frac{m\\hbar}{(\\nabla p)^2}.$ We can also use $\\nabla x\\nabla p\\approx \\hbar$ to get $\\frac{\\nabla p}mt\\ll\\nabla x.$ This makes sense. Fourier inverse Given $\\Psi(x,0)$ we can get [!faq] Fourier Inverse $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ Then we can sorta hack with $E=\\hbar\\omega=\\frac{p^2}{2m}=\\frac{\\hbar^2k^2}{2m}$ to get $\\omega=\\frac{\\hbar k^2}{2m}.$ Then we can reconstruct all $\\Psi(x,t).$","title":"8.04.6 Uncertainty and Fourier Inverse"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.6%20Uncertainty%20and%20Fourier%20Inverse/#fourier-inverse","text":"Given $\\Psi(x,0)$ we can get [!faq] Fourier Inverse $$\\Phi(k)=\\frac1{2\\pi}\\int\\Psi(x,0)e^{-ikx}\\,dx.$$ Then we can sorta hack with $E=\\hbar\\omega=\\frac{p^2}{2m}=\\frac{\\hbar^2k^2}{2m}$ to get $\\omega=\\frac{\\hbar k^2}{2m}.$ Then we can reconstruct all $\\Psi(x,t).$","title":"Fourier inverse"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/","text":"Typically, Fourier Inversion tells us $$\\hat f(\\xi)=\\int f(x)e^{-2\\pi i\\xi x}\\,dx\\implies f(x)=\\int \\hat f(\\xi)e^{2\\pi i\\xi x}\\,d\\xi.$$ If we take $\\xi=\\frac1{2\\pi}k$ then $\\hat f(k)=\\int f(x)e^{-ikx},$ $f(x)=\\frac1{2\\pi}\\int f(x)e^{ikx}\\,dk.$ Splitting this factor and changing things around, we can use [!important] QM coordinate space Fourier transform $$\\Psi(x)=\\frac1{\\sqrt{2\\pi}}\\int \\Phi(x)e^{ikx}\\,dk,$$ $$\\Phi(k)=\\frac1{\\sqrt{2\\pi}}\\int \\Psi(x)e^{-ikx}\\,dx.$$ Through some interesting manipulation, one can arrive at $\\delta(k)=\\frac1{2\\pi}\\int e^{i kx}\\,dx.$ Just a broken version of the usual $\\delta(\\xi)=\\int e^{2\\pi i x\\xi}\\,dx.$ Now if we want to rewrite in terms of momentum we do something similar; now $k=\\frac1\\hbar p,$ so distributing that factor yields [!important] QM momentum space Fourier transform $$\\Psi(x)=\\frac1{\\sqrt{2\\pi\\hbar}}\\int \\Phi(p)e^{ipx/\\hbar}\\,dp,$$ $$\\Phi(p)=\\frac1{\\sqrt{2\\pi\\hbar}}\\int \\Psi(x)e^{-ipx/\\hbar}\\,dx.$$ Then by Parseval/Plancherel's theorem $\\int |f|^2\\,dx=\\int|\\hat f|^2\\,d\\xi,$ we get $$\\int|\\Psi|^2\\,dx=\\int|\\Phi|^2\\,dp.$$ Then $\\Phi$ denotes probability distribution of getting that particular momentum. If we take the wavenumber interpretation, $\\Phi$ becomes distribution of getting that particular wavenumber. Expectation Pretty simple, really. $$\\mathbb E(Q)=\\int Q(x)P(x)\\,dx=\\int Q(x)|\\Psi(x)|^2\\,dx=\\int \\Psi^ Q\\Psi\\,dx.$$ Then we have \"joint probabilities\" $\\langle f|g\\rangle$ which is basically $\\int f^ g\\,dx$. We denote expectation using $\\langle Q\\rangle.$ Time derivative plugged into Schrodinger's yields [!faq] ROC of Expectation $$\\frac{\\partial}{\\partial t}\\langle Q\\rangle=\\frac1{i\\hbar}\\left\\langle[\\hat Q,\\hat H]\\right\\rangle.$$ Uncertainty Computation We can now even plug-and-chug uncertainty, as follows. Proof 1 (smarter) First note that for arbitrary $u,v$ and constant $t,$ $\\int (u+tv)^2=\\int u^2+2t\\int uv+t^2\\int v^2\\geq 0,$ so the discriminant in $t$ must be nonpositive i.e. $(2\\int uv)^2\\leq 4(\\int u^2)(\\int v^2),$ yielding [!c] Cauchy-Schwarz Inequality $$\\int u^2\\int v^2\\geq \\int uv.$$ Recall that $A$ is Hermitian if $\\int \\phi^ A\\psi=\\int\\psi^ A\\phi.$ Now let $A_0$ be the expected for $A$, and $B_0$ the same for $B$. Assume that $A_0=B_0=0.$ We will not use this condition, and are justified because $\\mathbb E[(X-c)^2]$ in general is minimized when $c=\\mathbb E[X],$ so we don't lose information by moving the distribution/assuming the mean is $0$ and not using that condition later. Then $\\sigma_A\\sigma_b=\\langle |A|^2\\rangle\\langle |B|^2\\rangle.$ Note that $\\langle |A|^2\\rangle=\\int \\Psi^ A^ A\\Psi=\\int|A\\Psi|^2.$ Then using Cauchy's from above, $$ \\begin{aligned} \\int|A\\Psi|^2\\int|B\\Psi^2| &\\geq\\int|A\\Psi||B\\Psi|\\ &\\geq\\int\\left|\\text{Im}((A\\Psi)^ B\\Psi)\\right|\\ &\\geq\\left|\\text{Im}\\left(\\int(A\\Psi)^ B\\Psi\\right)\\right|\\ &=\\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right|. \\end{aligned} $$ Now because $A,B$ are Hermitian, $$ \\begin{aligned} \\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right| &=\\left|\\frac{\\int \\Psi^ BA\\Psi-\\Psi^ AB\\Psi}{2i}\\right|\\ &=\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. \\end{aligned} $$ Thus we've arrived at [!important] Robertson Uncertainty If $A,B$ are Hermitian operators, $$\\sigma_A\\sigma_B\\geq\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. $$ Finally, $$[\\hat x,\\hat p]=x\\frac\\hbar i\\frac{\\partial}{\\partial x}-\\frac\\hbar i\\frac{\\partial}{\\partial x} x=-\\frac\\hbar i=\\hbar i.$$ Therefore, [!important] Heisenberg Uncertainty $$\\sigma_x\\sigma_p\\geq\\frac\\hbar 2.$$ Proof 2 (dumber) As before, note that $\\sigma_A=\\int\\Psi^ (A-A_0)^2\\Psi=\\int\\Psi^ (A-A_0)^*(A-A_0)\\Psi=\\int|(A-A_0)\\Psi)|^2.$ Assume that $\\Psi(x),\\Phi(p)$ are 0-meaned. Then $\\sigma_x^2=\\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle,$ and $\\sigma_p^2=\\int p|\\Phi(p)|^2\\,dp.$ Note that by Parseval's, $\\int p|\\Phi(p)|^2\\,dp=\\langle\\mathcal F^{-1}(p\\Phi(p))\\mid\\mathcal F^{-1}(p\\Phi(p))\\rangle,$ so now we compute this function using integration by parts. First $$ \\begin{aligned} p\\Phi(p)\\sqrt{2\\pi\\hbar} &=\\int p\\Psi(x)e^{-ipx/\\hbar}\\,dx\\ &=\\left.i\\hbar\\Psi(x)e^{-ipx/\\hbar}\\right|_{-\\infty}^{\\infty} - i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx\\ &=-i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx. \\end{aligned} $$ Then $$ \\begin{aligned} \\mathcal F^{-1}(p\\Phi(p))(x) &=\\frac1{\\sqrt{2\\pi\\hbar}}\\int p\\Phi(p)e^{ipx/\\hbar}\\,dp\\ &=\\frac1{2\\pi\\hbar}\\int e^{ipx/\\hbar}\\,dp(-i\\hbar)\\int\\frac{\\partial\\Psi}{\\partial x'}e^{-ipx'/\\hbar}\\,dx'\\ &=\\frac1{2\\pi i}\\int \\frac{\\partial\\Psi}{\\partial x'}\\,dx'\\int e^{\\frac{ip(x-x')}\\hbar}\\,dp\\ &=\\frac{\\hbar}i\\frac{\\partial\\Psi}{\\partial x}\\ &=\\hat p\\Psi(x). \\end{aligned} $$ Finally, $$ \\begin{aligned} \\sigma_x^2\\sigma_p^2 &= \\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle\\langle \\hat p\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle\\ &\\geq |\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle|^2\\ &\\geq \\left(\\frac{\\langle x\\Psi(x)\\mid \\hat p\\Psi(x)\\rangle-\\langle \\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle}{2i}\\right)^2. \\end{aligned} $$ Then (because of magic and how self-adjoint operators like $\\hat p$ work) $$ \\begin{aligned} \\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle &=\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\Psi(x)\\mid \\hat p(x\\Psi(x))\\rangle\\ &=\\int x\\Psi^ \\frac\\hbar i\\Psi'-\\Psi^ \\frac\\hbar{i}\\frac{\\partial}{\\partial x}(x\\Psi)\\,dx\\ &=\\frac\\hbar i\\int -\\Psi\\Psi^*\\,dx\\ &=\\hbar i. \\end{aligned} $$ Thus $\\sigma_x^2\\sigma_p^2\\geq \\left(\\frac{\\hbar}2\\right)^2$ and $\\sigma_x\\sigma_p\\geq\\frac\\hbar2.$","title":"8.04.7 Momentum space, Expectation, Uncertainty 2"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#expectation","text":"Pretty simple, really. $$\\mathbb E(Q)=\\int Q(x)P(x)\\,dx=\\int Q(x)|\\Psi(x)|^2\\,dx=\\int \\Psi^ Q\\Psi\\,dx.$$ Then we have \"joint probabilities\" $\\langle f|g\\rangle$ which is basically $\\int f^ g\\,dx$. We denote expectation using $\\langle Q\\rangle.$ Time derivative plugged into Schrodinger's yields [!faq] ROC of Expectation $$\\frac{\\partial}{\\partial t}\\langle Q\\rangle=\\frac1{i\\hbar}\\left\\langle[\\hat Q,\\hat H]\\right\\rangle.$$","title":"Expectation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#uncertainty-computation","text":"We can now even plug-and-chug uncertainty, as follows.","title":"Uncertainty Computation"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#proof-1-smarter","text":"First note that for arbitrary $u,v$ and constant $t,$ $\\int (u+tv)^2=\\int u^2+2t\\int uv+t^2\\int v^2\\geq 0,$ so the discriminant in $t$ must be nonpositive i.e. $(2\\int uv)^2\\leq 4(\\int u^2)(\\int v^2),$ yielding [!c] Cauchy-Schwarz Inequality $$\\int u^2\\int v^2\\geq \\int uv.$$ Recall that $A$ is Hermitian if $\\int \\phi^ A\\psi=\\int\\psi^ A\\phi.$ Now let $A_0$ be the expected for $A$, and $B_0$ the same for $B$. Assume that $A_0=B_0=0.$ We will not use this condition, and are justified because $\\mathbb E[(X-c)^2]$ in general is minimized when $c=\\mathbb E[X],$ so we don't lose information by moving the distribution/assuming the mean is $0$ and not using that condition later. Then $\\sigma_A\\sigma_b=\\langle |A|^2\\rangle\\langle |B|^2\\rangle.$ Note that $\\langle |A|^2\\rangle=\\int \\Psi^ A^ A\\Psi=\\int|A\\Psi|^2.$ Then using Cauchy's from above, $$ \\begin{aligned} \\int|A\\Psi|^2\\int|B\\Psi^2| &\\geq\\int|A\\Psi||B\\Psi|\\ &\\geq\\int\\left|\\text{Im}((A\\Psi)^ B\\Psi)\\right|\\ &\\geq\\left|\\text{Im}\\left(\\int(A\\Psi)^ B\\Psi\\right)\\right|\\ &=\\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right|. \\end{aligned} $$ Now because $A,B$ are Hermitian, $$ \\begin{aligned} \\left|\\frac{\\int (A\\Psi)^ B\\Psi - (B\\Psi)^ A\\Psi}{2i}\\right| &=\\left|\\frac{\\int \\Psi^ BA\\Psi-\\Psi^ AB\\Psi}{2i}\\right|\\ &=\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. \\end{aligned} $$ Thus we've arrived at [!important] Robertson Uncertainty If $A,B$ are Hermitian operators, $$\\sigma_A\\sigma_B\\geq\\left|\\frac{\\langle [A,B]\\rangle}{2}\\right|. $$ Finally, $$[\\hat x,\\hat p]=x\\frac\\hbar i\\frac{\\partial}{\\partial x}-\\frac\\hbar i\\frac{\\partial}{\\partial x} x=-\\frac\\hbar i=\\hbar i.$$ Therefore, [!important] Heisenberg Uncertainty $$\\sigma_x\\sigma_p\\geq\\frac\\hbar 2.$$","title":"Proof 1 (smarter)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.7%20Momentum%20space%2C%20Expectation%2C%20Uncertainty%202/#proof-2-dumber","text":"As before, note that $\\sigma_A=\\int\\Psi^ (A-A_0)^2\\Psi=\\int\\Psi^ (A-A_0)^*(A-A_0)\\Psi=\\int|(A-A_0)\\Psi)|^2.$ Assume that $\\Psi(x),\\Phi(p)$ are 0-meaned. Then $\\sigma_x^2=\\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle,$ and $\\sigma_p^2=\\int p|\\Phi(p)|^2\\,dp.$ Note that by Parseval's, $\\int p|\\Phi(p)|^2\\,dp=\\langle\\mathcal F^{-1}(p\\Phi(p))\\mid\\mathcal F^{-1}(p\\Phi(p))\\rangle,$ so now we compute this function using integration by parts. First $$ \\begin{aligned} p\\Phi(p)\\sqrt{2\\pi\\hbar} &=\\int p\\Psi(x)e^{-ipx/\\hbar}\\,dx\\ &=\\left.i\\hbar\\Psi(x)e^{-ipx/\\hbar}\\right|_{-\\infty}^{\\infty} - i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx\\ &=-i\\hbar\\int\\frac{\\partial\\Psi}{\\partial x}e^{-ipx/\\hbar}\\,dx. \\end{aligned} $$ Then $$ \\begin{aligned} \\mathcal F^{-1}(p\\Phi(p))(x) &=\\frac1{\\sqrt{2\\pi\\hbar}}\\int p\\Phi(p)e^{ipx/\\hbar}\\,dp\\ &=\\frac1{2\\pi\\hbar}\\int e^{ipx/\\hbar}\\,dp(-i\\hbar)\\int\\frac{\\partial\\Psi}{\\partial x'}e^{-ipx'/\\hbar}\\,dx'\\ &=\\frac1{2\\pi i}\\int \\frac{\\partial\\Psi}{\\partial x'}\\,dx'\\int e^{\\frac{ip(x-x')}\\hbar}\\,dp\\ &=\\frac{\\hbar}i\\frac{\\partial\\Psi}{\\partial x}\\ &=\\hat p\\Psi(x). \\end{aligned} $$ Finally, $$ \\begin{aligned} \\sigma_x^2\\sigma_p^2 &= \\langle x\\Psi(x)\\mid x\\Psi(x)\\rangle\\langle \\hat p\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle\\ &\\geq |\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle|^2\\ &\\geq \\left(\\frac{\\langle x\\Psi(x)\\mid \\hat p\\Psi(x)\\rangle-\\langle \\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle}{2i}\\right)^2. \\end{aligned} $$ Then (because of magic and how self-adjoint operators like $\\hat p$ work) $$ \\begin{aligned} \\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\hat p\\Psi(x)\\mid x\\Psi(x)\\rangle &=\\langle x\\Psi(x)\\mid\\hat p\\Psi(x)\\rangle-\\langle\\Psi(x)\\mid \\hat p(x\\Psi(x))\\rangle\\ &=\\int x\\Psi^ \\frac\\hbar i\\Psi'-\\Psi^ \\frac\\hbar{i}\\frac{\\partial}{\\partial x}(x\\Psi)\\,dx\\ &=\\frac\\hbar i\\int -\\Psi\\Psi^*\\,dx\\ &=\\hbar i. \\end{aligned} $$ Thus $\\sigma_x^2\\sigma_p^2\\geq \\left(\\frac{\\hbar}2\\right)^2$ and $\\sigma_x\\sigma_p\\geq\\frac\\hbar2.$","title":"Proof 2 (dumber)"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.8%20Eigenstates%2C%20Hermitian%2C%20Observables%2C%20Measurement/","text":"Define $\\langle \\psi,\\phi\\rangle=\\int\\psi^*\\phi.$ Operator $Q$ is Hermitian if $\\langle\\psi,\\hat Q\\phi\\rangle=\\langle\\hat Q\\psi,\\phi\\rangle.$ We can analyze all eigenstates $\\psi$ sat. $\\hat Q\\psi=q\\psi.$ Turns out that such $\\psi_i$ can be organized to be orthonormal (usual linalg) By making assumptions on space, assume eigenstates generate whole space Then $\\phi=\\sum_i a_i\\psi_i,$ where $a_i=\\langle \\psi_i,\\phi\\rangle.$ The \"state\" now becomes ${a_i}.$ For instance, take momentum on a circle; eigenstates are countable $e^{ix\\theta}$ waves [!important] Measurement Postulate (Copenhagen interp.) Assuming $\\langle\\phi,\\phi\\rangle=1,$ given $\\phi=\\sum_i a_i\\psi_i,$ interpretation of \"operator\" is when we observe that value, we get $p_i=a_i^2$ probability of obtaining the eigenvalue $q_i.$ Notice that this actually matches interp. for $\\hat x,$ eigenstates are normalized delta functions. Uncertainty is as established/studied previously. Note that $\\Phi$ is an eigenstate iff $\\Delta \\hat Q_\\Phi=0.$ ROC of Uncertainty $\\Delta x=\\langle x^2\\rangle-\\langle x\\rangle^2.$ We know $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\langle x^2\\rangle &=\\frac1{i\\hbar}\\langle [x^2,\\hat H]\\rangle\\ &=\\frac1{i\\hbar}\\cdot-\\frac{\\hbar^2}{2m}\\langle -2+4x\\nabla\\rangle\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ \\end{aligned} $$ $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\ \\end{aligned} $$","title":"8.04.8 Eigenstates, Hermitian, Observables, Measurement"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.8%20Eigenstates%2C%20Hermitian%2C%20Observables%2C%20Measurement/#roc-of-uncertainty","text":"$\\Delta x=\\langle x^2\\rangle-\\langle x\\rangle^2.$ We know $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\langle x^2\\rangle &=\\frac1{i\\hbar}\\langle [x^2,\\hat H]\\rangle\\ &=\\frac1{i\\hbar}\\cdot-\\frac{\\hbar^2}{2m}\\langle -2+4x\\nabla\\rangle\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ &=\\frac\\hbar{im}(1+2\\int x\\nabla(\\Phi^2)\\,dx),\\ \\end{aligned} $$ $$ \\begin{aligned} \\frac{\\partial}{\\partial t}\\ \\end{aligned} $$","title":"ROC of Uncertainty"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/","text":"Recall Schrodinger equation; if we want a \"time independent\" or \"stationary\" state we need wavefunction ROC to be proportional to the wavefunction, i.e. an eigenstate of the Hamiltonian. More formally, [!important] Stationary States $\\Psi(x,t)=e^{-iEt/\\hbar}\\psi(x),$ where $E\\in\\mathbb R$ and $\\hat H\\psi=E\\psi.$ The exponential is actually a \"rotation\", just a scalar complex multiplication of norm 1. This preserves the normalization. The expectation value of a stationary state for a time-independent operator is itself time-independent; notice how the rotation cancels out in the integral $\\int \\Psi Q \\Psi^*.$ If we compose two stationary states we don't get another stationary state. Eigenstates for Hamiltonian are also called energy eigenstates . Characterization If we make assumptions on the potential, we can get info about the wavefunction. We will allow - Piecewise continuous - Delta functions - Infinities at hard walls As a result $\\psi'$ exists and has finitely many discontinuities. Free Particle on Circle We have $x\\in[0,L]$ a looped domain. Then we want to compute all of the stationary eigenstates. We get $e^{ikx}$ as an eigenstate for some $k,$ at which point $kL=2\\pi n$ from periodicity, so we've got our finite collection of eigenstates, and can characterize all solutions by Fourier expansion. (Discrete spread of eigenvalues, i.e. observables) We see that energy and momentum is quantized on this looped domain. Both $\\psi_n$ and $\\psi_{-n}$ have energy $E_n=\\frac{2\\pi^2\\hbar n^2}{mL^2},$ the eigenvalue from the Hamiltonian. The momentum eigenvalues of each of the eigenstates are different, so they must all be orthonormal. The resulting \"infinite vector\" of Fourier coefficients has norm 1.","title":"8.04.9 Time Independence, Free Particle on Circle"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/#characterization","text":"If we make assumptions on the potential, we can get info about the wavefunction. We will allow - Piecewise continuous - Delta functions - Infinities at hard walls As a result $\\psi'$ exists and has finitely many discontinuities.","title":"Characterization"},{"location":"Material%20Knowledge/Sciences/Physics/8.04/Messy%20Notes/8.04.9%20Time%20Independence%2C%20Free%20Particle%20on%20Circle/#free-particle-on-circle","text":"We have $x\\in[0,L]$ a looped domain. Then we want to compute all of the stationary eigenstates. We get $e^{ikx}$ as an eigenstate for some $k,$ at which point $kL=2\\pi n$ from periodicity, so we've got our finite collection of eigenstates, and can characterize all solutions by Fourier expansion. (Discrete spread of eigenvalues, i.e. observables) We see that energy and momentum is quantized on this looped domain. Both $\\psi_n$ and $\\psi_{-n}$ have energy $E_n=\\frac{2\\pi^2\\hbar n^2}{mL^2},$ the eigenvalue from the Hamiltonian. The momentum eigenvalues of each of the eigenstates are different, so they must all be orthonormal. The resulting \"infinite vector\" of Fourier coefficients has norm 1.","title":"Free Particle on Circle"},{"location":"Public%20Pages/homepage/","text":"All raw files available on Github . These files are a hand-picked subset of my personal Obsidian vault. About Me love food Sections Food \u2013 selected food and cooking notes","title":"Welcome"},{"location":"Public%20Pages/homepage/#about-me","text":"love food","title":"About Me"},{"location":"Public%20Pages/homepage/#sections","text":"Food \u2013 selected food and cooking notes","title":"Sections"}]}