<!doctype html>
<html data-bs-theme="">
    <head>
                <title>Matrix Calc - Alex Zhao's Public Pages</title>

            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">

            
            
            

            
                <link  rel="icon" type="image/x-icon" href="../../../../../assets/favicon.ico">
            

        <script>var base_url = '../../../../..';</script>
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

            <link rel="stylesheet" href="../../../../../assets/css/bootstrap.min.css">
            <link rel="stylesheet" href="../../../../../assets/css/root.min.css">
            <link rel="stylesheet" href="../../../../../assets/css/main.min.css">
            <link rel="stylesheet" href="../../../../../assets/css/media.min.css">
            <link rel="stylesheet" href="../../../../../assets/css/mkdocstrings.min.css">
                <link href="../../../../../extra.css" rel="stylesheet">
                <script src="../../../../../extra.js"></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                <script src="../../../../../search/main.js"></script>

            
                
                     <style>:root {--text: var(--color-white);}</style> 
                     <style>:root {--title: var(--color-orange);}</style> 
                     <style>:root {--primary: var(--color-orange);}</style> 
                     <style>:root {--background: var(--color-black);}</style> 
                
            
          
    </head>

    <body>
        <div class="container py-3">
            <header>
                    <!-- block header -->
<nav class="navbar navbar-expand-xl border-bottom">
    <div class="container-fluid">
        

        
            <span class=" fs-4 title-color site-name" id="component-site-name" style="text-transform: uppercase;">Alex Zhao's Public Pages</span>
        

        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsMenu"
            aria-controls="navbarsMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse flex-column ml-auto" id="navbarsMenu">
            <ul class="navbar-nav">

                <!-- block menu -->
                <li class="nav-item">
                    <!-- block menu -->
    
        <li class="nav-item" id="component-menu">
            <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="
                            nav-link text-gray text-decoration-none" href="../../../../../Public%20Pages/homepage/">[Welcome]</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Crypto]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Material Knowledge/Sciences/Coding/Crypto/Proving Systems (Plonky3, Halo2, Circle STARKs, Binius).md" class="dropdown-item text-decoration-none ">ZK Notes</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Coding]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../Sciences/Coding/Vim%20Tricks/" class="dropdown-item text-decoration-none ">Vim Tricks</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/" class="dropdown-item text-decoration-none ">USACO Notes</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../Sciences/ML/ML%20papers%20to%20read/" class="dropdown-item text-decoration-none ">ML papers</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Food and Cooking]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/" class="dropdown-item text-decoration-none ">糖醋排骨</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
            </ul>
        </li>
<!-- endblock -->
                </li>
                <!-- endblock -->

                <!-- block search -->
                <li class="nav-item">
                    <a class="collapsed" data-bs-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
                        <div class="md-search-icon">
                            <i class="fa fa-search" aria-hidden="true"></i>
                        </div>
                    </a>
                </li>
                <!--  endblock -->

                <!-- block source -->
                <li class="nav-item">
                    
                </li>
                <!--  endblock -->
            </ul>
        </div>
    </div>
</nav>
<!--  endblock -->
            </header>

            <main><!-- block search -->
<div class="collapse" id="collapseExample">
    <div role="search" class="search-box">
        <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
            <input type="text" name="q" class="search-query"
            placeholder="Search docs" title="Type search term here" />
        </form>
    </div>
</div>
<!-- endblock -->

                
                        <!-- block content -->
<section class="container post" id="component-content">
    <article>
        <header>
            
                <h1 class=" title" id="component-title">Matrix Calc</h1>
            
        </header>
        <p><p>Crucially high-dimensional derivative is defined as as linear form
$$
f'(x)[dx] = f(x+dx)-f(x)
$$
where $f'(x)$ is a linear form on $[dx].$ In vector spaces equipped with inner products the linear form can be represented as an inner product with an element of the vector space, and said element becomes the gradient.</p>
<p>Key thing to know is good ol product rule, which is derived only from distributive property and not commutativity hence we can use in matrix calc.</p>
<blockquote>
<p>Abstractions like Kronecker product and $vec$ are nice but at the end of it all is still Einstein summation lol.</p>
</blockquote>
<p>Kronecker product $A\otimes B$ is where each element $a_{ij}$ is replaced by $a_{ij}B$ so that output is $2\times 2$ mat with product'd dims. Then $vec(M)$ flattens a matrix into a vector columnwise.</p>
<p>Then for three matrices of the same dim, note that
$$
(A \otimes  B) \cdot vec(C^{\dagger})
$$
can be partitioned by rows of $A$. (Note that $vec C^{\dagger}$ is just flattening rowwise to match Kronecker rows of $B$.) For the first row of $A$ (hence first chunk of output) we get
$$
A_{1j}B_{kl}C_{jl}
$$
for element number $k$. Then rearranging yields
$$
vec(M)=(A \otimes  B) \cdot vec(C^{\dagger})\implies M_{ik}=A_{kj}B_{il}C_{jl}=B_{il}C^{\dagger}<em>{lj}A^{\dagger}</em>{jk},
$$
hence
$$
(A\otimes B)\cdot vec(C)=vec(BCA^{\dagger}).
$$</p>
<p>Can also do finite difference stuff. Sure. Can also do standard exponent tricks, e.g. $d(AA^{-1})=(dA)A^{-1}+A(dA^{-1})=0$ hence $dA^{-1}=-A^{-1}dAA^{-1}.$</p>
<p>Also remember that $\mathrm{Tr}(AB)=\mathrm{Tr}(BA)$ (just do sum of product elementwise of the "square intersection").</p>
<p>In the vector space of matrices with fixed dim, standard <em>Frobenius inner product</em> is $\left&lt; vec(A),vec(B) \right&gt; =Trace(A^{\dagger}B)$ and hence Frobenius norm is $\sqrt{ \mathrm{Tr}(A^{\dagger}A) }$.</p>
<p>Also recall that second derivative of $\mathbb{R}^{n}\to \mathbb{R}$ is called Hessian, gradient of $\mathbb{R}^{n}\to \mathbb{R}^{m}$ is Jacobian with shape $m\times n.$</p>
<h2 id="autograd">Autograd</h2>
<p>It's really cool. When using with libraries, could get a big boost from writing custom autograd for those parts, e.g. especially with Newton solvers (just implicit differentiate instead).</p>
<p>Goal is to get gradients of an output in terms of an input.
For long chains represented as computational DAGs there two ways—forward-mode diff and backward-mode diff.</p>
<p>Forward means start from input. Cost scales with number of inputs (for every elem compute change at elem for each input).
Backward means start from output, cost scales with outputs (for every elem compute change output for that elem). Still chain rule multiplicative, always making gradient longer with mul (not shorter with division).
In ML output is almost always 1 so backward is the best—gradient attached to each param has size 1.</p>
<p>Note that Hessian matrix is really annoying for $f: \mathbb{R}^{n}\to \mathbb{R}$ since $f'$ is $\mathbb{R}^{n}\to \mathbb{R}^{n}$ and so backward/forward on the second is the same. Hence "forward-over-backward". Use forward or backward depending on input/output shape.</p>
<h2 id="special-matrix-ops">Special Matrix Ops</h2>
<p>Don't forget determinant: defined recursively using either
- permutation parity
- recursively using "minors" (drop out row/column), sum using "cofactors" (use $-1^{(i+j)}$, sum along any row/column)
Then
$$
\nabla \det A=\det(A)A^{-\dagger}=\text{adj}(A^{\dagger})=\text{cofactor}(A)
$$
where $\text{cofactor}$ is the transpose of matrix of elementwise cofactors. Note that $A\cdot\text{cofactor}(A)=\det(A)I_{n}$ (self-evident).</p>
<p>Proof 1. Note that $\det(I+dA)-\det(I)=\mathrm{Tr}(dA),$ so
$$
\det(A+dA)=\det(A)(\det(I+A^{-1}dA))=\det(A)+\det(A)\mathrm{Tr}(A^{-1}dA)
$$
and hence
$$
d \det(A)=\left&lt; \det(A) A^{-\dagger},dA\right&gt;.
$$
Proof 2. $\det(A)=\sum_{j}A_{ij}C_{ij}$ for any $i$ so $d \det(A)=C_{ij}$ (lmao).</p>
<h2 id="functions-mathbbrnto-mathbbr-as-banach-spaces">Functions $\mathbb{R}^{n}\to \mathbb{R}$ as Banach Spaces</h2>
<p>Couple ideas: calculus of variations, random functions, second derivatives.
As always, the key idea is that a derivative is a different type; its a linear form, as oppposed to "another vector space element".</p>
<p>Calculus of variations: we can have functions $u: (\mathbb{R}\to \mathbb{R})\to \mathbb{R}$ that map functions $f$ to reals. Generally we can take $\int G(f,f',\dots,t)\,dx$. Then we want $\nabla u[df]=u(f+df)-u(f).$ Then we also care about $df', df'',\dots$ the derivatives of $f'$ and we can feed them directly into $G$, i.e.
$$
\nabla u=\int \nabla G[df,df',df'',\dots]\,dx
$$
Note that here, $df',df'',$ etc. are no longer "independent" in the usual sense. It's just that the $x$-wise term $\Delta G$ needs to know $\Delta (f')$ and that's expressed in $df'$.</p>
<p>Euler-Lagrange equations: if we have some "potential" expressed as 
$$
f(u)=\int F(u,u',x)\,dx
$$
with fixed endpoints, then at equilibrium for all $du$ we get
$$
0=df=\int \left(   \frac{dF}{du}[du]+\frac{dF}{du'}[du']\right)\,dx=\left. \frac{dF}{du'} du \right| + \int \left(   \frac{dF}{du}- \left(   \frac{dF}{du'}\right)' \right)du\,dx
$$
hence
$$
\int \frac{dF}{du}=\int \left( \frac{dF}{du'} \right) '.
$$</p>
<p>There's random functions, i.e. we feed some input and get a distribution back (that we sample from). Maybe we want to measure delta of expectation or smth, using Monte Carlo diffs. But, it can be very noisy if our two samples are uncorrelated. Hence correlate them with some confounding variable that preserved marginal but makes the joint difference really small, e.g. use CDF inverse.</p>
<p>Then there's bilinear maps. Take a function $f:\mathbb{R}^{n}\to \mathbb{R}.$ Second derivative could be represented as "differentiate first in $x_{i}$ then $x_{j}$" i.e. just Hessian matrix all over again.</p>
<p>However this doesn't work all that well. Consider taking second derivative of $\det(A).$ Then linear form is $\det(A)A^{-1}$ as derived earlier (either linear expansion around 0 or cofactor expansion). The second differential we denote $dA'$, so
$$
\begin{align<em>}
d^{2}\det(A)
&amp;=d(\left&lt; \det(A)A^{-\dagger}, dA \right&gt; )[dA']\
&amp;=d(\det(A)\mathrm{Tr}(A^{-1}dA))[dA']\
&amp;=d(\det(A))[dA']\cdot\mathrm{Tr}(A^{-1}dA)+\det(A)\cdot d(\mathrm{Tr}(A^{-1}dA))[dA']\
&amp;=\det(A)\mathrm{Tr}(A^{-1}dA')\mathrm{Tr}(A^{-1}dA) + \det(A)\mathrm{Tr}(d(A^{-1})[dA']dA)\
&amp;=\det(A)\left(\mathrm{Tr}(A^{-1}dA')\mathrm{Tr}(A^{-1}dA) - \mathrm{Tr}(A^{-1}dA'A^{-1}dA)  \right).
\end{align</em>}
$$
(I did that correctly first try wow) This is how it's done. Now $d^{2}\det(A)$ becomes a generalized bilinear form on $dA$ and $dA'$ as opposed to the dual of some linear form.</p>
<p>We can use these bilinear forms in Taylor expansions, i.e. $f(x)+f'(x)[\Delta x] + \frac{1}{2}f''(x)[\Delta x,\Delta x]$. Can also use this for "perfect gradient descent" i.e. Newton's method on the gradient; given gradient $g$ and Hessian $H$ just set $\Delta x=H^{-1}g.$</p></p>
    </article>
</section>
<!-- endblock -->
                
                
            </main>

            
                    <!-- block preview -->
<!-- endblock -->
            

            
                    <!-- block footer -->
<footer class="pt-4 my-md-5 pt-md-5 border-top" id="component-footer">
    <div class="row">
        <div class="col-12 col-md">
                <!-- block copyright -->

    <small class="d-block mb-3">
        Made with
        <a href="https://github.com/FernandoCelmer/mkdocs-simple-blog" target="_blank" rel="noopener">
            Simple Blog for MkDocs
        </a>
    </small>

<!-- endblock -->
        </div>
    </div>
</footer>
<!-- endblock -->
            
        </div>

            <script src="../../../../../assets/js/jquery-3.3.1.slim.min.js""></script>
            <script src="../../../../../assets/js/bootstrap.bundle.min.js""></script>
            <script src="../../../../../assets/js/main.min.js""></script>
                <script src="../../../../../extra.js" defer></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
                <script src="../../../../../search/main.js" defer></script>

    </body>

</html>