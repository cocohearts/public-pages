<!doctype html>
<html data-bs-theme="">
    <head>
                <title>6.7800.3 Info theory - Alex Zhao's Public Pages</title>

            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">

            
            
            

            
                <link  rel="icon" type="image/x-icon" href="../../../../../../assets/favicon.ico">
            
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

            <link rel="stylesheet" href="../../../../../../assets/css/bootstrap.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/root.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/main.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/media.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/mkdocstrings.min.css">
                <link href="../../../../../../extra.css" rel="stylesheet">
                <script src="../../../../../../extra.js"></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                <script src="../../../../../../search/main.js"></script>

            
                
                     <style>:root {--text: var(--color-white);}</style> 
                     <style>:root {--title: var(--color-orange);}</style> 
                     <style>:root {--primary: var(--color-orange);}</style> 
                     <style>:root {--background: var(--color-black);}</style> 
                
            
    </head>

    <body>
        <div class="container py-3">
            <header>
                    <!-- block header -->
<nav class="navbar navbar-expand-xl border-bottom">
    <div class="container-fluid">
        

        
            <span class=" fs-4 title-color site-name" id="component-site-name" style="text-transform: uppercase;">Alex Zhao's Public Pages</span>
        

        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsMenu"
            aria-controls="navbarsMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse flex-column ml-auto" id="navbarsMenu">
            <ul class="navbar-nav">

                <!-- block menu -->
                <li class="nav-item">
                    <!-- block menu -->
    
        <li class="nav-item" id="component-menu">
            <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="
                            nav-link text-gray text-decoration-none" href="../../../../../../Public%20Pages/homepage/">[Welcome]</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Crypto]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../../Material Knowledge/Sciences/Coding/Crypto/Proving Systems (Plonky3, Halo2, Circle STARKs, Binius).md" class="dropdown-item text-decoration-none ">ZK Notes</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Coding]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Sciences/Coding/Vim%20Tricks/" class="dropdown-item text-decoration-none ">Vim Tricks</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Sciences/Coding/USACO/General%20Thoughts%2C%20Paradigms/" class="dropdown-item text-decoration-none ">USACO Notes</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Sciences/ML/ML%20papers%20to%20read/" class="dropdown-item text-decoration-none ">ML papers</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Food and Cooking]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/" class="dropdown-item text-decoration-none ">糖醋排骨</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
            </ul>
        </li>
<!-- endblock -->
                </li>
                <!-- endblock -->

                <!-- block search -->
                <li class="nav-item">
                    <a class="collapsed" data-bs-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
                        <div class="md-search-icon">
                            <i class="fa fa-search" aria-hidden="true"></i>
                        </div>
                    </a>
                </li>
                <!--  endblock -->

                <!-- block source -->
                <li class="nav-item">
                    
                </li>
                <!--  endblock -->
            </ul>
        </div>
    </div>
</nav>
<!--  endblock -->
            </header>

            <main><!-- block search -->
<div class="collapse" id="collapseExample">
    <div role="search" class="search-box">
        <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
            <input type="text" name="q" class="search-query"
            placeholder="Search docs" title="Type search term here" />
        </form>
    </div>
</div>
<!-- endblock -->

                
                        <!-- block content -->
<section class="container post" id="component-content">
    <article>
        <header>
            
                <h1 class=" title" id="component-title">6.7800.3 Info theory</h1>
            
        </header>
        <p><h2 id="families">Families</h2>
<h3 id="exponential-families">Exponential families</h3>
<p>Usually used over discrete probability spaces, but also over continuous. Basically if we just ignore normalization and take geometric means across distributions. Yields an affine subspace in log-prob space. This is the "basis" interpretation. The parameter set is $\mathbb{R}^{n},$ and an $n-$parameter family of distributions is
$$
\exp \left{ \left(   \sum_{i}\lambda_{i}(x_{i})t_{i}(y) \right) - \alpha(x_{i})+\beta(y)\right} .
$$</p>
<p>Here $\alpha$ is <em>log-partition function</em>, $t$ is the <em>sufficient statistic</em>, $\lambda$ is <em>natural statistic</em>, and $\beta$ is <em>log base function</em>.</p>
<p>The single-parameter case has some nice properties; partial in $\alpha$ is $\mathbb{E}[t(y)],$ and $\ddot{\alpha}=\text{var}\left[ t(y) \right]=J_{y}(x).$</p>
<h3 id="linear-families">Linear families</h3>
<p>We can also take the linear nullspace interpretation. Take some indicators $\lambda,$ then $\mathbb{E}<em>{q</em>{x}}[ p(x)]=e_{0}$ becomes a linear constraint in the probability space.</p>
<h2 id="entropy-mutual-info">Entropy / Mutual Info</h2>
<p>We make choices. For instance most standard inference uses log-loss cost; we get a distribution for the parameter, and cost is log-prob of true value. E.g. for Bayesian we use $-A \log q(x)+B(x)$ where $A=1$ and $B=-\log p_{prior}.$</p>
<p>A cost (input truth, distribution) is <em>proper</em> if argmin is at the true Bayesian posterior. Meanwhile it is <em>local</em> if cost only depends on $x,q(x).$</p>
<p>Log-lsos turns out to be the only smooth, local, proper cost function. If we introduce discontinuities then uniform can do ok.</p>
<p>Here we can define entropy $H(p)=\mathbb{E}[-\log p]$, joint uncertainty $H(x,y)=\mathbb{E}<em>{p</em>{xy}}[-\log p],$ and mutual information $H(x)+H(y)-H(x,y),$ i.e. how much we learn about $x$ once we know $y$.</p>
<p>Always remember
$$
I(x;y,z)=I(x;z)+I(x;y|z).
$$
Note also that if $s$ is deterministic func of $x$ then $H(x)=H(s)+H(x|s).$ Here conditional entropy is weighted over joint distribution. Alternatively, think of it as sampling the conditioning variable $y$, then computing expected entropy over all resulting parameterized distributions $p_{x}(x;y).$</p>
<p>Hence recall that $$H(x|y)+I(x;y)=H(x),H(y|x)+I(x;y)=H(y), H(x)+H(y)-I(x;y)=H(x,y).$$
Recall that we can also do entropy for continuous variables. It's just defined as $\int_{-\infty}^{\infty}p(x)\log p(x)\,dx.$ It changes on coordinate shifts, but the value change is only a function of the transformation. Letting $x=g(s),$ we get $p(x)=\frac{p(s)}{\dot{g}(s)}$ so 
$$
\begin{align<em>}
h_{p,q}(x)
&amp;=-\int p\log q(x)\,dx\
&amp;=-\int \frac{p(s)}{\dot{g}(s)}\log \frac{q(s)}{\dot{g}(s)}\,\dot{g}(s)ds\
&amp;=\int p(s)\dot{g}(s)\,ds-\int p(s)\log q(s)\,ds\
&amp;=h_{p,q}(s)+\mathbb{E} [\dot{g}(s)].
\end{align</em>}
$$
Hence <em>all relative entropies, including KL and mutual info, are independent of variable changes.</em></p>
<h2 id="info-geometry">Info geometry</h2>
<p>Now we define a "distance," the KL/information divergence $D(p||q)=\mathbb{E}<em>{p(x)}[-\log q(x)+\log p(x)].$ Can be called "relative entropy" of $q$ to $p,$ or thought of as the entropy of the odds $\frac{q}{p}$ when $p$ is truth. Note that $I(x;y)=D(p</em>{xy}||p_{x}p_{y}).$</p>
<p>Note that the curvature of information divergence as parameterized by $x$ is determined by Fisher info. Specifically,</p>
<p>$$
\frac{\partial^{2}}{\partial x^{2}} D(p_{y}(y;x_{0})||p_{y}(y;x))=J_{y}(x).
$$
(Add a $\log e$ for bit-based systems.) Just rewrite as $\mathbb{E}<em>{p} \log p</em>{y}(y;x)- \mathbb{E}<em>{p} \log p</em>{y}(y;x_{0})$ and we immediately get $\mathbb{E}<em>{p} \frac{\partial^{2}}{\partial x^{2}}\log p</em>{y}(y;x)=J_{y}(x).$</p>
<p>Now we can use KL divergence to look at distances between distributions, geometry in distribution space, etc. As from [[6.7800.3 Info theory#Exponential families]], normalized distributions for an affine space because of sum to 1, and un-normalized exponential families are affine spaces in log-prob space.</p>
<p>The "projection" of $q$ onto a set $P$ is the "ground truth" $p$ that minimizes $D(p||q).$ $P$ is the set of hypotheses, and we want to find the one that "most easily explains" the data $q.$ Let $p^<em>$ be this projection. Take any other $p,$ then by walking from $p^</em>$ to $p$, derivative of $D(p||q)$ must be nonnegative.</p>
<p>We get $\sum_{y}(p-p^<em>) \log \frac{p^</em>}{q}=D(p||q)-D(p||p^<em>)-D(p^</em>||q)$ i.e.</p>
<blockquote>
<p>[!important] Pythagoras', Info Version
$$
D(p||q)\geq D(p||p^<em>)+D(p^</em>||q)
$$
for all $p \in P.$</p>
</blockquote>
<p>In other words, when trying to go $p\to p' \to q,$ always pick $p'=p^*$ i.e. greedy for the second step.</p>
<p>When $P$ is a [[6.7800.3 Info theory#Linear families]] we get equality, since we can walk either backwards or forwards and so the derivative must be 0.</p>
<p>Finally, we can precisely describe the set whose projection to a linear family is $q.$ Note that $$D(p||q_{2})=\mathbb{E}<em>{p} \log \frac{q}{p}+\log \frac{q</em>{2}}{q}=D(p||q)+\mathbb{E}<em>{p}\log \frac{q</em>{2}}{q}.$$
Hence if $\mathbb{E}<em>{p} \log \frac{q</em>{2}}{q}$ is independent of $q$ then the projection remains the same. Equivalently $\log \frac{q_{2}}{q}$ must be some linear combination of $t_{i}.$</p>
<blockquote>
<p>[!important] Orthogonality Correspondence
If the linear family has constraints $\mathbb{E}_{p}[t(y)]=\overline{t}$ for some vector-valued function $t,$ then the set orthogonal to fixed $p^<em>$ is the exponential family $p^</em>(y)\exp \left{ x^{\dagger}t(y)-\alpha(x) \right}$ for free vector parameters $x.$</p>
</blockquote>
<p>We can also represent EM algorithm in info geometry. The $E$ step becomes
$$
\hat{p}<em>{z}^{(l-1)}=\arg\min</em>{p_{z}} D(p_{z}||p_{z}(\cdot,\hat{x}^{l-1})),
$$
$$
\hat{x}^{l} = \arg\min_{x} D(\hat{p})
$$</p></p>
    </article>
</section>
<!-- endblock -->
                
                
            </main>

            
                    <!-- block preview -->
<!-- endblock -->
            

            
                    <!-- block footer -->
<footer class="pt-4 my-md-5 pt-md-5 border-top" id="component-footer">
    <div class="row">
        <div class="col-12 col-md">
                <!-- block copyright -->

    <small class="d-block mb-3">
        Made with
        <a href="https://github.com/FernandoCelmer/mkdocs-simple-blog" target="_blank" rel="noopener">
            Simple Blog for MkDocs
        </a>
    </small>

<!-- endblock -->
        </div>
    </div>
</footer>
<!-- endblock -->
            
        </div>

            <script>var base_url = '../../../../../..';</script>
            <script src="../../../../../../assets/js/jquery-3.3.1.slim.min.js""></script>
            <script src="../../../../../../assets/js/bootstrap.bundle.min.js""></script>
            <script src="../../../../../../assets/js/main.min.js""></script>
                <script src="../../../../../../extra.js" defer></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
                <script src="../../../../../../search/main.js" defer></script>

    </body>

</html>