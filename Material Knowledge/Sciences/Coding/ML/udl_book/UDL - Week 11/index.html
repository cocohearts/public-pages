<!doctype html>
<html>
    <head>
                <title>UDL   Week 11 - Alex Zhao's Public Pages</title>

            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">

            
            
            

            
                <link  rel="icon" type="image/x-icon" href="../../../../../../favicon.ico">
            
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
                        <script>hljs.initHighlightingOnLoad();</script>
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/bootstrap.min.css">
            <link rel="stylesheet" href="../../../../../../assets/css/main.min.css">
                <link href="../../../../../../extra.css" rel="stylesheet">
                <script src="../../../../../../extra.js"></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                <script src="../../../../../../search/main.js"></script>

            
                
                     <style>:root {--text: var(--color-white);}</style> 
                     <style>:root {--title: var(--color-orange);}</style> 
                     <style>:root {--primary: var(--color-orange);}</style> 
                     <style>:root {--background: var(--color-black);}</style> 
                
            
    </head>

    <body>
        <div class="container py-3">
            <header>
                    <!-- block header -->
<nav class="navbar navbar-expand-xl border-bottom">
    <div class="container-fluid">
        

        
            <span class=" fs-4 title-color site-name" id="component-site-name" style="text-transform: uppercase;">Alex Zhao's Public Pages</span>
        

        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsMenu"
            aria-controls="navbarsMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse flex-column ml-auto" id="navbarsMenu">
            <ul class="navbar-nav">

                <!-- block menu -->
                <li class="nav-item">
                    <!-- block menu -->
    
        <li class="nav-item" id="component-menu">
            <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="
                            nav-link text-gray text-decoration-none" href="../../../../../../Public Pages/homepage.md">[Welcome]</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Crypto]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../Crypto/Proving%20Systems%20%28Plonky3%2C%20Halo2%2C%20Circle%20STARKs%2C%20Binius%29/" class="dropdown-item text-decoration-none ">ZK Notes</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Coding]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../Vim%20Tricks/" class="dropdown-item text-decoration-none ">Vim Tricks</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../USACO/General%20Thoughts%2C%20Paradigms/" class="dropdown-item text-decoration-none ">USACO Notes</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Food and Cooking]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../../Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/" class="dropdown-item text-decoration-none ">糖醋排骨</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
            </ul>
        </li>
<!-- endblock -->
                </li>
                <!-- endblock -->

                <!-- block search -->
                <li class="nav-item">
                    <a class="collapsed" data-bs-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
                        <div class="md-search-icon">
                            <i class="fa fa-search" aria-hidden="true"></i>
                        </div>
                    </a>
                </li>
                <!--  endblock -->

                <!-- block source -->
                <li class="nav-item">
                    
                </li>
                <!--  endblock -->
            </ul>
        </div>
    </div>
</nav>
<!--  endblock -->
            </header>

            <main><!-- block search -->
<div class="collapse" id="collapseExample">
    <div role="search" class="search-box">
        <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
            <input type="text" name="q" class="search-query"
            placeholder="Search docs" title="Type search term here" />
        </form>
    </div>
</div>
<!-- endblock -->
                    <!-- block content -->
<section class="container post">
    <article>
        <header>
            
                <h1 class=" title" id="component-title">UDL   Week 11</h1>
            
        </header>
        <p><h2 id="chapter-19-rl">Chapter 19: RL</h2>
<p>So we have <em>partially observable Markov decision processes</em>, where
- at each timestep world state encoded in $s$, fixes a set of actions
- each action $a$ has distribution describing subsequent state
- actions can have reward $r$
- observations $o$ drawn from distribution based on state</p>
<p>A policy $\pi[a|s]$ is the distribution of next actions by state. We estimate the value of future rewards with a reverse interest rate $\gamma,$ and so can define the value of a sequence of actions or a policy from a given starting point. Also define $q$, the value from given state with given action.</p>
<p>Relations are
$$
\begin{align}
v[s_{t}]&amp;=\sum_{a_{t}}\pi[a_{t}|s_{t}]q[s_{t},a_{t}], \
q[s_{t},a_{t}]&amp;=r[s_{t},a_{t}]+\gamma \sum Pr(s_{t+1}|s_{t},a_{t})v[s_{t+1}].
\end{align}
$$</p>
<h3 id="markov-decision-problems-full-state-known">Markov Decision Problems (full state known)</h3>
<h4 id="direct-dp">Direct DP</h4>
<p>Now, if we know the entire action-&gt;state distribution for all states, we can initialize values for $v$ randomly, sweep through by plugging in $q$ into the first equation once, and then greedily update $\pi$. This eventually converges.</p>
<h4 id="monte-carlo">Monte Carlo</h4>
<p>Set policy randomly, then after each run of experimentation, for each pair $a,s$ update the policy to value of $a$ that yielded the best measured average $q$ over all runs.
However, this requires us to see every $a,s$ pair many times, which is not always possible.</p>
<h4 id="temporal-difference">Temporal Difference</h4>
<p>Live updating while doing Monte Carlo, so that the new value of $q[s_{t},a_{t}]$ is the old value plus $\alpha$ times the delta in expectation.</p>
<p>SARSA is $(r[s_{t},a_{t}]+\gamma\cdot q[s_{t+1},a_{t+1}])-q[s_{t},a_{t}].$</p>
<p>Instead we could do <em>off-policy</em>, meaning the policy that is acting is "behavioral" whereas optimal "target" is being learned.
Typically, behavioral is stochastic but target is deterministic. Behavioral can be modified by e.g. <em>epsilon greedy</em> where pick greedy with chance $1-\epsilon,$ random otherwise.</p>
<p>Then we have some behavioral, but then $q[s_{t+1},a_{t+1}]$ is replaced with the best move (target policy), $\max_{a}(q[s_{t+1},a]).$ This is called <em>Q-learning</em>, however faces the same size issue as Monte Carlo.</p>
<h4 id="fitted-q-learning">Fitted Q-learning</h4>
<p>Represent state $s_{t}$ as a vector, put into model $q$ with parameters $\phi$. Then loss $L(\phi)$ is the average TD delta $\left( r[s_{t},a_{t}]+\gamma\cdot \max_{a}(q[s_{t+1},a]) \right) -q[s_{t},a_{t}]$ squared.</p>
<p>Then $\phi$ gets updated using partial of this.</p>
<h3 id="abandon-action-values">Abandon Action Values</h3>
<p>Let's just do estimation of policy effectiveness. That would be expected discounted reward over the whole trajectory (measuring state and action). Then after taking $I$ many samples and gradient ascent, we get the addition to $\theta$ of
$$
\begin{align}
\alpha\cdot \frac{1}{I}\sum_{i}^I \frac{r[\tau_{i}]}{Pr(\tau_{i}|\theta)} \frac{{\partial Pr(\tau_{i}|\theta)}}{\partial\theta}
&amp;= \alpha \cdot Mean \left[ r[\tau]\frac{{\partial \ln(Pr[\tau|\theta])}}{\partial\theta} \right]  \
&amp;=\alpha \cdot Mean\left[ r[\tau] \sum_{i}\frac{\partial \ln(Pr[s_{i+1}|s_{i},\theta])}{\partial\theta} \right] .
\end{align}
$$
The <em>REINFORCE</em> algorithm does this for a discrete action space, using a softmaxxed neural net for probabilities. Walking through Monte-Carlo style and updating $\theta$ does the trick.</p>
<p>Another good thing to do is to just train a parallel value model that predicts value of a state $s_{t}$ and replace $r[\tau_{it}]$ with $r[\tau_{t}]-b[s_{t}].$ This compensates for some states just being better than others.</p>
<p>To do this at each step using temporal difference, we can update $\theta$ using $\alpha(r[s_{t},a_{t}]+\gamma v(s_{t+1})-v(s_{t}))$, where $v(s,\phi)$ is a parallel network predicting each state's value.</p>
<p>Now $\pi[s_{t},\theta]$ is the "actor" and $v[s_{t},\phi]$ is the "critic". </p>
<h3 id="offline-rl">Offline RL</h3>
<p>Where we want to build a model based on past, noninteractive transcripts with the environment. This is new, and rather hard. The above is one paradigm; sequence prediction with transformers or LSTM is another, called "decision transformer," which tries to predict the reward for each action.</p></p>
    </article>
</section>
<!-- endblock -->
            </main>

            
                    <!-- block preview -->
<!-- endblock -->
            

            
                    <!-- block footer -->
<footer class="pt-4 my-md-5 pt-md-5 border-top" id="component-footer">
    <div class="row">
        <div class="col-12 col-md">
                <!-- block copyright -->

    <small class="d-block mb-3">
        Made with
        <a href="https://github.com/FernandoCelmer/mkdocs-simple-blog" target="_blank" rel="noopener">
            Simple Blog for MkDocs
        </a>
    </small>

<!-- endblock -->
        </div>
    </div>
</footer>
<!-- endblock -->
            
        </div>

            <script>var base_url = '../../../../../..';</script>
            <script src="../../../../../../assets/js/jquery-3.3.1.slim.min.js""></script>
            <script src="../../../../../../assets/js/bootstrap.bundle.min.js""></script>
            <script src="../../../../../../assets/js/main.min.js""></script>
                <script src="../../../../../../extra.js" defer></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
                <script src="../../../../../../search/main.js" defer></script>

    </body>

</html>