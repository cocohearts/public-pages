<!doctype html>
<html data-bs-theme="">
    <head>
                <title>ML papers - Alex Zhao's Public Pages</title>

            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">

            
            
            

            
                <link  rel="icon" type="image/x-icon" href="../../../../assets/favicon.ico">
            
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

            <link rel="stylesheet" href="../../../../assets/css/bootstrap.min.css">
            <link rel="stylesheet" href="../../../../assets/css/root.min.css">
            <link rel="stylesheet" href="../../../../assets/css/main.min.css">
            <link rel="stylesheet" href="../../../../assets/css/media.min.css">
            <link rel="stylesheet" href="../../../../assets/css/mkdocstrings.min.css">
                <link href="../../../../extra.css" rel="stylesheet">
                <script src="../../../../extra.js"></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                <script src="../../../../search/main.js"></script>

            
                
                     <style>:root {--text: var(--color-white);}</style> 
                     <style>:root {--title: var(--color-orange);}</style> 
                     <style>:root {--primary: var(--color-orange);}</style> 
                     <style>:root {--background: var(--color-black);}</style> 
                
            
    </head>

    <body>
        <div class="container py-3">
            <header>
                    <!-- block header -->
<nav class="navbar navbar-expand-xl border-bottom">
    <div class="container-fluid">
        

        
            <span class=" fs-4 title-color site-name" id="component-site-name" style="text-transform: uppercase;">Alex Zhao's Public Pages</span>
        

        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsMenu"
            aria-controls="navbarsMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse flex-column ml-auto" id="navbarsMenu">
            <ul class="navbar-nav">

                <!-- block menu -->
                <li class="nav-item">
                    <!-- block menu -->
    
        <li class="nav-item" id="component-menu">
            <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="
                            nav-link text-gray text-decoration-none" href="../../../../Public%20Pages/homepage/">[Welcome]</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Crypto]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../../Material Knowledge/Sciences/Coding/Crypto/Proving Systems (Plonky3, Halo2, Circle STARKs, Binius).md" class="dropdown-item text-decoration-none ">ZK Notes</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class=" active 
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Coding]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../Coding/Vim%20Tricks/" class="dropdown-item text-decoration-none ">Vim Tricks</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../Coding/USACO/General%20Thoughts%2C%20Paradigms/" class="dropdown-item text-decoration-none ">USACO Notes</a>
    </li>
<!-- endblock -->
                                    <!-- block dropdown-menu -->
    <li>
        <a href="./" class="dropdown-item text-decoration-none  active ">ML papers</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="
                            nav-link dropdown-toggle text-decoration-none" href="#" data-bs-toggle="dropdown">[Food and Cooking]</a>
                            <ul class="dropdown-menu">
                                    <!-- block dropdown-menu -->
    <li>
        <a href="../../../Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/" class="dropdown-item text-decoration-none ">Á≥ñÈÜãÊéíÈ™®</a>
    </li>
<!-- endblock -->
                            </ul>
                        </li>
            </ul>
        </li>
<!-- endblock -->
                </li>
                <!-- endblock -->

                <!-- block search -->
                <li class="nav-item">
                    <a class="collapsed" data-bs-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
                        <div class="md-search-icon">
                            <i class="fa fa-search" aria-hidden="true"></i>
                        </div>
                    </a>
                </li>
                <!--  endblock -->

                <!-- block source -->
                <li class="nav-item">
                    
                </li>
                <!--  endblock -->
            </ul>
        </div>
    </div>
</nav>
<!--  endblock -->
            </header>

            <main><!-- block search -->
<div class="collapse" id="collapseExample">
    <div role="search" class="search-box">
        <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
            <input type="text" name="q" class="search-query"
            placeholder="Search docs" title="Type search term here" />
        </form>
    </div>
</div>
<!-- endblock -->

                
                        <!-- block content -->
<section class="container post" id="component-content">
    <article>
        <header>
            
                <h1 class=" title" id="component-title">ML papers</h1>
            
        </header>
        <p><p>https://arxiv.org/pdf/2503.14476
dreamcoder
https://sander.ai/2025/04/15/latents.html
https://sander.ai/2024/06/14/noise-schedules.html
sesame paper
<a href="https://arxiv.org/pdf/2210.01776">diffdock</a>
<a href="https://arxiv.org/abs/2410.13643">discrete diff for proteins</a>
af3 architecture, with config
rosettafold architecture, with config
proteinmpnn architecture, with config
boltz-1
simulating time w sqrt space
"AI and the future of work" pdf in downloads from us national academies
tencent t1
qwen 2.5 omni
gemma
<a href="https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf">nvidia whitepaper</a>
tarflow
thunderkittens
https://arxiv.org/pdf/2503.07565
https://www.arxiv.org/pdf/2503.03965
- [x] https://openai.com/index/chain-of-thought-monitoring/
grok linear transformers, mamba
<a href="https://arxiv.org/abs/2006.10503">SE3 transformers for rfdiffusion</a>
https://diffusion.csail.mit.edu/
https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf
https://arcinstitute.org/news/blog/evo2
https://arxiv.org/pdf/2502.09609
https://huggingface.co/spaces/nanotron/ultrascale-playbook
https://arxiv.org/pdf/2412.15287
sana, again
openvla
pizero source sent by ge
moviegen
<a href="https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f">prime</a>
video transformer arch
flows, euler flow, etc. read flux codebase
flow-dpm
titans from gr
boyuan diffusion papers (done)</p>
<p><a href="https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f">prime</a>
<a href="https://arxiv.org/pdf/2501.00656">olmo</a>
<a href="https://arxiv.org/abs/2501.01005">flashinfer</a>
srush tensor puzzles
<a href="https://github.com/facebookresearch/lingua">meta lingua</a>
deepseek paper again
learn triton
<a href="https://maharshi.bearblog.dev/optimizing-softmax-cuda/">triton softmax</a>
srush triton puzzles
read math in <a href="https://probml.github.io/pml-book/toc1.pdf">prob ml</a>
- [x] ditto
    - can backprop differentiable losses on diffusion model outputs to noise inputs! and get desired outputs
Large Memory Layers with Product Keys
https://arxiv.org/pdf/1907.05242
Patchscope: A Unifying Framework for Inspecting Hidden Representations of LLMs
https://arxiv.org/pdf/2401.06102
Recovering the Pre-Fine-Tuning Weights of Generative Models
https://arxiv.org/abs/2402.10208
Location Verification for AI chips
https://static1.squarespace.com/static/64edf8e7f2b10d716b5ba0e1/t/6670467ebe2a477eb1554f40/1718634112482/Location%2BVerification%2Bfor%2BAI%2BChips.pdf
Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
https://arxiv.org/pdf/1703.10593
Improving Alignment and Robustness with Circuit Breakers (s/o rowan for coauthoring)
https://arxiv.org/pdf/2406.04313
Quantization Model of Neural Scaling (s/o Uzay)
https://arxiv.org/pdf/2303.13506
Gradient Routing (s/o Jacob)
https://arxiv.org/pdf/2410.04332
Binlear MLPs for Interp
https://openreview.net/pdf?id=gI0kPklUKS</p>
<p>Not really AI:
Anatomy of a Bit: Information in a Time Series Observation
https://arxiv.org/pdf/1105.2988
Prediction, Retrodiction, and The Amount of Information Stored in the Present
https://arxiv.org/abs/0905.3587</p>
<p>sasha rush awesome (outdated?)
https://github.com/huggingface/awesome-papers
long awesome ML
https://github.com/aimerou/awesome-ai-papers?tab=readme-ov-file
o1-related
https://github.com/srush/awesome-o1/
awesome ML tools
https://github.com/srush/awesome-machine-learning
awesome discrete diffusion
https://github.com/kuleshov-group/awesome-discrete-diffusion-models</p>
<h2 id="sysinference-optimization-large-text-models">sys/inference optimization, large text models</h2>
<ul>
<li>[ ] PRIME</li>
<li>[ ] grok 1</li>
<li>[ ] siglip</li>
<li>[x] <a href="https://arxiv.org/abs/2205.14135">flashattention</a><ul>
<li>want to focus on minimizing i/o writes to and from gpu vram</li>
<li>the goal is to do chunked attention QV muls in the cache close to the gpu</li>
<li>make multiple passes over the output, each time updating by adjusting softmax denom and adding new term</li>
<li>obv. keep running log of the softmax denom and have to re-read on every block</li>
<li>prefetch/kernel fuse as desired</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2501.00656">olmo</a><ul>
<li>training curriculum with general text/fineweb-edu, then text for specific tasks to push benchmark scores</li>
<li>stability tricks: Z-score regularization, gradient clipping, KV-norm, RMSnorm, no biases, model soups, careful model init, low lr (1e-8), post-norm, weight decay, dropout</li>
</ul>
</li>
<li>what i learned from william brandon:<ul>
<li>nvidia gpus have two caches, L2 and L1</li>
<li>each streaming multiprocessor has its own L1 physically colocated with the SM</li>
<li>in the time it takes to send data between L1 and SM, can do 100 tensor operations</li>
<li>four datatypes to be aware of:<ul>
<li>fp32, horrible; tf32, better than bf16 but takes more memory</li>
<li>fp16, really not great; bf16, what everyone uses</li>
</ul>
</li>
<li>bf16 (if not careful) will get converted into fp32 which is horrible</li>
<li>V100s are horrible for the above reason</li>
<li>also talked about modular duality, lipschitz stuff from jeremy + laker newhouse, new optimizers</li>
<li>life advice: do stuff, make stuff, remake primitives from scratch, when possible if i'm scared of something just do it</li>
<li>also learn cuda in 100hrs in <a href="https://accelerated-computing-class.github.io/fall24/labs/">6.S894 labs</a> (use 1xRTX A4000)</li>
</ul>
</li>
<li>[ ] ray python dist package</li>
<li>[ ] deepspeed</li>
<li>[ ] megatron</li>
<li>[ ] <a href="https://arxiv.org/abs/2411.19722">jetformer</a></li>
<li>[ ] ddp, fsdp, megatron, gpipe</li>
<li>[ ] <a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">what to knwo about cpu memory</a></li>
<li>[ ] flexattention</li>
<li>[x] streamingllm, linformer<ul>
<li>linformer just projects $n\times d$ key, weight matrices into $k\times d$ using direct linear $n\to k$ linear map ("squishing" vectors to make the seq shorter)</li>
<li>streamingllm analyzed attention magnitudes, found that "leftover" attention would be dumped into "attention sinks" in the softmax which would then be ignored</li>
<li>added initial tokens back to inference, perplexity fixed (compared to sliding window)</li>
</ul>
</li>
<li>[ ] <a href="https://arxiv.org/pdf/2211.17192">speculative decoding</a></li>
<li>[ ] <a href="https://arxiv.org/pdf/2309.00071">yarn</a><ul>
<li>uses RoPE (instead of adding pos. embedding, multiply by $D/2$ complex numbers with AP magnitude)</li>
<li>pretrain on 10K context/RoPE, then interpolate arguments of complex numbers in pos. embedding carefully</li>
<li>preserve higher frequency numbers and move lower </li>
</ul>
</li>
<li>[ ] <a href="https://arxiv.org/pdf/2410.10254">lolcats</a></li>
<li>[ ] deepseek v3<ul>
<li>MTP, Yarn, "long-winded and overly reflective" R1 with system prompt during SFT for transfer, carefully load balanced MoE</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2411.13676">hymba</a><ul>
<li>combine SSM and transformer <em>side by side</em> as opposed to in sequence, has good "synergy" because of opposing behaviors</li>
<li>Transformers arch with LN, LinUp, split SSM/Attn, LinDown, (+), LN, FFN, (+)</li>
<li>(N-3)/2 SWA Hymba blocks on each side of global attention</li>
</ul>
</li>
</ul>
<h2 id="interp">interp</h2>
<ul>
<li>[ ] <a href="https://papers.nips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf">automated circuit discovery</a></li>
<li>[ ] <a href="https://arxiv.org/pdf/2304.05969">path patching</a></li>
<li>[ ] <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">the logit lens (blogpost)</a></li>
<li>[ ] <a href="https://nnsight.net/about/">nnsight docs</a></li>
<li>[x] <a href="https://arxiv.org/pdf/2404.14349">achyuta's sts paper</a><ul>
<li>uses cross-layer-attribution matrix based on gradient of next layer on each neuron in current</li>
<li>take top $k$ in each layer locally, then iteratively refine by taking last layer, grab most influential neurons from prev, etc. go backwards, then go forwards, until circuit stops changing</li>
<li>stich images from two classes into a single image, analyze behavior of both circuits under this hybrid image
<strong>FROM TAMAR</strong></li>
</ul>
</li>
<li>[x] <a href="https://anishk23733.github.io/vl-interp/">https://anishk23733.github.io/vl-interp/</a>  <ul>
<li>VLM embeds images and places before text in transformer decoder</li>
<li>hence image embeddings are in the same "space" as text embeddings</li>
<li>goal: measure "internal confidence" about an object in an image, potentially ablate</li>
<li>defines "internal confidence" about an object in an image as maximum of logit lens (result of applying final unembedding matrix and grabbing logit for token for that object) over all image embeddings (one for each patch) over all layers in the VLM</li>
<li>cutoff at a certain internal confidence, and then ablate/activate based on that</li>
<li>questions:<ul>
<li>why are all layers in the model treated equally? can we learn a better combination of information to get internal confidence in a self-supervised way?</li>
<li>what about image encoders that don't keep spatial information in an easily interpretable way, how to check segmentation for these?</li>
<li>logit lens seems "naive"‚Äîwhy do we expect image tokens to speak "exactly" the "same language"? should we try to learn adjustments to the embedding matrix, esp. wrt. position?</li>
<li>good results for end task‚Äîhallucination reduction‚Äîbut gap between hallucation reduction v.s. cd reduction seems small, perhaps can be made more effective with ablation engineering or internal confidence engineering</li>
</ul>
</li>
</ul>
</li>
<li>[x] https://vision-of-vlm.github.io/ <ul>
<li>"the mechanisms underlying how VLMs process visual information remain largely unexplored."</li>
<li>through a creative "knockout" experiment, demonstrate implicit image information through text tokens is sufficient, alternatively image tokens only needed for layers 20-40</li>
<li>in layers 20-40 attention values make an attempt at segmentation</li>
<li>able to get "compressed context" by, for each layer, take only the image tokens with top-K attention values</li>
<li>notably works much better with InternVL2-76B (patches into 40x40) than with LLaVa-1.5-7B (patches in 16x16), suggesting that large patching is just very redundant/inefficient and should no longer be pursued</li>
<li>question: if we take the best tokens for <em>each</em> layer then perhaps over all layers we end up including all image tokens at least once? even worse, if we change which image tokens are included on inference for each text token, perhaps over all text tokens, each layer eventually sees all image tokens? also perhaps these results are to be "expected" because of dropout training schedules?</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2410.07149">https://arxiv.org/pdf/2410.07149</a><ul>
<li>adapter VLM that maps CLIP patch embeddings into language token space</li>
<li>refers to a register token hypothesis‚Äîthat blank background patches collect global information; relegates as a side effect</li>
<li>uses three methods of checking token understanding, finds that ablating object tokens (replacing with global mean) removes understanding‚Äîexpected; how to know there's a dog when there are no dog patches?</li>
<li>uses logit lens, finds that tokens become aligned with corresponding text embeddings as layers progress (surprising! no compulsion to align with text, some implicit discovery of "best coding" for each patch)</li>
<li>most valuable communication between image and text tokens happens in later layers (attention knockout)</li>
<li>i'm surprised they discard register hypothesis so quickly, i believe register</li>
</ul>
</li>
<li>
<p>[x] <a href="https://arxiv.org/abs/2310.10348">https://arxiv.org/abs/2310.10348</a></p>
<ul>
<li>general program of ACDC: Automated Circuit DisCovery</li>
<li>activation patching does one forward pass for every edge/node/wtv. that has to be tweaked, replacing activation at that node with either than average ablation or the activation from some other "corrupted" prompt, sees what the change in metric is</li>
<li>attribution patching uses derivative of metric in activation at each node/edge to construct a circuit</li>
<li>represent network as a computational graph, differentiate between modules, nodes, edges</li>
<li>specifically use EAP, edge attribution patching</li>
<li>measure false positive rate/true positive rate from human expert baseline, both increase as %pruning increases</li>
<li>two approaches to calculating relevance: take a gradient all the way to the top, or take gradient of next layer(s) from this layer and compute recursively</li>
<li>recursive is easier to ensure selected subgraph is actually connected</li>
</ul>
</li>
<li>
<p>[ ] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf">A Mathematical Framework for Transformer Circuits</a></p>
</li>
</ul>
<h2 id="rl-agentsautomation">rl agents/automation</h2>
<ul>
<li>[ ] <a href="https://arxiv.org/abs/2408.06292">sakana</a></li>
<li>[ ] <a href="https://diffusion-policy.cs.columbia.edu/">diffusion policy</a><ul>
<li>use diffusion to denoise action latents that guide some agent in rl</li>
<li>for whatever reason diffusion is rlly good lol idk</li>
</ul>
</li>
</ul>
<h2 id="roboticsrl">robotics/rl</h2>
<ul>
<li>[ ] <a href="https://arxiv.org/abs/2410.23208">kinetix rl physics lib</a></li>
<li>[ ] <a href="https://mobile-aloha.github.io/resources/mobile-aloha.pdf">aloha robotics</a></li>
<li>[ ] <a href="https://arxiv.org/abs/2406.09246">openvla</a></li>
<li>[x] stockfish vs alphazero<ul>
<li>stockfish has a hardcoded/manually finetuned eval, uses early pruning on a huge (depth 18, 20) MCTS to calculate minimax value</li>
<li>alphazero uses a CNN to model, has a policy and value head, policy head is policy backprop'd against "stronger version" of itself from MCTS, value head is pushed up to rollout result</li>
</ul>
</li>
<li>[x] <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">ppo</a><ul>
<li>want your policy to change but not by too much</li>
<li>use advantage weighted log likelihood updates, but clip the upside to some $\epsilon$ makes it a lot more stable</li>
</ul>
</li>
<li>[ ] <a href="https://arxiv.org/pdf/2402.03300">grpo/deepseekmath</a><ul>
<li>data mix using iterative pipeline</li>
<li>word n-gram neural model, averages all n-gram embeddings and mlp classification</li>
<li>collect dataset, do grpo on DeepSeekCoder</li>
<li>unified rl perspective: all algos can be described in terms of space parameterization, reward signal parametrization, policy gradient coefficient, GRPO just uses rule </li>
</ul>
</li>
<li>[ ] <a href="https://github.com/mbreuss/diffusion-literature-for-robotics?tab=readme-ov-file#Diffusion-in-Robotics">diffusion for robotics</a></li>
<li>[x] pi's <a href="https://www.physicalintelligence.company/download/pi0.pdf">new paper</a> (read ??)</li>
<li>[x] <a href="https://arxiv.org/html/2410.11758v1">action latents</a></li>
<li>[ ] <a href="https://people.eecs.berkeley.edu/~svlevine/">sergey levine</a> from berkeley has takes</li>
<li>[ ] some yilun du papers: original <a href="https://arxiv.org/abs/1903.08689">EBM</a>, his <a href="https://yilundu.github.io/thesis.pdf">thesis</a>, his <a href="https://yilundu.github.io/research_statement.pdf">research statement</a></li>
</ul>
<h2 id="diffusion">diffusion</h2>
<ul>
<li>[ ] <a href="https://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/">diffusion amth</a></li>
<li>[ ] figure out diffusion/normalizing flow/flow forcing, fr, https://boyuan.space/diffusion-forcing/</li>
<li>[ ] <a href="https://arxiv.org/abs/2210.02747">flow matching math</a></li>
<li>[ ] <a href="https://ai.meta.com/static-resource/movie-gen-research-paper">moviegen</a></li>
<li>[ ] dall-e algorithm</li>
<li>[ ] <a href="https://arxiv.org/abs/2305.03486">papers</a> from <a href="https://arxiv.org/abs/2206.00364">william brandon</a></li>
</ul>
<h2 id="sci-of-dl">sci of dl</h2>
<ul>
<li>[ ] <a href="https://arxiv.org/abs/2004.10802">scaling via data manifolds</a></li>
<li>[x] <a href="https://arxiv.org/abs/2106.09685">low rank</a><ul>
<li>can update matrix weights using low rank factorizations of matrices</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2410.21265">modular duality</a> by bernstein<ul>
<li>two angles for motivation:</li>
<li>gradients are not same type as weights, they are linear functionals on the weights i.e. $\Delta \mathcal{L}=g^{T}w.$ Hence we should update by the result of some "duality map" applied to the gradient.</li>
<li>In the gradient case where we want to optimize $g^{\dagger}\Delta w + \lVert \Delta w \rVert^{2}$ we want $\Delta w$ to be in the direction of the dualizer.</li>
<li>input/outputs are normed, we want to update weights so that activations stay in the same approximate norm. Specifically we set weight norms to be dual norms on input/output using their respective norms, i.e. $\lVert W \rVert=\max \frac{{\lVert W x_{in} \rVert_{out}}}{\lVert x_{in} \rVert_{in}}.$</li>
<li>Generalizing, we want to maintain smoothness of our layers for easy future gradient descent. Given the gradient, want to update in the direction that gives the most "bang for buck" accounting for some norm on the weights. Above maximum can be interpreted as curvature, i.e. fastest ROC of output given fixed input change. Dualizer is the argmax of this norm.</li>
<li>They arrive at several duality maps for different modules using specific norms, e.g. RMS for linear, max(RMS) over channels for convolution, $l_{1}\to RMS$ for embedding. Using the above max expression, get spectral nom for Linear, max spectral norm for Conv2D, some other weird stuff idk lmao</li>
<li>Formalize modules into a fwd, mass, sensitivity (curvature), and norm for the weights (defined by norms over input/output).</li>
</ul>
</li>
</ul>
<h2 id="ssm">ssm</h2>
<ul>
<li>[ ] albert gu's thesis</li>
<li>[ ] <a href="https://arxiv.org/abs/2405.21060">mamba2: ssm &gt; transformers</a></li>
<li>[ ] <a href="https://arxiv.org/abs/2403.01590">mamba is attentino</a></li>
</ul>
<h2 id="audioimagevideo-specific">audio/image/video specific</h2>
<ul>
<li>[x] <a href="https://arxiv.org/pdf/2404.02905">var</a><ul>
<li>motivation: want good bidirectional, want to be able to know bottom from top, want matryoshka-doll like stuff, just think harder about key causality relationship; tokenize-flatten seems bad</li>
<li>autoregressive "chunk" is a discrete token "resolution", each chunk is still several tokens generated either in parallel or autoregressively (prolly AR)</li>
</ul>
</li>
<li>in audio "conv subsampling" is just decreasing spatial resolution with convs</li>
<li>[x] <a href="https://arxiv.org/pdf/2005.08100">conformer</a><ul>
<li>FFM, MHA, Conv, FFM</li>
<li>Conv is actually kinda MLP, with linear up channel (pointwise), activation, norm, conv, activatoin, linear down</li>
</ul>
</li>
<li>[x] moondream<ul>
<li>uses image patches with crops of fixed sizes, vit, recombine crops to get image embedding</li>
<li>image adapter, prompt prefix, text tokens</li>
<li></li>
</ul>
</li>
<li>[ ] pixtral (vlm)</li>
<li>[x] <a href="https://arxiv.org/pdf/2411.19842">Stable Diffusion audio codec</a> (400-700 bps!!)<ul>
<li>patch ViT style, 300 ish</li>
<li>blocks of strided conv followed by lots of transformers (sliding attention, 128 tokens)</li>
<li>fully scalar quantization‚Äîbottleneck into even lower dim, and then quantize each dim -&gt; one token</li>
<li>either learn N scalars to snap to, or parametrize (they use floor tanh)</li>
<li>quantizer-dropout occasionally</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2306.00989">Hiera</a><ul>
<li>develops a new fully-transformer based vit encoder/decoder for classification/segmentation tasks on image/video</li>
<li>on each downsize, instead of using strided conv, use Q-pooled attention, similar to cross attention but Q terms get pooled via maxpool</li>
<li>masked loss, mask tokens don't get fed into the encoder only the decoder (they get "filled in")</li>
<li>use local window attention at lower resolutions, then global later on</li>
</ul>
</li>
<li>[x] <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/NVIDIA%20Cosmos_2.pdf">Nvidia Cosmos 2</a> AE and generative<ul>
<li>start with Haar 3D wavelet in time/space to downsample spatially</li>
<li>alternating factorized resblock and factorized attention with downsampling</li>
<li>ends at latent dimension 16 for continuous (diffusion) and 6 for discrete (autoregressive)</li>
<li>every video token is predicted autoregressively after flattening in sequence, use finite scalar quantization to quantize to total 64K vocab size</li>
<li>because attention is used on "every layer" context length is only 34 frames-quite sad lmao</li>
<li>use 8x16x16 compression for autoregressive and 8x8x8 for diffusion</li>
<li>autoregressive: hybrid 3D RoPE with absolute embeddings per head for each token, cross attention from text prompt (NOT concat, think abt why)</li>
<li>diffusion: typical DiT, patchify using 1x2x2 and project/flatten, then text CA plus adaLN from time</li>
<li>like suno, best is to use autoregressive to generate discrete tokens, then use "diffusion decoder"‚Äîcondition on autoregressive tokens and regenerate continuous latents using diffusion</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/abs/2111.06377">MAE</a><ul>
<li>Make AE modeling a masked task by masking some of the patches</li>
<li>feed into encoder (with pos embeddings) without mask tokens, feed into decoder with mask tokens, allows huge speedup</li>
<li>downstream ViT task still great, AE learns to encode global "semantic properties" cuz of masking task</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2501.09755">meta visual tokenizers</a><ul>
<li>opposing the conv/attention interleave strategy used by cosmos/SD, try full send single conv with 8x16x16 stride=kernel (aka patching) into ~1k feature dim, then full ViT with 3D RoPE/SwiGLU, then bottleneck</li>
<li>works well, however performance does <em>not</em> scale with encoder/decoder size (why???)</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2212.09748">original dit</a><ul>
<li>u-nets are great but let's try transformers instead</li>
<li>standard latent diffusion, break latent into $p\times p$ patches (use $p=2$)</li>
<li>classifier-free guidance, i.e. constraining $p(c|x)$ implicitly by using $\log p(x | c) - \log p(x|\emptyset)$ since $p(c|x)=\frac{{p(x|c)p(c)}}{p(x)}$, then setting $\mathcal{L}=p(x)+\gamma p^*(c|x)$ for $\gamma&gt;1$</li>
<li>various conditioning strategies including<ul>
<li>cross-attention</li>
<li>adaptive layernorm with "zero scaling" (scaling right before residual by multiplying by MLP output $\alpha$ initialized to 0)</li>
<li>straight concat</li>
</ul>
</li>
<li>Transformer decoder with just a linear and rearrange to get predicted $\epsilon$ and diagonal $\Sigma.$</li>
</ul>
</li>
<li>[ ] original <a href="https://arxiv.org/pdf/2010.11929">vit</a></li>
<li>[x] <a href="https://arxiv.org/pdf/2410.10629">sana</a><ul>
<li>huge compression vae, with F32C32P1 instead of standard F8C8P2, leads to 4x fewer tokens (1/16 "patches", 4x channels, 4x patches)</li>
<li>linear attention using $ReLU(Q)ReLU(K)^{T}$ instead of softmax, then instead of computing $n\times n$ matrix can do $ReLU(K)^{T}V$ first to get $d\times d$ matrix for linear time full sequence training<ul>
<li>inference is $O(1)$‚Äîadditional K,V vectors add to the new $d\times d$ matrix, then mv-mul with new Q vector</li>
</ul>
</li>
<li>add nonlinearity with 1x1 convs along channel and 3x3 conv along channel, token index and GLU</li>
<li>no positional encoding! rely on 3x3 conv for implicit positional info</li>
<li>bit quantization, triton cuda shit, for faster inference</li>
<li>[x] some Flow-DPM math ‚è´ üìÖ 2025-01-31 ‚úÖ 2025-02-01</li>
</ul>
</li>
<li>[x] <a href="https://arxiv.org/pdf/2006.11477">wav2vec</a><ul>
<li>architecture: 6-deep convnet for latent representation (stride 20ms, receptive field 25ms at 16k hz), then quantize using product codebook w linear projection (gumbel softmax to get $V\times G$ logits for codebook), causal transformer on latents to get context representation</li>
<li>train by masking some latents with learned mask token, cosine similarity for cross-entropy loss between "real" masked quantized latent and several distractors<ul>
<li>mask $\approx$ half of latents</li>
</ul>
</li>
<li>intuition:<ul>
<li>can reconstruct missing latent (when quantized) i.e. latents contain useful info (note no reconstruction!)</li>
<li>quantization isn't used for one-hot encodings autoregressive-style, rather is used to "regularize" the latents with some amount of "smoothness"</li>
</ul>
</li>
<li>fine-tune final contextual representations into text token classes with a linear layer (includes blank token)</li>
<li>uses Connectionist Temporal Classification loss i.e. takes "best possible" alignment allowing for repetitions and blank tokens, takes log prob</li>
</ul>
</li>
<li>[ ] seq-2-seq (in comparison to wave2vec)</li>
<li>[ ] <a href="https://arxiv.org/pdf/2406.02560v3">non-spiky ctc loss</a><ul>
<li>basically, it's easy to just say blank by default, want to keep blank</li>
<li>use <em>priors</em> that prefer unigram i.e. non-blank</li>
<li>so that probability for seeing given character is divided by prior probability to incentivize picking unigrams more often</li>
</ul>
</li>
<li>[x] <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">ctc loss</a><ul>
<li>CTC probability is sum over <em>all</em> alignments of getting the desired output, because this is subset of all possible labellings of all things this is guaranteed $\leq 1$</li>
<li>use DP to compute sum of probabilities of all alignments ending at given target sequence index, for given frame</li>
<li>introduce blank token and merging adjacent equal tokens to get "skips"</li>
</ul>
</li>
<li>[ ] <a href="https://arxiv.org/pdf/2106.07447">HuBERT</a><ul>
<li>cnn encoder ofc (it's 2021)</li>
<li>idea: masked loss training on quantized targets</li>
<li>but what are the targets? a: really degenerate bootstrapped middle activation<ul>
<li>start with random quantization with some linear map</li>
<li>do the best you can, then use middle layer of transformer as representation, k-means cluster</li>
<li>try again</li>
</ul>
</li>
</ul>
</li>
<li>[ ] <a href="https://tracks-to-4d.github.io/">tracks-to-4d</a></li>
<li>[ ] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf">mqvae</a></li>
</ul>
<h2 id="miscell">miscell</h2>
<ul>
<li>[ ]  <a href="https://arxiv.org/pdf/2305.18290">DPO</a></li>
<li>[ ] <a href="https://arxiv.org/pdf/2010.03978">transfer learning survey</a></li>
<li>[ ] <a href="https://arxiv.org/pdf/2309.06497">shampoo</a></li>
<li>[ ] decision transformer</li>
<li>[x] dqvae</li>
<li>[ ] vqgan</li>
<li>[x] vqvae</li>
<li>[ ] <a href="https://www.arxiv.org/pdf/2408.08152">deepseek prover</a></li>
<li>[ ] <a href="https://fleuret.org/public/lbdl-a5-booklet.pdf">little book of dl</a></li>
<li>[x] BERT and original <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">transformers</a><ul>
<li>attention (2017): add in positional encodings, also mask attention before softmax using causal, transformers solve text gen</li>
<li>BERT: full self-attention with masks using masked language modeling (MLM) loss; cross-entropy on logits on masked tokens</li>
</ul>
</li>
</ul></p>
    </article>
</section>
<!-- endblock -->
                
                
            </main>

            
                    <!-- block preview -->
    <div class="preview row row-cols-md-3 text-center pt-md-3" id="component-preview">
        <div class="col themed-grid-col">
            <a rel="prev" href="../../Coding/USACO/General%20Thoughts%2C%20Paradigms/" class="nav-link">
                <i class="fa fa-arrow-left"></i> Previous
            </a>
        </div>
        <div class="col themed-grid-col"></div>
        <div class="col themed-grid-col">
            <a rel="next" href="../../../Food%2BCooking/%E5%AE%B6%E5%B8%B8%E8%8F%9C%E8%B0%B1/%E7%B3%96%E9%86%8B%E6%8E%92%E9%AA%A8/" class="nav-link">
                Next <i class="fa fa-arrow-right"></i>
            </a>
        </div>
    </div>
<!-- endblock -->
            

            
                    <!-- block footer -->
<footer class="pt-4 my-md-5 pt-md-5 border-top" id="component-footer">
    <div class="row">
        <div class="col-12 col-md">
                <!-- block copyright -->

    <small class="d-block mb-3">
        Made with
        <a href="https://github.com/FernandoCelmer/mkdocs-simple-blog" target="_blank" rel="noopener">
            Simple Blog for MkDocs
        </a>
    </small>

<!-- endblock -->
        </div>
    </div>
</footer>
<!-- endblock -->
            
        </div>

            <script>var base_url = '../../../..';</script>
            <script src="../../../../assets/js/jquery-3.3.1.slim.min.js""></script>
            <script src="../../../../assets/js/bootstrap.bundle.min.js""></script>
            <script src="../../../../assets/js/main.min.js""></script>
                <script src="../../../../extra.js" defer></script>
                <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
                <script src="../../../../search/main.js" defer></script>

    </body>

</html>